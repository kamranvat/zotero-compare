[
	{
		"id": "http://zotero.org/users/local/escwiks7/items/23GUHKBX",
		"type": "article",
		"abstract": "Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates (by an environment simulator), which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.",
		"note": "arXiv:2005.09382 [cs]",
		"number": "arXiv:2005.09382",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text",
		"URL": "http://arxiv.org/abs/2005.09382",
		"author": [
			{
				"family": "Hill",
				"given": "Felix"
			},
			{
				"family": "Mokra",
				"given": "Sona"
			},
			{
				"family": "Wong",
				"given": "Nathaniel"
			},
			{
				"family": "Harley",
				"given": "Tim"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					5,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/V56C542Y",
		"type": "paper-conference",
		"container-title": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)",
		"page": "236–244",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Attention based natural language grounding by navigating virtual environment",
		"URL": "https://ieeexplore.ieee.org/abstract/document/8658389/?casa_token=xmG-s_OdnGQAAAAA:pELuxIEIL0dUhpZq43PFcMF5z0d-Ulb2Pg2E51mTfZcj15nd86lc61YvHwPLQ_x-UuZqeos8gsj9",
		"author": [
			{
				"family": "Sinha",
				"given": "Abhishek"
			},
			{
				"family": "Akilesh",
				"given": "B."
			},
			{
				"family": "Sarkar",
				"given": "Mausoom"
			},
			{
				"family": "Krishnamurthy",
				"given": "Balaji"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IDLRCB7B",
		"type": "article",
		"abstract": "We are increasingly surrounded by artificially intelligent technology that takes decisions and executes actions on our behalf. This creates a pressing need for general means to communicate with, instruct and guide artificial agents, with human language the most compelling means for such communication. To achieve this in a scalable fashion, agents must be able to relate language to the world and to actions; that is, their understanding of language must be grounded and embodied. However, learning grounded language is a notoriously challenging problem in artificial intelligence research. Here we present an agent that learns to interpret language in a simulated 3D environment where it is rewarded for the successful execution of written instructions. Trained via a combination of reinforcement and unsupervised learning, and beginning with minimal prior knowledge, the agent learns to relate linguistic symbols to emergent perceptual representations of its physical surroundings and to pertinent sequences of actions. The agent's comprehension of language extends beyond its prior experience, enabling it to apply familiar language to unfamiliar situations and to interpret entirely novel instructions. Moreover, the speed with which this agent learns new words increases as its semantic knowledge grows. This facility for generalising and bootstrapping semantic knowledge indicates the potential of the present approach for reconciling ambiguous natural language with the complexity of the physical world.",
		"note": "arXiv:1706.06551 [cs, stat]",
		"number": "arXiv:1706.06551",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Grounded Language Learning in a Simulated 3D World",
		"URL": "http://arxiv.org/abs/1706.06551",
		"author": [
			{
				"family": "Hermann",
				"given": "Karl Moritz"
			},
			{
				"family": "Hill",
				"given": "Felix"
			},
			{
				"family": "Green",
				"given": "Simon"
			},
			{
				"family": "Wang",
				"given": "Fumin"
			},
			{
				"family": "Faulkner",
				"given": "Ryan"
			},
			{
				"family": "Soyer",
				"given": "Hubert"
			},
			{
				"family": "Szepesvari",
				"given": "David"
			},
			{
				"family": "Czarnecki",
				"given": "Wojciech Marian"
			},
			{
				"family": "Jaderberg",
				"given": "Max"
			},
			{
				"family": "Teplyashin",
				"given": "Denis"
			},
			{
				"family": "Wainwright",
				"given": "Marcus"
			},
			{
				"family": "Apps",
				"given": "Chris"
			},
			{
				"family": "Hassabis",
				"given": "Demis"
			},
			{
				"family": "Blunsom",
				"given": "Phil"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					6,
					26
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LHTJGF7X",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE conference on computer vision and pattern recognition",
		"page": "3674–3683",
		"source": "Google Scholar",
		"title": "Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments",
		"title-short": "Vision-and-language navigation",
		"URL": "http://openaccess.thecvf.com/content_cvpr_2018/html/Anderson_Vision-and-Language_Navigation_Interpreting_CVPR_2018_paper.html",
		"author": [
			{
				"family": "Anderson",
				"given": "Peter"
			},
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Teney",
				"given": "Damien"
			},
			{
				"family": "Bruce",
				"given": "Jake"
			},
			{
				"family": "Johnson",
				"given": "Mark"
			},
			{
				"family": "Sünderhauf",
				"given": "Niko"
			},
			{
				"family": "Reid",
				"given": "Ian"
			},
			{
				"family": "Gould",
				"given": "Stephen"
			},
			{
				"family": "Van Den Hengel",
				"given": "Anton"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZLKFSKN8",
		"type": "article",
		"abstract": "We introduce The House Of inteRactions (THOR), a framework for visual AI research, available at http://ai2thor.allenai.org. AI2-THOR consists of near photo-realistic 3D indoor scenes, where AI agents can navigate in the scenes and interact with objects to perform tasks. AI2-THOR enables research in many different domains including but not limited to deep reinforcement learning, imitation learning, learning by interaction, planning, visual question answering, unsupervised representation learning, object detection and segmentation, and learning models of cognition. The goal of AI2-THOR is to facilitate building visually intelligent models and push the research forward in this domain.",
		"note": "arXiv:1712.05474 [cs]",
		"number": "arXiv:1712.05474",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AI2-THOR: An Interactive 3D Environment for Visual AI",
		"title-short": "AI2-THOR",
		"URL": "http://arxiv.org/abs/1712.05474",
		"author": [
			{
				"family": "Kolve",
				"given": "Eric"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Han",
				"given": "Winson"
			},
			{
				"family": "VanderBilt",
				"given": "Eli"
			},
			{
				"family": "Weihs",
				"given": "Luca"
			},
			{
				"family": "Herrasti",
				"given": "Alvaro"
			},
			{
				"family": "Deitke",
				"given": "Matt"
			},
			{
				"family": "Ehsani",
				"given": "Kiana"
			},
			{
				"family": "Gordon",
				"given": "Daniel"
			},
			{
				"family": "Zhu",
				"given": "Yuke"
			},
			{
				"family": "Kembhavi",
				"given": "Aniruddha"
			},
			{
				"family": "Gupta",
				"given": "Abhinav"
			},
			{
				"family": "Farhadi",
				"given": "Ali"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					26
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DVLZMHRU",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2022",
		"event-place": "Cham",
		"ISBN": "978-3-031-19841-0",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-031-19842-7_37",
		"page": "638-655",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Learning from Unlabeled 3D Environments for Vision-and-Language Navigation",
		"URL": "https://link.springer.com/10.1007/978-3-031-19842-7_37",
		"volume": "13699",
		"editor": [
			{
				"family": "Avidan",
				"given": "Shai"
			},
			{
				"family": "Brostow",
				"given": "Gabriel"
			},
			{
				"family": "Cissé",
				"given": "Moustapha"
			},
			{
				"family": "Farinella",
				"given": "Giovanni Maria"
			},
			{
				"family": "Hassner",
				"given": "Tal"
			}
		],
		"author": [
			{
				"family": "Chen",
				"given": "Shizhe"
			},
			{
				"family": "Guhur",
				"given": "Pierre-Louis"
			},
			{
				"family": "Tapaswi",
				"given": "Makarand"
			},
			{
				"family": "Schmid",
				"given": "Cordelia"
			},
			{
				"family": "Laptev",
				"given": "Ivan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/F6TU6S2P",
		"type": "article-journal",
		"container-title": "IEEE Robotics and Automation Letters",
		"issue": "3",
		"note": "publisher: IEEE",
		"page": "6870–6877",
		"source": "Google Scholar",
		"title": "Following natural language instructions for household tasks with landmark guided search and reinforced pose adjustment",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9785410/?casa_token=GZpQPsORhuwAAAAA:tFNe2vlm-ohSctxo08YIHZvsI7yILkWDxO0016OT4OeVYc6JWAjfRTomqWXT8uW4GGqOiitzrlwQ",
		"volume": "7",
		"author": [
			{
				"family": "Murray",
				"given": "Michael"
			},
			{
				"family": "Cakmak",
				"given": "Maya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2GW3PMTV",
		"type": "article",
		"abstract": "Vision-Language Navigation (VLN) tasks require an agent to follow human language instructions to navigate in previously unseen environments. This challenging field involving problems in natural language processing, computer vision, robotics, etc., has spawn many excellent works focusing on various VLN tasks. This paper provides a comprehensive survey and an insightful taxonomy of these tasks based on the different characteristics of language instructions in these tasks. Depending on whether the navigation instructions are given for once or multiple times, this paper divides the tasks into two categories, i.e., single-turn and multi-turn tasks. For single-turn tasks, we further subdivide them into goal-oriented and route-oriented based on whether the instructions designate a single goal location or specify a sequence of multiple locations. For multi-turn tasks, we subdivide them into passive and interactive tasks based on whether the agent is allowed to question the instruction or not. These tasks require different capabilities of the agent and entail various model designs. We identify progress made on the tasks and look into the limitations of existing VLN models and task settings. Finally, we discuss several open issues of VLN and point out some opportunities in the future, i.e., incorporating knowledge with VLN models and implementing them in the real physical world.",
		"note": "arXiv:2108.11544 [cs]",
		"number": "arXiv:2108.11544",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Vision-Language Navigation: A Survey and Taxonomy",
		"title-short": "Vision-Language Navigation",
		"URL": "http://arxiv.org/abs/2108.11544",
		"author": [
			{
				"family": "Wu",
				"given": "Wansen"
			},
			{
				"family": "Chang",
				"given": "Tao"
			},
			{
				"family": "Li",
				"given": "Xinmeng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					4,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ADQHPJVB",
		"type": "article-journal",
		"container-title": "IEEE transactions on pattern analysis and machine intelligence",
		"issue": "12",
		"note": "publisher: IEEE",
		"page": "4205–4216",
		"source": "Google Scholar",
		"title": "Vision-language navigation policy learning and adaptation",
		"URL": "https://ieeexplore.ieee.org/abstract/document/8986691/?casa_token=k38UaIfT5V8AAAAA:RuaneGA45OA8PTTjE2fMNH1r9k6e17ZGJwLRlccnb5pcDXbD3ekgyR-FGfg44M1Y_6sKNuytX4LN",
		"volume": "43",
		"author": [
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "Huang",
				"given": "Qiuyuan"
			},
			{
				"family": "Celikyilmaz",
				"given": "Asli"
			},
			{
				"family": "Gao",
				"given": "Jianfeng"
			},
			{
				"family": "Shen",
				"given": "Dinghan"
			},
			{
				"family": "Wang",
				"given": "Yuan-Fang"
			},
			{
				"family": "Wang",
				"given": "William Yang"
			},
			{
				"family": "Zhang",
				"given": "Lei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7A3J6Z9T",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
		"page": "6629–6638",
		"source": "Google Scholar",
		"title": "Reinforced cross-modal matching and self-supervised imitation learning for vision-language navigation",
		"URL": "http://openaccess.thecvf.com/content_CVPR_2019/html/Wang_Reinforced_Cross-Modal_Matching_and_Self-Supervised_Imitation_Learning_for_Vision-Language_Navigation_CVPR_2019_paper.html",
		"author": [
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "Huang",
				"given": "Qiuyuan"
			},
			{
				"family": "Celikyilmaz",
				"given": "Asli"
			},
			{
				"family": "Gao",
				"given": "Jianfeng"
			},
			{
				"family": "Shen",
				"given": "Dinghan"
			},
			{
				"family": "Wang",
				"given": "Yuan-Fang"
			},
			{
				"family": "Wang",
				"given": "William Yang"
			},
			{
				"family": "Zhang",
				"given": "Lei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GT3FFM3Y",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2020",
		"event-place": "Cham",
		"ISBN": "978-3-030-58585-3",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-030-58586-0_25",
		"page": "413-430",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Environment-Agnostic Multitask Learning for Natural Language Grounded Navigation",
		"URL": "https://link.springer.com/10.1007/978-3-030-58586-0_25",
		"volume": "12369",
		"editor": [
			{
				"family": "Vedaldi",
				"given": "Andrea"
			},
			{
				"family": "Bischof",
				"given": "Horst"
			},
			{
				"family": "Brox",
				"given": "Thomas"
			},
			{
				"family": "Frahm",
				"given": "Jan-Michael"
			}
		],
		"author": [
			{
				"family": "Wang",
				"given": "Xin Eric"
			},
			{
				"family": "Jain",
				"given": "Vihan"
			},
			{
				"family": "Ie",
				"given": "Eugene"
			},
			{
				"family": "Wang",
				"given": "William Yang"
			},
			{
				"family": "Kozareva",
				"given": "Zornitsa"
			},
			{
				"family": "Ravi",
				"given": "Sujith"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZG5UTSIP",
		"type": "paper-conference",
		"container-title": "Conference on Robot Learning",
		"page": "706–717",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "A persistent spatial semantic representation for high-level natural language instruction execution",
		"URL": "https://proceedings.mlr.press/v164/blukis22a.html",
		"author": [
			{
				"family": "Blukis",
				"given": "Valts"
			},
			{
				"family": "Paxton",
				"given": "Chris"
			},
			{
				"family": "Fox",
				"given": "Dieter"
			},
			{
				"family": "Garg",
				"given": "Animesh"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/S8GBTKXG",
		"type": "chapter",
		"container-title": "Computer Games",
		"event-place": "Cham",
		"ISBN": "978-3-030-24336-4",
		"language": "en",
		"note": "collection-title: Communications in Computer and Information Science\nDOI: 10.1007/978-3-030-24337-1_3",
		"page": "41-75",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "TextWorld: A Learning Environment for Text-Based Games",
		"title-short": "TextWorld",
		"URL": "http://link.springer.com/10.1007/978-3-030-24337-1_3",
		"volume": "1017",
		"editor": [
			{
				"family": "Cazenave",
				"given": "Tristan"
			},
			{
				"family": "Saffidine",
				"given": "Abdallah"
			},
			{
				"family": "Sturtevant",
				"given": "Nathan"
			}
		],
		"author": [
			{
				"family": "Côté",
				"given": "Marc-Alexandre"
			},
			{
				"family": "Kádár",
				"given": "Ákos"
			},
			{
				"family": "Yuan",
				"given": "Xingdi"
			},
			{
				"family": "Kybartas",
				"given": "Ben"
			},
			{
				"family": "Barnes",
				"given": "Tavian"
			},
			{
				"family": "Fine",
				"given": "Emery"
			},
			{
				"family": "Moore",
				"given": "James"
			},
			{
				"family": "Hausknecht",
				"given": "Matthew"
			},
			{
				"family": "El Asri",
				"given": "Layla"
			},
			{
				"family": "Adada",
				"given": "Mahmoud"
			},
			{
				"family": "Tay",
				"given": "Wendy"
			},
			{
				"family": "Trischler",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2SQTWVZ9",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Cornell University",
		"source": "Google Scholar",
		"title": "Generalizable Learning for Natural Language Instruction Following on Physical Robots",
		"URL": "https://search.proquest.com/openview/ce5795b4f2bebef9c5d25a2a8ff7626e/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Blukis",
				"given": "Valts"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CARUYCGQ",
		"type": "article",
		"abstract": "Natural language is perhaps the most flexible and intuitive way for humans to communicate tasks to a robot. Prior work in imitation learning typically requires each task be specified with a task id or goal image -- something that is often impractical in open-world environments. On the other hand, previous approaches in instruction following allow agent behavior to be guided by language, but typically assume structure in the observations, actuators, or language that limit their applicability to complex settings like robotics. In this work, we present a method for incorporating free-form natural language conditioning into imitation learning. Our approach learns perception from pixels, natural language understanding, and multitask continuous control end-to-end as a single neural network. Unlike prior work in imitation learning, our method is able to incorporate unlabeled and unstructured demonstration data (i.e. no task or language labels). We show this dramatically improves language conditioned performance, while reducing the cost of language annotation to less than 1% of total data. At test time, a single language conditioned visuomotor policy trained with our method can perform a wide variety of robotic manipulation skills in a 3D environment, specified only with natural language descriptions of each task (e.g. \"open the drawer...now pick up the block...now press the green button...\"). To scale up the number of instructions an agent can follow, we propose combining text conditioned policies with large pretrained neural language models. We find this allows a policy to be robust to many out-of-distribution synonym instructions, without requiring new demonstrations. See videos of a human typing live text commands to our agent at language-play.github.io",
		"note": "arXiv:2005.07648 [cs]",
		"number": "arXiv:2005.07648",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language Conditioned Imitation Learning over Unstructured Data",
		"URL": "http://arxiv.org/abs/2005.07648",
		"author": [
			{
				"family": "Lynch",
				"given": "Corey"
			},
			{
				"family": "Sermanet",
				"given": "Pierre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5A85KSZV",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Common sense and Semantic-Guided Navigation via Language in Embodied Environments",
		"URL": "https://openreview.net/forum?id=Bkx5ceHFwH",
		"author": [
			{
				"family": "Yu",
				"given": "Dian"
			},
			{
				"family": "Khatri",
				"given": "Chandra"
			},
			{
				"family": "Papangelis",
				"given": "Alexandros"
			},
			{
				"family": "Namazifar",
				"given": "Mahdi"
			},
			{
				"family": "Madotto",
				"given": "Andrea"
			},
			{
				"family": "Zheng",
				"given": "Huaixiu"
			},
			{
				"family": "Tur",
				"given": "Gokhan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/75PSZ4DI",
		"type": "article",
		"abstract": "Vision language navigation is the task that requires an agent to navigate through a 3D environment based on natural language instructions. One key challenge in this task is to ground instructions with the current visual information that the agent perceives. Most of the existing work employs soft attention over individual words to locate the instruction required for the next action. However, different words have different functions in a sentence (e.g., modifiers convey attributes, verbs convey actions). Syntax information like dependencies and phrase structures can aid the agent to locate important parts of the instruction. Hence, in this paper, we propose a navigation agent that utilizes syntax information derived from a dependency tree to enhance alignment between the instruction and the current visual scenes. Empirically, our agent outperforms the baseline model that does not use syntax information on the Room-to-Room dataset, especially in the unseen environment. Besides, our agent achieves the new state-of-the-art on Room-Across-Room dataset, which contains instructions in 3 languages (English, Hindi, and Telugu). We also show that our agent is better at aligning instructions with the current visual information via qualitative visualizations. Code and models: https://github.com/jialuli-luka/SyntaxVLN",
		"note": "arXiv:2104.09580 [cs]",
		"number": "arXiv:2104.09580",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information",
		"URL": "http://arxiv.org/abs/2104.09580",
		"author": [
			{
				"family": "Li",
				"given": "Jialu"
			},
			{
				"family": "Tan",
				"given": "Hao"
			},
			{
				"family": "Bansal",
				"given": "Mohit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					4,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9AYTAUYX",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Gated-Attention Architectures for Task-Oriented Language Grounding",
		"URL": "https://www.researchgate.net/profile/Devendra-Chaplot-2/publication/317823228_Gated-Attention_Architectures_for_Task-Oriented_Language_Grounding/links/5970fa310f7e9b25e8605c65/Gated-Attention-Architectures-for-Task-Oriented-Language-Grounding.pdf",
		"author": [
			{
				"family": "Sathyendra",
				"given": "Devendra Singh Chaplot Kanthashree Mysore"
			},
			{
				"family": "Pasumarthi",
				"given": "Rama Kumar"
			},
			{
				"family": "Rajagopal",
				"given": "Dheeraj"
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3I5Q4V4B",
		"type": "article",
		"abstract": "Sparse reward is one of the most challenging problems in reinforcement learning (RL). Hindsight Experience Replay (HER) attempts to address this issue by converting a failed experience to a successful one by relabeling the goals. Despite its effectiveness, HER has limited applicability because it lacks a compact and universal goal representation. We present Augmenting experienCe via TeacheR's adviCE (ACTRCE), an efficient reinforcement learning technique that extends the HER framework using natural language as the goal representation. We first analyze the differences among goal representation, and show that ACTRCE can efficiently solve difficult reinforcement learning problems in challenging 3D navigation tasks, whereas HER with non-language goal representation failed to learn. We also show that with language goal representations, the agent can generalize to unseen instructions, and even generalize to instructions with unseen lexicons. We further demonstrate it is crucial to use hindsight advice to solve challenging tasks, and even small amount of advice is sufficient for the agent to achieve good performance.",
		"note": "arXiv:1902.04546 [cs, stat]",
		"number": "arXiv:1902.04546",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ACTRCE: Augmenting Experience via Teacher's Advice For Multi-Goal Reinforcement Learning",
		"title-short": "ACTRCE",
		"URL": "http://arxiv.org/abs/1902.04546",
		"author": [
			{
				"family": "Chan",
				"given": "Harris"
			},
			{
				"family": "Wu",
				"given": "Yuhuai"
			},
			{
				"family": "Kiros",
				"given": "Jamie"
			},
			{
				"family": "Fidler",
				"given": "Sanja"
			},
			{
				"family": "Ba",
				"given": "Jimmy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					2,
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8KKZ4ZZU",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "10012–10022",
		"source": "Google Scholar",
		"title": "Vision-language navigation with self-supervised auxiliary reasoning tasks",
		"URL": "http://openaccess.thecvf.com/content_CVPR_2020/html/Zhu_Vision-Language_Navigation_With_Self-Supervised_Auxiliary_Reasoning_Tasks_CVPR_2020_paper.html",
		"author": [
			{
				"family": "Zhu",
				"given": "Fengda"
			},
			{
				"family": "Zhu",
				"given": "Yi"
			},
			{
				"family": "Chang",
				"given": "Xiaojun"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XDL7AUD9",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Learning to navigate by distilling visual information and natural language instructions",
		"URL": "https://openreview.net/forum?id=HJPSN3gRW",
		"author": [
			{
				"family": "Sinha",
				"given": "Abhishek"
			},
			{
				"family": "Akilesh",
				"given": "B."
			},
			{
				"family": "Sarkar",
				"given": "Mausoom"
			},
			{
				"family": "Krishnamurthy",
				"given": "Balaji"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9IS3HIN8",
		"type": "paper-conference",
		"abstract": "A long-term goal of AI research is to build intelligent agents that can communicate with humans in natural language, perceive the environment, and perform real-world tasks. Vision-and-Language Navigation (VLN) is a fundamental and interdisciplinary research topic towards this goal, and receives increasing attention from natural language processing, computer vision, robotics, and machine learning communities. In this paper, we review contemporary studies in the emerging field of VLN, covering tasks, evaluation metrics, methods, etc. Through structured analysis of current progress and challenges, we highlight the limitations of current VLN and opportunities for future work. This paper serves as a thorough reference for the VLN research community.",
		"container-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
		"DOI": "10.18653/v1/2022.acl-long.524",
		"note": "arXiv:2203.12667 [cs]",
		"page": "7606-7623",
		"source": "arXiv.org",
		"title": "Vision-and-Language Navigation: A Survey of Tasks, Methods, and Future Directions",
		"title-short": "Vision-and-Language Navigation",
		"URL": "http://arxiv.org/abs/2203.12667",
		"author": [
			{
				"family": "Gu",
				"given": "Jing"
			},
			{
				"family": "Stefani",
				"given": "Eliana"
			},
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Thomason",
				"given": "Jesse"
			},
			{
				"family": "Wang",
				"given": "Xin Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ANQ47IVY",
		"type": "article",
		"abstract": "In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle 'off the path' scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based on the shortest path from the agent's location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work do not measure how much of a language instruction the agent is able to follow. In this work, we propose a simple and effective language-aligned supervision scheme, and a new metric that measures the number of sub-instructions the agent has completed during navigation.",
		"note": "arXiv:2109.15207 [cs]",
		"number": "arXiv:2109.15207",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments",
		"URL": "http://arxiv.org/abs/2109.15207",
		"author": [
			{
				"family": "Raychaudhuri",
				"given": "Sonia"
			},
			{
				"family": "Wani",
				"given": "Saim"
			},
			{
				"family": "Patel",
				"given": "Shivansh"
			},
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Chang",
				"given": "Angel X."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					9,
					30
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3XMCMRCH",
		"type": "article",
		"abstract": "Recent years have seen an increasing amount of work on embodied AI agents that can perform tasks by following human language instructions. However, most of these agents are reactive, meaning that they simply learn and imitate behaviors encountered in the training data. These reactive agents are insufficient for long-horizon complex tasks. To address this limitation, we propose a neuro-symbolic deliberative agent that, while following language instructions, proactively applies reasoning and planning based on its neural and symbolic representations acquired from past experience (e.g., natural language and egocentric vision). We show that our deliberative agent achieves greater than 70% improvement over reactive baselines on the challenging TEACh benchmark. Moreover, the underlying reasoning and planning processes, together with our modular framework, offer impressive transparency and explainability to the behaviors of the agent. This enables an in-depth understanding of the agent's capabilities, which shed light on challenges and opportunities for future embodied agents for instruction following. The code is available at https://github.com/sled-group/DANLI.",
		"note": "arXiv:2210.12485 [cs]",
		"number": "arXiv:2210.12485",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "DANLI: Deliberative Agent for Following Natural Language Instructions",
		"title-short": "DANLI",
		"URL": "http://arxiv.org/abs/2210.12485",
		"author": [
			{
				"family": "Zhang",
				"given": "Yichi"
			},
			{
				"family": "Yang",
				"given": "Jianing"
			},
			{
				"family": "Pan",
				"given": "Jiayi"
			},
			{
				"family": "Storks",
				"given": "Shane"
			},
			{
				"family": "Devraj",
				"given": "Nikhil"
			},
			{
				"family": "Ma",
				"given": "Ziqiao"
			},
			{
				"family": "Yu",
				"given": "Keunwoo Peter"
			},
			{
				"family": "Bao",
				"given": "Yuwei"
			},
			{
				"family": "Chai",
				"given": "Joyce"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					22
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IZ8GSL7X",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2020",
		"event-place": "Cham",
		"ISBN": "978-3-030-58603-4",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-030-58604-1_7",
		"page": "104-120",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments",
		"title-short": "Beyond the Nav-Graph",
		"URL": "https://link.springer.com/10.1007/978-3-030-58604-1_7",
		"volume": "12373",
		"editor": [
			{
				"family": "Vedaldi",
				"given": "Andrea"
			},
			{
				"family": "Bischof",
				"given": "Horst"
			},
			{
				"family": "Brox",
				"given": "Thomas"
			},
			{
				"family": "Frahm",
				"given": "Jan-Michael"
			}
		],
		"author": [
			{
				"family": "Krantz",
				"given": "Jacob"
			},
			{
				"family": "Wijmans",
				"given": "Erik"
			},
			{
				"family": "Majumdar",
				"given": "Arjun"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			},
			{
				"family": "Lee",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5JKD2I22",
		"type": "article",
		"abstract": "Vision-and-language navigation requires an agent to navigate through a real 3D environment following natural language instructions. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent's performance at following each part of the instruction cannot be assessed during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the traceability of agents through the completion of an instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the benchmark dataset Room-to-Room (R2R) with sub-instructions and their corresponding paths. To make use of this data, we propose effective sub-instruction attention and shifting modules that select and attend to a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline models, and show that our proposed method improves the performance of all four agents. We release the Fine-Grained R2R dataset (FGR2R) and the code at https://github.com/YicongHong/Fine-Grained-R2R.",
		"note": "arXiv:2004.02707 [cs]",
		"number": "arXiv:2004.02707",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Sub-Instruction Aware Vision-and-Language Navigation",
		"URL": "http://arxiv.org/abs/2004.02707",
		"author": [
			{
				"family": "Hong",
				"given": "Yicong"
			},
			{
				"family": "Rodriguez-Opazo",
				"given": "Cristian"
			},
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Gould",
				"given": "Stephen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WIHFYRSW",
		"type": "paper-conference",
		"container-title": "Proceedings of the 2020 on Great Lakes Symposium on VLSI",
		"DOI": "10.1145/3386263.3407652",
		"event-place": "Virtual Event China",
		"event-title": "GLSVLSI '20: Great Lakes Symposium on VLSI 2020",
		"ISBN": "978-1-4503-7944-1",
		"language": "en",
		"page": "131-136",
		"publisher": "ACM",
		"publisher-place": "Virtual Event China",
		"source": "DOI.org (Crossref)",
		"title": "Energy-Efficient Hardware for Language Guided Reinforcement Learning",
		"URL": "https://dl.acm.org/doi/10.1145/3386263.3407652",
		"author": [
			{
				"family": "Shiri",
				"given": "Aidin"
			},
			{
				"family": "Mazumder",
				"given": "Arnab Neelim"
			},
			{
				"family": "Prakash",
				"given": "Bharat"
			},
			{
				"family": "Manjunath",
				"given": "Nitheesh Kumar"
			},
			{
				"family": "Homayoun",
				"given": "Houman"
			},
			{
				"family": "Sasan",
				"given": "Avesta"
			},
			{
				"family": "Waytowich",
				"given": "Nicholas R."
			},
			{
				"family": "Mohsenin",
				"given": "Tinoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					9,
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7JNASJTF",
		"type": "paper-conference",
		"container-title": "Language in Reinforcement Learning Workshop at ICML 2020",
		"source": "Google Scholar",
		"title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments–Extended Abstract",
		"title-short": "Beyond the Nav-Graph",
		"URL": "https://openreview.net/forum?id=BRjplxPwk1",
		"author": [
			{
				"family": "Krantz",
				"given": "Jacob"
			},
			{
				"family": "Wijmans",
				"given": "Erik"
			},
			{
				"family": "Majumdar",
				"given": "Arjun"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			},
			{
				"family": "Lee",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/R3R4NR3D",
		"type": "article",
		"abstract": "CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.",
		"note": "arXiv:2303.08127 [cs]",
		"number": "arXiv:2303.08127",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CB2: Collaborative Natural Language Interaction Research Platform",
		"title-short": "CB2",
		"URL": "http://arxiv.org/abs/2303.08127",
		"author": [
			{
				"family": "Sharf",
				"given": "Jacob"
			},
			{
				"family": "Gul",
				"given": "Mustafa Omer"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					29
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NJTJSI2Y",
		"type": "paper-conference",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"note": "issue: 11",
		"page": "13300–13308",
		"source": "Google Scholar",
		"title": "Learning compositional tasks from language instructions",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/26561",
		"volume": "37",
		"author": [
			{
				"family": "Logeswaran",
				"given": "Lajanugen"
			},
			{
				"family": "Carvalho",
				"given": "Wilka"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HDJW2M6F",
		"type": "paper-conference",
		"container-title": "Conference on Robot Learning",
		"page": "1743–1754",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Embodied concept learner: Self-supervised learning of concepts and mapping through instruction following",
		"title-short": "Embodied concept learner",
		"URL": "https://proceedings.mlr.press/v205/ding23b.html",
		"author": [
			{
				"family": "Ding",
				"given": "Mingyu"
			},
			{
				"family": "Xu",
				"given": "Yan"
			},
			{
				"family": "Chen",
				"given": "Zhenfang"
			},
			{
				"family": "Cox",
				"given": "David Daniel"
			},
			{
				"family": "Luo",
				"given": "Ping"
			},
			{
				"family": "Tenenbaum",
				"given": "Joshua B."
			},
			{
				"family": "Gan",
				"given": "Chuang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GIHM2FK4",
		"type": "article-journal",
		"container-title": "UMBC Student Collection",
		"source": "Google Scholar",
		"title": "Guiding safe reinforcement learning policies using structured language constraints",
		"URL": "https://mdsoar.org/handle/11603/17463",
		"author": [
			{
				"family": "Prakash",
				"given": "Bharat"
			},
			{
				"family": "Waytowich",
				"given": "Nicholas"
			},
			{
				"family": "Ganesan",
				"given": "Ashwinkumar"
			},
			{
				"family": "Oates",
				"given": "Tim"
			},
			{
				"family": "Mohsenin",
				"given": "Tinoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/54ELJCU5",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2005.07648",
		"source": "Google Scholar",
		"title": "Grounding language in play",
		"URL": "https://www.academia.edu/download/93604914/2005.07648v1.pdf",
		"volume": "3",
		"author": [
			{
				"family": "Lynch",
				"given": "Corey"
			},
			{
				"family": "Sermanet",
				"given": "Pierre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/49G87GF8",
		"type": "article",
		"abstract": "Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the onus of designing language-conditional reward functions which may not be easily or tractably implemented as the complexity of the environment and the language scales. To overcome this limitation, we present a framework within which instruction-conditional RL agents are trained using rewards obtained not from the environment, but from reward models which are jointly trained from expert examples. As reward models improve, they learn to accurately reward agents for completing tasks for environment configurations---and for instructions---not present amongst the expert data. This framework effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, it enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new expert examples.",
		"note": "arXiv:1806.01946 [cs]",
		"number": "arXiv:1806.01946",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning to Understand Goal Specifications by Modelling Reward",
		"URL": "http://arxiv.org/abs/1806.01946",
		"author": [
			{
				"family": "Bahdanau",
				"given": "Dzmitry"
			},
			{
				"family": "Hill",
				"given": "Felix"
			},
			{
				"family": "Leike",
				"given": "Jan"
			},
			{
				"family": "Hughes",
				"given": "Edward"
			},
			{
				"family": "Hosseini",
				"given": "Arian"
			},
			{
				"family": "Kohli",
				"given": "Pushmeet"
			},
			{
				"family": "Grefenstette",
				"given": "Edward"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12,
					23
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/G37CRSXM",
		"type": "paper-conference",
		"container-title": "Proceedings of the National Conference on Emerging Computer Applications (NCECA)",
		"page": "22",
		"source": "Google Scholar",
		"title": "Vision and Language Navigation Using Minimal Voice Instructions",
		"URL": "https://nceca.in/index/NCECA2021%20(68).pdf",
		"author": [
			{
				"family": "Shah",
				"given": "Ansh"
			},
			{
				"family": "Tawde",
				"given": "Prachi"
			},
			{
				"family": "Kansara",
				"given": "Parth"
			},
			{
				"family": "Meswani",
				"given": "Parth"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4IDUR7WS",
		"type": "paper-conference",
		"container-title": "2021 IEEE International Conference on Robotics and Automation (ICRA)",
		"page": "13238–13246",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Hierarchical cross-modal agent for robotics vision-and-language navigation",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9561806/?casa_token=AoYKEj7H2bIAAAAA:j6xVE3AnuJ8M0j3HhCfcmM5a-TwDz5GtrrKEhHNcWKZf_D4KV16nEJk0aR-0dtjK2uM5nZVcyyKO",
		"author": [
			{
				"family": "Irshad",
				"given": "Muhammad Zubair"
			},
			{
				"family": "Ma",
				"given": "Chih-Yao"
			},
			{
				"family": "Kira",
				"given": "Zsolt"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/TPQVW47X",
		"type": "paper-conference",
		"container-title": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
		"page": "877–884",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Human-Agent Collaboration Strategies for Vision-Grounded Instruction Following",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9688143/?casa_token=iv1BxNnJTwAAAAAA:-EAAvtouiPPaeApIBgTLka887J5dY8LyeqqNfvz12exgOEWzcfuFwG2CSto36gvT9n_Tme3Tpij1",
		"author": [
			{
				"family": "Chao",
				"given": "Guan-Lin"
			},
			{
				"family": "Lane",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/PMQ4SR3W",
		"type": "article-journal",
		"container-title": "IEEE Robotics and Automation Letters",
		"issue": "4",
		"note": "publisher: IEEE",
		"page": "11205–11212",
		"source": "Google Scholar",
		"title": "What matters in language conditioned robotic imitation learning over unstructured data",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9849097/?casa_token=Sua7IP8ndpMAAAAA:hv-wGpSjAX3UZLuf8v5zDT-eMAjhkBK18WUXIx-2xjTvCCukZCvs8-cg4dw-kgtZM95VM8QodUTz",
		"volume": "7",
		"author": [
			{
				"family": "Mees",
				"given": "Oier"
			},
			{
				"family": "Hermann",
				"given": "Lukas"
			},
			{
				"family": "Burgard",
				"given": "Wolfram"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8WMIGHHF",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "9982–9991",
		"source": "Google Scholar",
		"title": "Reverie: Remote embodied visual referring expression in real indoor environments",
		"title-short": "Reverie",
		"URL": "http://openaccess.thecvf.com/content_CVPR_2020/html/Qi_REVERIE_Remote_Embodied_Visual_Referring_Expression_in_Real_Indoor_Environments_CVPR_2020_paper.html",
		"author": [
			{
				"family": "Qi",
				"given": "Yuankai"
			},
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Anderson",
				"given": "Peter"
			},
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "Wang",
				"given": "William Yang"
			},
			{
				"family": "Shen",
				"given": "Chunhua"
			},
			{
				"family": "Hengel",
				"given": "Anton",
				"dropping-particle": "van den"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/C9FXK28I",
		"type": "article",
		"abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions on World of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.",
		"note": "arXiv:1812.09195 [cs, stat]",
		"number": "arXiv:1812.09195",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning to Navigate the Web",
		"URL": "http://arxiv.org/abs/1812.09195",
		"author": [
			{
				"family": "Gur",
				"given": "Izzeddin"
			},
			{
				"family": "Rueckert",
				"given": "Ulrich"
			},
			{
				"family": "Faust",
				"given": "Aleksandra"
			},
			{
				"family": "Hakkani-Tur",
				"given": "Dilek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					21
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9FG5Z44H",
		"type": "paper-conference",
		"container-title": "2020 6th International Conference on Big Data and Information Analytics (BigDIA)",
		"page": "74–79",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Vision and language navigation using multi-head attention mechanism",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9384528/?casa_token=tzeRws4udmAAAAAA:XJbZYEJLfLY3gmhdqp8s09fphFyti4vP09z_CDRZwztfnwBodbmLPxCl2jh6vA9lLCp1kF4cm2bX",
		"author": [
			{
				"family": "Mao",
				"given": "Sai"
			},
			{
				"family": "Wu",
				"given": "Junmin"
			},
			{
				"family": "Hong",
				"given": "Siqi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZABA34GU",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2020",
		"event-place": "Cham",
		"ISBN": "978-3-030-58538-9",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-030-58539-6_5",
		"page": "71-86",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Counterfactual Vision-and-Language Navigation via Adversarial Path Sampler",
		"URL": "https://link.springer.com/10.1007/978-3-030-58539-6_5",
		"volume": "12351",
		"editor": [
			{
				"family": "Vedaldi",
				"given": "Andrea"
			},
			{
				"family": "Bischof",
				"given": "Horst"
			},
			{
				"family": "Brox",
				"given": "Thomas"
			},
			{
				"family": "Frahm",
				"given": "Jan-Michael"
			}
		],
		"author": [
			{
				"family": "Fu",
				"given": "Tsu-Jui"
			},
			{
				"family": "Wang",
				"given": "Xin Eric"
			},
			{
				"family": "Peterson",
				"given": "Matthew F."
			},
			{
				"family": "Grafton",
				"given": "Scott T."
			},
			{
				"family": "Eckstein",
				"given": "Miguel P."
			},
			{
				"family": "Wang",
				"given": "William Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9NPN2NXI",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
		"page": "10740–10749",
		"source": "Google Scholar",
		"title": "Alfred: A benchmark for interpreting grounded instructions for everyday tasks",
		"title-short": "Alfred",
		"URL": "http://openaccess.thecvf.com/content_CVPR_2020/html/Shridhar_ALFRED_A_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks_CVPR_2020_paper.html",
		"author": [
			{
				"family": "Shridhar",
				"given": "Mohit"
			},
			{
				"family": "Thomason",
				"given": "Jesse"
			},
			{
				"family": "Gordon",
				"given": "Daniel"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Han",
				"given": "Winson"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Zettlemoyer",
				"given": "Luke"
			},
			{
				"family": "Fox",
				"given": "Dieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JEHYJH3K",
		"type": "paper-conference",
		"container-title": "2022 26th International Conference on Pattern Recognition (ICPR)",
		"page": "4065–4071",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Semantically-aware spatio-temporal reasoning agent for vision-and-language navigation in continuous environments",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9956561/?casa_token=vh-7ojSqHCMAAAAA:4UfpQP7aoY4BJ0N965evNp_3Yq-HSJhvcA9bSSQ6PMEywwBVWM4OcGb1xbf3eyf35LLaTn6XC6uC",
		"author": [
			{
				"family": "Irshad",
				"given": "Muhammad Zubair"
			},
			{
				"family": "Mithun",
				"given": "Niluthpol Chowdhury"
			},
			{
				"family": "Seymour",
				"given": "Zachary"
			},
			{
				"family": "Chiu",
				"given": "Han-Pang"
			},
			{
				"family": "Samarasekera",
				"given": "Supun"
			},
			{
				"family": "Kumar",
				"given": "Rakesh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/TL29ELLR",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "12689–12699",
		"source": "Google Scholar",
		"title": "Soon: Scenario oriented object navigation with graph-based exploration",
		"title-short": "Soon",
		"URL": "http://openaccess.thecvf.com/content/CVPR2021/html/Zhu_SOON_Scenario_Oriented_Object_Navigation_With_Graph-Based_Exploration_CVPR_2021_paper.html",
		"author": [
			{
				"family": "Zhu",
				"given": "Fengda"
			},
			{
				"family": "Liang",
				"given": "Xiwen"
			},
			{
				"family": "Zhu",
				"given": "Yi"
			},
			{
				"family": "Yu",
				"given": "Qizhi"
			},
			{
				"family": "Chang",
				"given": "Xiaojun"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/F45MFXNG",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "7504–7519",
		"source": "Google Scholar",
		"title": "Grounded reinforcement learning: Learning to win the game under human commands",
		"title-short": "Grounded reinforcement learning",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/318f3ae8be3c97cb7555e1c932f472a1-Abstract-Conference.html",
		"volume": "35",
		"author": [
			{
				"family": "Xu",
				"given": "Shusheng"
			},
			{
				"family": "Wang",
				"given": "Huaijie"
			},
			{
				"family": "Wu",
				"given": "Yi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/H3DTCLEG",
		"type": "book",
		"publisher": "Cornell University",
		"source": "Google Scholar",
		"title": "Scalable and Interpretable Approaches for Learning to Follow Natural Language Instructions",
		"URL": "https://search.proquest.com/openview/b88ee4b29287b0644da7d4b940f16c67/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Misra",
				"given": "Dipendra Kumar"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/T5W9X9I9",
		"type": "article-journal",
		"container-title": "IEEE Access",
		"note": "publisher: IEEE",
		"page": "135426–135442",
		"source": "Google Scholar",
		"title": "A survey on visual navigation for artificial agents with deep reinforcement learning",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9146614/",
		"volume": "8",
		"author": [
			{
				"family": "Zeng",
				"given": "Fanyu"
			},
			{
				"family": "Wang",
				"given": "Chen"
			},
			{
				"family": "Ge",
				"given": "Shuzhi Sam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HL39KZQD",
		"type": "article-journal",
		"container-title": "IEEE access",
		"note": "publisher: IEEE",
		"page": "209320–209344",
		"source": "Google Scholar",
		"title": "A gentle introduction to reinforcement learning and its application in different fields",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9261348/",
		"volume": "8",
		"author": [
			{
				"family": "Naeem",
				"given": "Muddasar"
			},
			{
				"family": "Rizvi",
				"given": "Syed Tahir Hussain"
			},
			{
				"family": "Coronato",
				"given": "Antonio"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BXI6KDXT",
		"type": "paper-conference",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"note": "issue: 03",
		"page": "2459–2466",
		"source": "Google Scholar",
		"title": "Just ask: An interactive learning framework for vision and language navigation",
		"title-short": "Just ask",
		"URL": "http://ojs.aaai.org/index.php/AAAI/article/view/5627",
		"volume": "34",
		"author": [
			{
				"family": "Chi",
				"given": "Ta-Chung"
			},
			{
				"family": "Shen",
				"given": "Minmin"
			},
			{
				"family": "Eric",
				"given": "Mihail"
			},
			{
				"family": "Kim",
				"given": "Seokhwan"
			},
			{
				"family": "Hakkani-tur",
				"given": "Dilek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DUSS5ULF",
		"type": "article-journal",
		"container-title": "Knowledge-Based Systems",
		"note": "publisher: Elsevier",
		"page": "110785",
		"source": "Google Scholar",
		"title": "Coarse-to-fine fusion for language grounding in 3D navigation",
		"URL": "https://www.sciencedirect.com/science/article/pii/S095070512300535X",
		"volume": "277",
		"author": [
			{
				"family": "Nguyen",
				"given": "Thanh Tin"
			},
			{
				"family": "Vo",
				"given": "Anh H."
			},
			{
				"family": "Choi",
				"given": "Soo-Mi"
			},
			{
				"family": "Kim",
				"given": "Yong-Guk"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BNWY8TGL",
		"type": "article-journal",
		"container-title": "International Journal of Advanced Computer Science and Applications",
		"issue": "12",
		"source": "Google Scholar",
		"title": "Training an agent for fps doom game using visual reinforcement learning and vizdoom",
		"URL": "https://www.researchgate.net/profile/Adil-Khan-2/publication/322152625_Training_an_Agent_for_FPS_Doom_Game_using_Visual_Reinforcement_Learning_and_VizDoom/links/5a48772aa6fdcce1971c86fb/Training-an-Agent-for-FPS-Doom-Game-using-Visual-Reinforcement-Learning-and-VizDoom.pdf",
		"volume": "8",
		"author": [
			{
				"family": "Adil",
				"given": "Khan"
			},
			{
				"family": "Jiang",
				"given": "Feng"
			},
			{
				"family": "Liu",
				"given": "Shaohui"
			},
			{
				"family": "Grigorev",
				"given": "Aleksei"
			},
			{
				"family": "Gupta",
				"given": "B. B."
			},
			{
				"family": "Rho",
				"given": "Seungmin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3T3LEYA2",
		"type": "paper-conference",
		"container-title": "Proceedings of the 30th ACM International Conference on Multimedia",
		"DOI": "10.1145/3503161.3548281",
		"event-place": "Lisboa Portugal",
		"event-title": "MM '22: The 30th ACM International Conference on Multimedia",
		"ISBN": "978-1-4503-9203-7",
		"language": "en",
		"page": "4194-4203",
		"publisher": "ACM",
		"publisher-place": "Lisboa Portugal",
		"source": "DOI.org (Crossref)",
		"title": "Target-Driven Structured Transformer Planner for Vision-Language Navigation",
		"URL": "https://dl.acm.org/doi/10.1145/3503161.3548281",
		"author": [
			{
				"family": "Zhao",
				"given": "Yusheng"
			},
			{
				"family": "Chen",
				"given": "Jinyu"
			},
			{
				"family": "Gao",
				"given": "Chen"
			},
			{
				"family": "Wang",
				"given": "Wenguan"
			},
			{
				"family": "Yang",
				"given": "Lirong"
			},
			{
				"family": "Ren",
				"given": "Haibing"
			},
			{
				"family": "Xia",
				"given": "Huaxia"
			},
			{
				"family": "Liu",
				"given": "Si"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GF4CILX3",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "13137–13146",
		"source": "Google Scholar",
		"title": "Towards learning a generic agent for vision-and-language navigation via pre-training",
		"URL": "http://openaccess.thecvf.com/content_CVPR_2020/html/Hao_Towards_Learning_a_Generic_Agent_for_Vision-and-Language_Navigation_via_Pre-Training_CVPR_2020_paper.html",
		"author": [
			{
				"family": "Hao",
				"given": "Weituo"
			},
			{
				"family": "Li",
				"given": "Chunyuan"
			},
			{
				"family": "Li",
				"given": "Xiujun"
			},
			{
				"family": "Carin",
				"given": "Lawrence"
			},
			{
				"family": "Gao",
				"given": "Jianfeng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QSC94IA4",
		"type": "paper-conference",
		"container-title": "Conference on Robot Learning",
		"page": "492–504",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action",
		"title-short": "Lm-nav",
		"URL": "https://proceedings.mlr.press/v205/shah23b.html",
		"author": [
			{
				"family": "Shah",
				"given": "Dhruv"
			},
			{
				"family": "Osiński",
				"given": "B\\lażej"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QMRFLWVX",
		"type": "article",
		"abstract": "A grand goal in AI is to build a robot that can accurately navigate based on natural language instructions, which requires the agent to perceive the scene, understand and ground language, and act in the real-world environment. One key challenge here is to learn to navigate in new environments that are unseen during training. Most of the existing approaches perform dramatically worse in unseen environments as compared to seen ones. In this paper, we present a generalizable navigational agent. Our agent is trained in two stages. The first stage is training via mixed imitation and reinforcement learning, combining the benefits from both off-policy and on-policy optimization. The second stage is fine-tuning via newly-introduced 'unseen' triplets (environment, path, instruction). To generate these unseen triplets, we propose a simple but effective 'environmental dropout' method to mimic unseen environments, which overcomes the problem of limited seen environment variability. Next, we apply semi-supervised learning (via back-translation) on these dropped-out environments to generate new paths and instructions. Empirically, we show that our agent is substantially better at generalizability when fine-tuned with these triplets, outperforming the state-of-art approaches by a large margin on the private unseen test set of the Room-to-Room task, and achieving the top rank on the leaderboard.",
		"note": "arXiv:1904.04195 [cs]",
		"number": "arXiv:1904.04195",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout",
		"title-short": "Learning to Navigate Unseen Environments",
		"URL": "http://arxiv.org/abs/1904.04195",
		"author": [
			{
				"family": "Tan",
				"given": "Hao"
			},
			{
				"family": "Yu",
				"given": "Licheng"
			},
			{
				"family": "Bansal",
				"given": "Mohit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					4,
					8
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7CKB2BE4",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
		"page": "8494–8502",
		"source": "Google Scholar",
		"title": "Virtualhome: Simulating household activities via programs",
		"title-short": "Virtualhome",
		"URL": "http://openaccess.thecvf.com/content_cvpr_2018/html/Puig_VirtualHome_Simulating_Household_CVPR_2018_paper.html",
		"author": [
			{
				"family": "Puig",
				"given": "Xavier"
			},
			{
				"family": "Ra",
				"given": "Kevin"
			},
			{
				"family": "Boben",
				"given": "Marko"
			},
			{
				"family": "Li",
				"given": "Jiaman"
			},
			{
				"family": "Wang",
				"given": "Tingwu"
			},
			{
				"family": "Fidler",
				"given": "Sanja"
			},
			{
				"family": "Torralba",
				"given": "Antonio"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GAJ7WE9S",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2022",
		"event-place": "Cham",
		"ISBN": "978-3-031-20058-8",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-031-20059-5_18",
		"page": "309-329",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Learning Disentanglement with Decoupled Labels for Vision-Language Navigation",
		"URL": "https://link.springer.com/10.1007/978-3-031-20059-5_18",
		"volume": "13696",
		"editor": [
			{
				"family": "Avidan",
				"given": "Shai"
			},
			{
				"family": "Brostow",
				"given": "Gabriel"
			},
			{
				"family": "Cissé",
				"given": "Moustapha"
			},
			{
				"family": "Farinella",
				"given": "Giovanni Maria"
			},
			{
				"family": "Hassner",
				"given": "Tal"
			}
		],
		"author": [
			{
				"family": "Cheng",
				"given": "Wenhao"
			},
			{
				"family": "Dong",
				"given": "Xingping"
			},
			{
				"family": "Khan",
				"given": "Salman"
			},
			{
				"family": "Shen",
				"given": "Jianbing"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/J5T8MUKS",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "7357–7367",
		"source": "Google Scholar",
		"title": "Soat: A scene-and object-aware transformer for vision-and-language navigation",
		"title-short": "Soat",
		"URL": "https://proceedings.neurips.cc/paper/2021/hash/3c8a49145944fed2bbcaade178a426c4-Abstract.html",
		"volume": "34",
		"author": [
			{
				"family": "Moudgil",
				"given": "Abhinav"
			},
			{
				"family": "Majumdar",
				"given": "Arjun"
			},
			{
				"family": "Agrawal",
				"given": "Harsh"
			},
			{
				"family": "Lee",
				"given": "Stefan"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/66NF5JH5",
		"type": "chapter",
		"container-title": "Visual Question Answering",
		"event-place": "Singapore",
		"ISBN": "978-981-19096-3-4",
		"language": "en",
		"note": "collection-title: Advances in Computer Vision and Pattern Recognition\nDOI: 10.1007/978-981-19-0964-1_10",
		"page": "147-164",
		"publisher": "Springer Nature Singapore",
		"publisher-place": "Singapore",
		"source": "DOI.org (Crossref)",
		"title": "Embodied VQA",
		"URL": "https://link.springer.com/10.1007/978-981-19-0964-1_10",
		"container-author": [
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Wang",
				"given": "Peng"
			},
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "He",
				"given": "Xiaodong"
			},
			{
				"family": "Zhu",
				"given": "Wenwu"
			}
		],
		"author": [
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Wang",
				"given": "Peng"
			},
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "He",
				"given": "Xiaodong"
			},
			{
				"family": "Zhu",
				"given": "Wenwu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8DA4JJ6V",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Generalized natural language grounded navigation via environment-agnostic multitask learning",
		"URL": "https://openreview.net/forum?id=HkxzNpNtDS",
		"author": [
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "Jain",
				"given": "Vihan"
			},
			{
				"family": "Ie",
				"given": "Eugene"
			},
			{
				"family": "Wang",
				"given": "William"
			},
			{
				"family": "Kozareva",
				"given": "Zornitsa"
			},
			{
				"family": "Ravi",
				"given": "Sujith"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ER8F8JEJ",
		"type": "article",
		"abstract": "Standard architectures used in instruction following often struggle on novel compositions of subgoals (e.g. navigating to landmarks or picking up objects) observed during training. We propose a modular architecture for following natural language instructions that describe sequences of diverse subgoals. In our approach, subgoal modules each carry out natural language instructions for a specific subgoal type. A sequence of modules to execute is chosen by learning to segment the instructions and predicting a subgoal type for each segment. When compared to standard, non-modular sequence-to-sequence approaches on ALFRED, a challenging instruction following benchmark, we find that modularization improves generalization to novel subgoal compositions, as well as to environments unseen in training.",
		"note": "arXiv:2010.12764 [cs]",
		"number": "arXiv:2010.12764",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Modular Networks for Compositional Instruction Following",
		"URL": "http://arxiv.org/abs/2010.12764",
		"author": [
			{
				"family": "Corona",
				"given": "Rodolfo"
			},
			{
				"family": "Fried",
				"given": "Daniel"
			},
			{
				"family": "Devin",
				"given": "Coline"
			},
			{
				"family": "Klein",
				"given": "Dan"
			},
			{
				"family": "Darrell",
				"given": "Trevor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					4,
					13
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CRENZAH6",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Robust Instruction-Following in a Situated Agent via Transfer-Learning from Text",
		"URL": "https://openreview.net/forum?id=rklraTNFwB",
		"author": [
			{
				"family": "Hill",
				"given": "Felix"
			},
			{
				"family": "Mokra",
				"given": "Sona"
			},
			{
				"family": "Wong",
				"given": "Nathaniel"
			},
			{
				"family": "Harley",
				"given": "Tim"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HTTVLQNW",
		"type": "article",
		"abstract": "For embodied agents, navigation is an important ability but not an isolated goal. Agents are also expected to perform specific tasks after reaching the target location, such as picking up objects and assembling them into a particular arrangement. We combine Vision-and-Language Navigation, assembling of collected objects, and object referring expression comprehension, to create a novel joint navigation-and-assembly task, named ArraMon. During this task, the agent (similar to a PokeMON GO player) is asked to find and collect different target objects one-by-one by navigating based on natural language instructions in a complex, realistic outdoor environment, but then also ARRAnge the collected objects part-by-part in an egocentric grid-layout environment. To support this task, we implement a 3D dynamic environment simulator and collect a dataset (in English; and also extended to Hindi) with human-written navigation and assembling instructions, and the corresponding ground truth trajectories. We also filter the collected instructions via a verification stage, leading to a total of 7.7K task instances (30.8K instructions and paths). We present results for several baseline models (integrated and biased) and metrics (nDTW, CTC, rPOD, and PTC), and the large model-human performance gap demonstrates that our task is challenging and presents a wide scope for future work. Our dataset, simulator, and code are publicly available at: https://arramonunc.github.io",
		"note": "arXiv:2011.07660 [cs]",
		"number": "arXiv:2011.07660",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments",
		"title-short": "ArraMon",
		"URL": "http://arxiv.org/abs/2011.07660",
		"author": [
			{
				"family": "Kim",
				"given": "Hyounghun"
			},
			{
				"family": "Zala",
				"given": "Abhay"
			},
			{
				"family": "Burri",
				"given": "Graham"
			},
			{
				"family": "Tan",
				"given": "Hao"
			},
			{
				"family": "Bansal",
				"given": "Mohit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					15
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WTQDTSAB",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
		"page": "15942–15952",
		"source": "Google Scholar",
		"title": "Episodic transformer for vision-and-language navigation",
		"URL": "http://openaccess.thecvf.com/content/ICCV2021/html/Pashevich_Episodic_Transformer_for_Vision-and-Language_Navigation_ICCV_2021_paper.html",
		"author": [
			{
				"family": "Pashevich",
				"given": "Alexander"
			},
			{
				"family": "Schmid",
				"given": "Cordelia"
			},
			{
				"family": "Sun",
				"given": "Chen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QNE4LTNF",
		"type": "article",
		"abstract": "Recent methods for embodied instruction following are typically trained end-to-end using imitation learning. This often requires the use of expert trajectories and low-level language instructions. Such approaches assume that neural states will integrate multimodal semantics to perform state tracking, building spatial memory, exploration, and long-term planning. In contrast, we propose a modular method with structured representations that (1) builds a semantic map of the scene and (2) performs exploration with a semantic search policy, to achieve the natural language goal. Our modular method achieves SOTA performance (24.46 %) with a substantial (8.17 % absolute) gap from previous work while using less data by eschewing both expert trajectories and low-level instructions. Leveraging low-level language, however, can further increase our performance (26.49 %). Our findings suggest that an explicit spatial memory and a semantic search policy can provide a stronger and more general representation for state-tracking and guidance, even in the absence of expert trajectories or low-level instructions.",
		"note": "arXiv:2110.07342 [cs]",
		"number": "arXiv:2110.07342",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "FILM: Following Instructions in Language with Modular Methods",
		"title-short": "FILM",
		"URL": "http://arxiv.org/abs/2110.07342",
		"author": [
			{
				"family": "Min",
				"given": "So Yeon"
			},
			{
				"family": "Chaplot",
				"given": "Devendra Singh"
			},
			{
				"family": "Ravikumar",
				"given": "Pradeep"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					16
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A7AA5BTC",
		"type": "article",
		"abstract": "While traditional methods for instruction-following typically assume prior linguistic and perceptual knowledge, many recent works in reinforcement learning (RL) have proposed learning policies end-to-end, typically by training neural networks to map joint representations of observations and instructions directly to actions. In this work, we present a novel framework for learning to perform temporally extended tasks using spatial reasoning in the RL framework, by sequentially imagining visual goals and choosing appropriate actions to fulfill imagined goals. Our framework operates on raw pixel images, assumes no prior linguistic or perceptual knowledge, and learns via intrinsic motivation and a single extrinsic reward signal measuring task completion. We validate our method in two environments with a robot arm in a simulated interactive 3D environment. Our method outperforms two flat architectures with raw-pixel and ground-truth states, and a hierarchical architecture with ground-truth states on object arrangement tasks.",
		"note": "arXiv:2001.09373 [cs, stat]",
		"number": "arXiv:2001.09373",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Following Instructions by Imagining and Reaching Visual Goals",
		"URL": "http://arxiv.org/abs/2001.09373",
		"author": [
			{
				"family": "Kanu",
				"given": "John"
			},
			{
				"family": "Dessalene",
				"given": "Eadom"
			},
			{
				"family": "Lin",
				"given": "Xiaomin"
			},
			{
				"family": "Fermuller",
				"given": "Cornelia"
			},
			{
				"family": "Aloimonos",
				"given": "Yiannis"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					1,
					25
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A9NQ2M9A",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
		"issue": "10",
		"note": "publisher: IEEE",
		"page": "7175–7189",
		"source": "Google Scholar",
		"title": "Adversarial reinforced instruction attacker for robust vision-language navigation",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9488322/?casa_token=I0trvI2C8AoAAAAA:h_-vskTGUh76SAr3T6Y5hJBHaNuEzWfj6sMSCeifDObh76LxoG49bNWoUfZcCi3ua8lWa3IhXOof",
		"volume": "44",
		"author": [
			{
				"family": "Lin",
				"given": "Bingqian"
			},
			{
				"family": "Zhu",
				"given": "Yi"
			},
			{
				"family": "Long",
				"given": "Yanxin"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			},
			{
				"family": "Ye",
				"given": "Qixiang"
			},
			{
				"family": "Lin",
				"given": "Liang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/Y9AC767D",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
		"page": "10730–10739",
		"source": "Google Scholar",
		"title": "Vision-dialog navigation by exploring cross-modal memory",
		"URL": "http://openaccess.thecvf.com/content_CVPR_2020/html/Zhu_Vision-Dialog_Navigation_by_Exploring_Cross-Modal_Memory_CVPR_2020_paper.html",
		"author": [
			{
				"family": "Zhu",
				"given": "Yi"
			},
			{
				"family": "Zhu",
				"given": "Fengda"
			},
			{
				"family": "Zhan",
				"given": "Zhaohuan"
			},
			{
				"family": "Lin",
				"given": "Bingqian"
			},
			{
				"family": "Jiao",
				"given": "Jianbin"
			},
			{
				"family": "Chang",
				"given": "Xiaojun"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BZQUNHV2",
		"type": "article-journal",
		"abstract": "Vision-and-Language Navigation (VLN) has been an emerging and fast-developing research topic, where an embodied agent is required to navigate in a real-world environment based on natural language instructions. In this article, we present a Direction-guided Navigator Agent (DNA) that novelly integrates direction clues derived from instructions into the essential encoder-decoder navigation framework. Particularly, DNA couples the standard instruction encoder with an additional direction branch which sequentially encodes the direction clues in the instructions to boost navigation. Furthermore, an Instruction Flipping mechanism is uniquely devised to enable fast data augmentation as well as a follow-up backtracing for navigating the agent in a backward direction. Such a way naturally amplifies the grounding of instruction in the local visual scenes along both forward and backward directions, and thus strengthens the alignment between instruction and action sequence. Extensive experiments conducted on Room to Room (R2R) dataset validate our proposal and demonstrate quantitatively compelling results.",
		"container-title": "ACM Transactions on Multimedia Computing, Communications, and Applications",
		"DOI": "10.1145/3526024",
		"ISSN": "1551-6857, 1551-6865",
		"issue": "1",
		"journalAbbreviation": "ACM Trans. Multimedia Comput. Commun. Appl.",
		"language": "en",
		"page": "1-16",
		"source": "DOI.org (Crossref)",
		"title": "Boosting Vision-and-Language Navigation with Direction Guiding and Backtracing",
		"URL": "https://dl.acm.org/doi/10.1145/3526024",
		"volume": "19",
		"author": [
			{
				"family": "Chen",
				"given": "Jingwen"
			},
			{
				"family": "Luo",
				"given": "Jianjie"
			},
			{
				"family": "Pan",
				"given": "Yingwei"
			},
			{
				"family": "Li",
				"given": "Yehao"
			},
			{
				"family": "Yao",
				"given": "Ting"
			},
			{
				"family": "Chao",
				"given": "Hongyang"
			},
			{
				"family": "Mei",
				"given": "Tao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					31
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4F9QEJNK",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "12014–12026",
		"source": "Google Scholar",
		"title": "HandMeThat: Human-Robot Communication in Physical and Social Environments",
		"title-short": "HandMeThat",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/4eb33c53ed5b14ce9028309431f565cc-Abstract-Datasets_and_Benchmarks.html",
		"volume": "35",
		"author": [
			{
				"family": "Wan",
				"given": "Yanming"
			},
			{
				"family": "Mao",
				"given": "Jiayuan"
			},
			{
				"family": "Tenenbaum",
				"given": "Josh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BD73KPIE",
		"type": "article-journal",
		"container-title": "Transactions of the Association for Computational Linguistics",
		"note": "publisher: MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info …",
		"page": "1303–1319",
		"source": "Google Scholar",
		"title": "Continual learning for grounded instruction generation by observing human following behavior",
		"URL": "https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00428/108610",
		"volume": "9",
		"author": [
			{
				"family": "Kojima",
				"given": "Noriyuki"
			},
			{
				"family": "Suhr",
				"given": "Alane"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/S22CA3BS",
		"type": "article",
		"abstract": "People always desire an embodied agent that can perform a task by understanding language instruction. Moreover, they also want to monitor and expect agents to understand commands the way they expected. But, how to build such an embodied agent is still unclear. Recently, people can explore this problem with the Vision-and-Language Interaction benchmark ALFRED, which requires an agent to perform complicated daily household tasks following natural language instructions in unseen scenes. In this paper, we propose LEBP -- Language Expectation and Binding Policy Module to tackle the ALFRED. The LEBP contains a two-stream process: 1) It first conducts a language expectation module to generate an expectation describing how to perform tasks by understanding the language instruction. The expectation consists of a sequence of sub-steps for the task (e.g., Pick an apple). The expectation allows people to access and check the understanding results of instructions before the agent takes actual actions, in case the task might go wrong. 2) Then, it uses the binding policy module to bind sub-steps in expectation to actual actions to specific scenarios. Actual actions include navigation and object manipulation. Experimental results suggest our approach achieves comparable performance to currently published SOTA methods and can avoid large decay from seen scenarios to unseen scenarios.",
		"note": "arXiv:2203.04637 [cs]",
		"number": "arXiv:2203.04637",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "LEBP -- Language Expectation & Binding Policy: A Two-Stream Framework for Embodied Vision-and-Language Interaction Task Learning Agents",
		"title-short": "LEBP -- Language Expectation & Binding Policy",
		"URL": "http://arxiv.org/abs/2203.04637",
		"author": [
			{
				"family": "Liu",
				"given": "Haoyu"
			},
			{
				"family": "Liu",
				"given": "Yang"
			},
			{
				"family": "He",
				"given": "Hongkai"
			},
			{
				"family": "Yang",
				"given": "Hangfang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IDE6A76E",
		"type": "article",
		"abstract": "Embodied instruction following is a challenging problem requiring an agent to infer a sequence of primitive actions to achieve a goal environment state from complex language and visual inputs. Action Learning From Realistic Environments and Directives (ALFRED) is a recently proposed benchmark for this problem consisting of step-by-step natural language instructions to achieve subgoals which compose to an ultimate high-level goal. Key challenges for this task include localizing target locations and navigating to them through visual inputs, and grounding language instructions to visual appearance of objects. To address these challenges, in this study, we augment the agent's field of view during navigation subgoals with multiple viewing angles, and train the agent to predict its relative spatial relation to the target location at each timestep. We also improve language grounding by introducing a pre-trained object detection module to the model pipeline. Empirical studies show that our approach exceeds the baseline model performance.",
		"note": "arXiv:2101.03431 [cs]",
		"number": "arXiv:2101.03431",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Are We There Yet? Learning to Localize in Embodied Instruction Following",
		"title-short": "Are We There Yet?",
		"URL": "http://arxiv.org/abs/2101.03431",
		"author": [
			{
				"family": "Storks",
				"given": "Shane"
			},
			{
				"family": "Gao",
				"given": "Qiaozi"
			},
			{
				"family": "Thattai",
				"given": "Govind"
			},
			{
				"family": "Tur",
				"given": "Gokhan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					1,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3WB8X9JS",
		"type": "article-journal",
		"container-title": "IEEE Robotics and Automation Letters",
		"issue": "99",
		"note": "publisher: IEEE",
		"page": "1–8",
		"source": "Google Scholar",
		"title": "PoSE: Suppressing Perceptual Noise in Embodied Agents for Enhanced Semantic Navigation",
		"title-short": "PoSE",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10295972/?casa_token=Lm8vsLOqaAIAAAAA:7aFvfr2LkD29GcUcE4B1ii9eaMqhBj0HicWcgtxZPGEMjjZPAVg8f71rXiKGW7dbuZcKB0hE3Ipt",
		"author": [
			{
				"family": "Zhuang",
				"given": "Benhui"
			},
			{
				"family": "Zhang",
				"given": "Chunhong"
			},
			{
				"family": "Hu",
				"given": "Zheng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A5J7TVSY",
		"type": "article",
		"abstract": "Whereas machine learning models typically learn language by directly training on language tasks (e.g., next-word prediction), language emerges in human children as a byproduct of solving non-language tasks (e.g., acquiring food). Motivated by this observation, we ask: can embodied reinforcement learning (RL) agents also indirectly learn language from non-language tasks? Learning to associate language with its meaning requires a dynamic environment with varied language. Therefore, we investigate this question in a multi-task environment with language that varies across the different tasks. Specifically, we design an office navigation environment, where the agent's goal is to find a particular office, and office locations differ in different buildings (i.e., tasks). Each building includes a floor plan with a simple language description of the goal office's location, which can be visually read as an RGB image when visited. We find RL agents indeed are able to indirectly learn language. Agents trained with current meta-RL algorithms successfully generalize to reading floor plans with held-out layouts and language phrases, and quickly navigate to the correct office, despite receiving no direct language supervision.",
		"note": "arXiv:2306.08400 [cs]",
		"number": "arXiv:2306.08400",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Simple Embodied Language Learning as a Byproduct of Meta-Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2306.08400",
		"author": [
			{
				"family": "Liu",
				"given": "Evan Zheran"
			},
			{
				"family": "Suri",
				"given": "Sahaana"
			},
			{
				"family": "Mu",
				"given": "Tong"
			},
			{
				"family": "Zhou",
				"given": "Allan"
			},
			{
				"family": "Finn",
				"given": "Chelsea"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					14
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KK8UA6ZI",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
		"page": "2998–3009",
		"source": "Google Scholar",
		"title": "Llm-planner: Few-shot grounded planning for embodied agents with large language models",
		"title-short": "Llm-planner",
		"URL": "https://openaccess.thecvf.com/content/ICCV2023/html/Song_LLM-Planner_Few-Shot_Grounded_Planning_for_Embodied_Agents_with_Large_Language_ICCV_2023_paper.html",
		"author": [
			{
				"family": "Song",
				"given": "Chan Hee"
			},
			{
				"family": "Wu",
				"given": "Jiaman"
			},
			{
				"family": "Washington",
				"given": "Clayton"
			},
			{
				"family": "Sadler",
				"given": "Brian M."
			},
			{
				"family": "Chao",
				"given": "Wei-Lun"
			},
			{
				"family": "Su",
				"given": "Yu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DEQXXV3F",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2020",
		"event-place": "Cham",
		"ISBN": "978-3-030-58451-1",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-030-58452-8_25",
		"page": "422-440",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes",
		"title-short": "ReferIt3D",
		"URL": "https://link.springer.com/10.1007/978-3-030-58452-8_25",
		"volume": "12346",
		"editor": [
			{
				"family": "Vedaldi",
				"given": "Andrea"
			},
			{
				"family": "Bischof",
				"given": "Horst"
			},
			{
				"family": "Brox",
				"given": "Thomas"
			},
			{
				"family": "Frahm",
				"given": "Jan-Michael"
			}
		],
		"author": [
			{
				"family": "Achlioptas",
				"given": "Panos"
			},
			{
				"family": "Abdelreheem",
				"given": "Ahmed"
			},
			{
				"family": "Xia",
				"given": "Fei"
			},
			{
				"family": "Elhoseiny",
				"given": "Mohamed"
			},
			{
				"family": "Guibas",
				"given": "Leonidas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BKFYGG96",
		"type": "article",
		"abstract": "There is a growing interest in the community in making an embodied AI agent perform a complicated task while interacting with an environment following natural language directives. Recent studies have tackled the problem using ALFRED, a well-designed dataset for the task, but achieved only very low accuracy. This paper proposes a new method, which outperforms the previous methods by a large margin. It is based on a combination of several new ideas. One is a two-stage interpretation of the provided instructions. The method first selects and interprets an instruction without using visual information, yielding a tentative action sequence prediction. It then integrates the prediction with the visual information etc., yielding the final prediction of an action and an object. As the object's class to interact is identified in the first stage, it can accurately select the correct object from the input image. Moreover, our method considers multiple egocentric views of the environment and extracts essential information by applying hierarchical attention conditioned on the current instruction. This contributes to the accurate prediction of actions for navigation. A preliminary version of the method won the ALFRED Challenge 2020. The current version achieves the unseen environment's success rate of 4.45% with a single view, which is further improved to 8.37% with multiple views.",
		"note": "arXiv:2106.00596 [cs]",
		"number": "arXiv:2106.00596",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Look Wide and Interpret Twice: Improving Performance on Interactive Instruction-following Tasks",
		"title-short": "Look Wide and Interpret Twice",
		"URL": "http://arxiv.org/abs/2106.00596",
		"author": [
			{
				"family": "Nguyen",
				"given": "Van-Quang"
			},
			{
				"family": "Suganuma",
				"given": "Masanori"
			},
			{
				"family": "Okatani",
				"given": "Takayuki"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					6
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/28Y4Z3EL",
		"type": "article",
		"abstract": "Learning to navigate in a visual environment following natural language instructions is a challenging task because natural language instructions are highly variable, ambiguous, and under-specified. In this paper, we present a novel training paradigm, Learn from EveryOne (LEO), which leverages multiple instructions (as different views) for the same trajectory to resolve language ambiguity and improve generalization. By sharing parameters across instructions, our approach learns more effectively from limited training data and generalizes better in unseen environments. On the recent Room-to-Room (R2R) benchmark dataset, LEO achieves 16% improvement (absolute) over a greedy agent as the base agent (25.3% $\\rightarrow$ 41.4%) in Success Rate weighted by Path Length (SPL). Further, LEO is complementary to most existing models for vision-and-language navigation, allowing for easy integration with the existing techniques, leading to LEO+, which creates the new state of the art, pushing the R2R benchmark to 62% (9% absolute improvement).",
		"note": "arXiv:2003.00857 [cs]",
		"number": "arXiv:2003.00857",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Multi-View Learning for Vision-and-Language Navigation",
		"URL": "http://arxiv.org/abs/2003.00857",
		"author": [
			{
				"family": "Xia",
				"given": "Qiaolin"
			},
			{
				"family": "Li",
				"given": "Xiujun"
			},
			{
				"family": "Li",
				"given": "Chunyuan"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Sui",
				"given": "Zhifang"
			},
			{
				"family": "Gao",
				"given": "Jianfeng"
			},
			{
				"family": "Choi",
				"given": "Yejin"
			},
			{
				"family": "Smith",
				"given": "Noah A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					3,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/TUWXWZEK",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "32340–32352",
		"source": "Google Scholar",
		"title": "Zson: Zero-shot object-goal navigation using multimodal goal embeddings",
		"title-short": "Zson",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/d0b8f0c8f79d3a621af945cafb669f4b-Abstract-Conference.html",
		"volume": "35",
		"author": [
			{
				"family": "Majumdar",
				"given": "Arjun"
			},
			{
				"family": "Aggarwal",
				"given": "Gunjan"
			},
			{
				"family": "Devnani",
				"given": "Bhavika"
			},
			{
				"family": "Hoffman",
				"given": "Judy"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/MYM7JKYY",
		"type": "paper-conference",
		"container-title": "2021 Tenth International Conference of Educational Innovation through Technology (EITT)",
		"page": "79–82",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "A Review of the Application of Artificial Intelligence in the Virtual Learning Environment",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9694079/?casa_token=-NvdsOM7GhEAAAAA:TeMpE2GBfBOMT8a9_qYFIMq5-ameKsIk_82PFfYNmLbaAKpO8sU499UV4utHSTD3rnX1CS6PQIND",
		"author": [
			{
				"family": "Wei",
				"given": "Xiaodong"
			},
			{
				"family": "Jia",
				"given": "Hongxue"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NEGVZ2L7",
		"type": "paper-conference",
		"container-title": "International Joint Conference on Artificial Intelligence 2023",
		"page": "1840–1848",
		"publisher": "Association for the Advancement of Artificial Intelligence (AAAI)",
		"source": "Google Scholar",
		"title": "Vision language navigation with Knowledge-driven Environmental Dreamer",
		"URL": "https://research.monash.edu/en/publications/vision-language-navigation-with-knowledge-driven-environmental-dr",
		"author": [
			{
				"family": "Zhu",
				"given": "Fengda"
			},
			{
				"family": "Lee",
				"given": "Vincent CS"
			},
			{
				"family": "Chang",
				"given": "Xiaojun"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/TG2JM9QG",
		"type": "article-journal",
		"container-title": "Interaction",
		"page": "4",
		"source": "Google Scholar",
		"title": "Multi-level Compositional Reasoning for Interactive Instruction Following",
		"URL": "https://ppolon.github.io/paper/aaai2023-alfred-mocha.pdf",
		"volume": "3",
		"author": [
			{
				"family": "Bhambri",
				"given": "Suvaansh"
			},
			{
				"family": "Kim",
				"given": "Byeonghwi"
			},
			{
				"family": "Choi",
				"given": "Jonghyun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AQ7RSHC8",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
		"page": "1644–1654",
		"source": "Google Scholar",
		"title": "Vision-language navigation with random environmental mixup",
		"URL": "http://openaccess.thecvf.com/content/ICCV2021/html/Liu_Vision-Language_Navigation_With_Random_Environmental_Mixup_ICCV_2021_paper.html",
		"author": [
			{
				"family": "Liu",
				"given": "Chong"
			},
			{
				"family": "Zhu",
				"given": "Fengda"
			},
			{
				"family": "Chang",
				"given": "Xiaojun"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			},
			{
				"family": "Ge",
				"given": "Zongyuan"
			},
			{
				"family": "Shen",
				"given": "Yi-Dong"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/363Z8D8J",
		"type": "article-journal",
		"container-title": "Applied Sciences",
		"issue": "14",
		"note": "publisher: MDPI",
		"page": "7053",
		"source": "Google Scholar",
		"title": "Incorporating External Knowledge Reasoning for Vision-and-Language Navigation with Assistant’s Help",
		"URL": "https://www.mdpi.com/2076-3417/12/14/7053",
		"volume": "12",
		"author": [
			{
				"family": "Li",
				"given": "Xin"
			},
			{
				"family": "Zhang",
				"given": "Yu"
			},
			{
				"family": "Yuan",
				"given": "Weilin"
			},
			{
				"family": "Luo",
				"given": "Junren"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EMX9GBBV",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments",
		"title-short": "Beyond the Nav-Graph",
		"URL": "https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123730103.pdf",
		"author": [
			{
				"family": "Wijmans",
				"given": "Jacob Krantz1 Erik"
			},
			{
				"family": "Batra",
				"given": "Arjun Majumdar2 Dhruv"
			},
			{
				"family": "Lee",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DMYWPF9Q",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "ACTRCE: Augmenting Experience via Teacher’s Advice",
		"title-short": "ACTRCE",
		"URL": "https://openreview.net/forum?id=HyM8V2A9Km",
		"author": [
			{
				"family": "Wu",
				"given": "Yuhuai"
			},
			{
				"family": "Chan",
				"given": "Harris"
			},
			{
				"family": "Kiros",
				"given": "Jamie"
			},
			{
				"family": "Fidler",
				"given": "Sanja"
			},
			{
				"family": "Ba",
				"given": "Jimmy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YZNG9WUY",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Pattern Analysis & Machine Intelligence",
		"issue": "01",
		"note": "publisher: IEEE Computer Society",
		"page": "1–1",
		"source": "Google Scholar",
		"title": "Retreat for advancing: Dynamic reinforced instruction attacker for robust visual navigation",
		"title-short": "Retreat for advancing",
		"URL": "https://www.computer.org/csdl/journal/tp/5555/01/09488322/1vhIavPG69W",
		"author": [
			{
				"family": "Lin",
				"given": "Bingqian"
			},
			{
				"family": "Zhu",
				"given": "Yi"
			},
			{
				"family": "Long",
				"given": "Yanxin"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			},
			{
				"family": "Ye",
				"given": "Qixiang"
			},
			{
				"family": "Lin",
				"given": "Liang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3ABREPE5",
		"type": "paper-conference",
		"container-title": "International Conference on Machine Learning",
		"page": "3875–3886",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Bootstrap latent-predictive representations for multitask reinforcement learning",
		"URL": "https://proceedings.mlr.press/v119/guo20g.html",
		"author": [
			{
				"family": "Guo",
				"given": "Zhaohan Daniel"
			},
			{
				"family": "Pires",
				"given": "Bernardo Avila"
			},
			{
				"family": "Piot",
				"given": "Bilal"
			},
			{
				"family": "Grill",
				"given": "Jean-Bastien"
			},
			{
				"family": "Altché",
				"given": "Florent"
			},
			{
				"family": "Munos",
				"given": "Rémi"
			},
			{
				"family": "Azar",
				"given": "Mohammad Gheshlaghi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LDU2GD5T",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "5296–5307",
		"source": "Google Scholar",
		"title": "Counterfactual vision-and-language navigation: Unravelling the unseen",
		"title-short": "Counterfactual vision-and-language navigation",
		"URL": "https://proceedings.neurips.cc/paper/2020/hash/39016cfe079db1bfb359ca72fcba3fd8-Abstract.html",
		"volume": "33",
		"author": [
			{
				"family": "Parvaneh",
				"given": "Amin"
			},
			{
				"family": "Abbasnejad",
				"given": "Ehsan"
			},
			{
				"family": "Teney",
				"given": "Damien"
			},
			{
				"family": "Shi",
				"given": "Javen Qinfeng"
			},
			{
				"family": "Van den Hengel",
				"given": "Anton"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9ABMEQUJ",
		"type": "article",
		"abstract": "We introduce a method for following high-level navigation instructions by mapping directly from images, instructions and pose estimates to continuous low-level velocity commands for real-time control. The Grounded Semantic Mapping Network (GSMN) is a fully-differentiable neural network architecture that builds an explicit semantic map in the world reference frame by incorporating a pinhole camera projection model within the network. The information stored in the map is learned from experience, while the local-to-world transformation is computed explicitly. We train the model using DAggerFM, a modified variant of DAgger that trades tabular convergence guarantees for improved training speed and memory use. We test GSMN in virtual environments on a realistic quadcopter simulator and show that incorporating an explicit mapping and grounding modules allows GSMN to outperform strong neural baselines and almost reach an expert policy performance. Finally, we analyze the learned map representations and show that using an explicit map leads to an interpretable instruction-following model.",
		"note": "arXiv:1806.00047 [cs]",
		"number": "arXiv:1806.00047",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Following High-level Navigation Instructions on a Simulated Quadcopter with Imitation Learning",
		"URL": "http://arxiv.org/abs/1806.00047",
		"author": [
			{
				"family": "Blukis",
				"given": "Valts"
			},
			{
				"family": "Brukhim",
				"given": "Nataly"
			},
			{
				"family": "Bennett",
				"given": "Andrew"
			},
			{
				"family": "Knepper",
				"given": "Ross A."
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					31
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EMK3PQPK",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Interpretable Agent for Language-guided 3D Indoor Navigation",
		"URL": "https://davidsonic.github.io/summary/EE546Final-JialiDuan.pdf",
		"author": [
			{
				"family": "Duan",
				"given": "Jiali"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/38S4UHZD",
		"type": "article",
		"abstract": "End-to-end Transformers have demonstrated an impressive success rate for Embodied Instruction Following when the environment has been seen in training. However, they tend to struggle when deployed in an unseen environment. This lack of generalizability is due to the agent's insensitivity to subtle changes in natural language instructions. To mitigate this issue, we propose explicitly aligning the agent's hidden states with the instructions via contrastive learning. Nevertheless, the semantic gap between high-level language instructions and the agent's low-level action space remains an obstacle. Therefore, we further introduce a novel concept of meta-actions to bridge the gap. Meta-actions are ubiquitous action patterns that can be parsed from the original action sequence. These patterns represent higher-level semantics that are intuitively aligned closer to the instructions. When meta-actions are applied as additional training signals, the agent generalizes better to unseen environments. Compared to a strong multi-modal Transformer baseline, we achieve a significant 4.5% absolute gain in success rate in unseen environments of ALFRED Embodied Instruction Following. Additional analysis shows that the contrastive objective and meta-actions are complementary in achieving the best results, and the resulting agent better aligns its states with corresponding instructions, making it more suitable for real-world embodied agents. The code is available at: https://github.com/joeyy5588/LACMA.",
		"note": "arXiv:2310.12344 [cs]",
		"number": "arXiv:2310.12344",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following",
		"title-short": "LACMA",
		"URL": "http://arxiv.org/abs/2310.12344",
		"author": [
			{
				"family": "Yang",
				"given": "Cheng-Fu"
			},
			{
				"family": "Chen",
				"given": "Yen-Chun"
			},
			{
				"family": "Yang",
				"given": "Jianwei"
			},
			{
				"family": "Dai",
				"given": "Xiyang"
			},
			{
				"family": "Yuan",
				"given": "Lu"
			},
			{
				"family": "Wang",
				"given": "Yu-Chiang Frank"
			},
			{
				"family": "Chang",
				"given": "Kai-Wei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					18
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/THYNS9S9",
		"type": "article",
		"abstract": "In order to successfully perform tasks specified by natural language instructions, an artificial agent operating in a visual world needs to map words, concepts, and actions from the instruction to visual elements in its environment. This association is termed as Task-Oriented Grounding. In this work, we propose a novel Dynamic Attention Network architecture for the efficient multi-modal fusion of text and visual representations which can generate a robust definition of state for the policy learner. Our model assumes no prior knowledge from visual and textual domains and is an end to end trainable. For a 3D visual world where the observation changes continuously, the attention on the visual elements tends to be highly co-related from a one-time step to the next. We term this as \"Dynamic Attention\". In this work, we show that Dynamic Attention helps in achieving grounding and also aids in the policy learning objective. Since most practical robotic applications take place in the real world where the observation space is continuous, our framework can be used as a generalized multi-modal fusion unit for robotic control through natural language. We show the effectiveness of using 1D convolution over Gated Attention Hadamard product on the rate of convergence of the network. We demonstrate that the cell-state of a Long Short Term Memory (LSTM) is a natural choice for modeling Dynamic Attention and shows through visualization that the generated attention is very close to how humans tend to focus on the environment.",
		"note": "arXiv:1910.06315 [cs]",
		"number": "arXiv:1910.06315",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Dynamic Attention Networks for Task Oriented Grounding",
		"URL": "http://arxiv.org/abs/1910.06315",
		"author": [
			{
				"family": "Dasgupta",
				"given": "Soumik"
			},
			{
				"family": "Patro",
				"given": "Badri N."
			},
			{
				"family": "Namboodiri",
				"given": "Vinay P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					14
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZRZI9CMG",
		"type": "article",
		"abstract": "Navigation is a rich and well-grounded problem domain that drives progress in many different areas of research: perception, planning, memory, exploration, and optimisation in particular. Historically these challenges have been separately considered and solutions built that rely on stationary datasets - for example, recorded trajectories through an environment. These datasets cannot be used for decision-making and reinforcement learning, however, and in general the perspective of navigation as an interactive learning task, where the actions and behaviours of a learning agent are learned simultaneously with the perception and planning, is relatively unsupported. Thus, existing navigation benchmarks generally rely on static datasets (Geiger et al., 2013; Kendall et al., 2015) or simulators (Beattie et al., 2016; Shah et al., 2018). To support and validate research in end-to-end navigation, we present StreetLearn: an interactive, first-person, partially-observed visual environment that uses Google Street View for its photographic content and broad coverage, and give performance baselines for a challenging goal-driven navigation task. The environment code, baseline agent code, and the dataset are available at http://streetlearn.cc",
		"note": "arXiv:1903.01292 [cs]",
		"number": "arXiv:1903.01292",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The StreetLearn Environment and Dataset",
		"URL": "http://arxiv.org/abs/1903.01292",
		"author": [
			{
				"family": "Mirowski",
				"given": "Piotr"
			},
			{
				"family": "Banki-Horvath",
				"given": "Andras"
			},
			{
				"family": "Anderson",
				"given": "Keith"
			},
			{
				"family": "Teplyashin",
				"given": "Denis"
			},
			{
				"family": "Hermann",
				"given": "Karl Moritz"
			},
			{
				"family": "Malinowski",
				"given": "Mateusz"
			},
			{
				"family": "Grimes",
				"given": "Matthew Koichi"
			},
			{
				"family": "Simonyan",
				"given": "Karen"
			},
			{
				"family": "Kavukcuoglu",
				"given": "Koray"
			},
			{
				"family": "Zisserman",
				"given": "Andrew"
			},
			{
				"family": "Hadsell",
				"given": "Raia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					3,
					4
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9EHRIH5D",
		"type": "article-journal",
		"container-title": "Frontiers in Robotics and AI",
		"note": "publisher: Frontiers",
		"page": "930486",
		"source": "Google Scholar",
		"title": "A modular vision language navigation and manipulation framework for long horizon compositional tasks in indoor environment",
		"URL": "https://www.frontiersin.org/articles/10.3389/frobt.2022.930486/full",
		"volume": "9",
		"author": [
			{
				"family": "Saha",
				"given": "Homagni"
			},
			{
				"family": "Fotouhi",
				"given": "Fateme"
			},
			{
				"family": "Liu",
				"given": "Qisai"
			},
			{
				"family": "Sarkar",
				"given": "Soumik"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IRI5VVHS",
		"type": "article",
		"abstract": "The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at https://github.com/chihyaoma/selfmonitoring-agent .",
		"note": "arXiv:1901.03035 [cs]",
		"number": "arXiv:1901.03035",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Self-Monitoring Navigation Agent via Auxiliary Progress Estimation",
		"URL": "http://arxiv.org/abs/1901.03035",
		"author": [
			{
				"family": "Ma",
				"given": "Chih-Yao"
			},
			{
				"family": "Lu",
				"given": "Jiasen"
			},
			{
				"family": "Wu",
				"given": "Zuxuan"
			},
			{
				"family": "AlRegib",
				"given": "Ghassan"
			},
			{
				"family": "Kira",
				"given": "Zsolt"
			},
			{
				"family": "Socher",
				"given": "Richard"
			},
			{
				"family": "Xiong",
				"given": "Caiming"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					1,
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VQVBM9B7",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "UC Santa Barbara",
		"source": "Google Scholar",
		"title": "Learning Natural Language Interfaces using Deep Neural Networks",
		"URL": "https://escholarship.org/content/qt70r066q9/qt70r066q9.pdf",
		"author": [
			{
				"family": "Gur",
				"given": "Izzeddin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NFNRK7H3",
		"type": "paper-conference",
		"container-title": "Conference on Robot Learning",
		"page": "394–406",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Vision-and-dialog navigation",
		"URL": "http://proceedings.mlr.press/v100/thomason20a.html",
		"author": [
			{
				"family": "Thomason",
				"given": "Jesse"
			},
			{
				"family": "Murray",
				"given": "Michael"
			},
			{
				"family": "Cakmak",
				"given": "Maya"
			},
			{
				"family": "Zettlemoyer",
				"given": "Luke"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KDQ7WCSQ",
		"type": "article",
		"abstract": "Vision-and-language navigation (VLN) is a task in which an agent is embodied in a realistic 3D environment and follows an instruction to reach the goal node. While most of the previous studies have built and investigated a discriminative approach, we notice that there are in fact two possible approaches to building such a VLN agent: discriminative \\textit{and} generative. In this paper, we design and investigate a generative language-grounded policy which uses a language model to compute the distribution over all possible instructions i.e. all possible sequences of vocabulary tokens given action and the transition history. In experiments, we show that the proposed generative approach outperforms the discriminative approach in the Room-2-Room (R2R) and Room-4-Room (R4R) datasets, especially in the unseen environments. We further show that the combination of the generative and discriminative policies achieves close to the state-of-the art results in the R2R dataset, demonstrating that the generative and discriminative policies capture the different aspects of VLN.",
		"note": "arXiv:2009.07783 [cs]",
		"number": "arXiv:2009.07783",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Generative Language-Grounded Policy in Vision-and-Language Navigation with Bayes' Rule",
		"URL": "http://arxiv.org/abs/2009.07783",
		"author": [
			{
				"family": "Kurita",
				"given": "Shuhei"
			},
			{
				"family": "Cho",
				"given": "Kyunghyun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					8
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NMJVYZGJ",
		"type": "article-journal",
		"container-title": "Tsinghua Science and Technology",
		"issue": "5",
		"note": "publisher: TUP",
		"page": "674–691",
		"source": "Google Scholar",
		"title": "Deep reinforcement learning based mobile robot navigation: A review",
		"title-short": "Deep reinforcement learning based mobile robot navigation",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9409758/",
		"volume": "26",
		"author": [
			{
				"family": "Zhu",
				"given": "Kai"
			},
			{
				"family": "Zhang",
				"given": "Tao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2LUN8TG5",
		"type": "paper-conference",
		"container-title": "2023 25th International Conference on Advanced Communication Technology (ICACT)",
		"page": "424–428",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Improving Embodied Instruction Following with Deterministic Methods",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10079273/?casa_token=r7mE_8iCu4sAAAAA:lWEiVGIIl8wkjHtOrV7XQwxvnnGQEeKb3cc2uXBSIwf17EhsVyVO8tEDfjVxVTwab57ZVNfnb_2C",
		"author": [
			{
				"family": "Kim",
				"given": "Dahyun"
			},
			{
				"family": "Lee",
				"given": "Yong-Ju"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/P2AW7W2E",
		"type": "article-journal",
		"container-title": "Artificial Intelligence Review",
		"issue": "1",
		"note": "publisher: Springer",
		"page": "365–427",
		"source": "Google Scholar",
		"title": "Visual language navigation: A survey and open challenges",
		"title-short": "Visual language navigation",
		"URL": "https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10462-022-10174-9&casa_token=x2EQYPQFBUQAAAAA:HkET9ADu3cjXinNIN3Nmda0Njd4OMMok5UZqTVIVOOok0PhGYQtt0ud1NoN1PcCwGJ48uhpHRP1JixgFmJA",
		"volume": "56",
		"author": [
			{
				"family": "Park",
				"given": "Sang-Min"
			},
			{
				"family": "Kim",
				"given": "Young-Gab"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FLU68AXS",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF international conference on computer vision",
		"page": "9339–9347",
		"source": "Google Scholar",
		"title": "Habitat: A platform for embodied ai research",
		"title-short": "Habitat",
		"URL": "http://openaccess.thecvf.com/content_ICCV_2019/html/Savva_Habitat_A_Platform_for_Embodied_AI_Research_ICCV_2019_paper.html",
		"author": [
			{
				"family": "Savva",
				"given": "Manolis"
			},
			{
				"family": "Kadian",
				"given": "Abhishek"
			},
			{
				"family": "Maksymets",
				"given": "Oleksandr"
			},
			{
				"family": "Zhao",
				"given": "Yili"
			},
			{
				"family": "Wijmans",
				"given": "Erik"
			},
			{
				"family": "Jain",
				"given": "Bhavana"
			},
			{
				"family": "Straub",
				"given": "Julian"
			},
			{
				"family": "Liu",
				"given": "Jia"
			},
			{
				"family": "Koltun",
				"given": "Vladlen"
			},
			{
				"family": "Malik",
				"given": "Jitendra"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/65W7U5N4",
		"type": "article-journal",
		"container-title": "Artificial Intelligence Review",
		"issue": "4",
		"note": "publisher: Springer",
		"page": "3711–3753",
		"source": "Google Scholar",
		"title": "A review of platforms for simulating embodied agents in 3D virtual environments",
		"URL": "https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10462-022-10253-x&casa_token=X9mrRljJEC8AAAAA:oKwYFFXf2wX9Gxh2of7i-O_7RuRujb9dPmrYGW7HigGqWfUNhufG953WoJirbtwPWat0baOk6uLNDkNqejg",
		"volume": "56",
		"author": [
			{
				"family": "Kaur",
				"given": "Deepti Prit"
			},
			{
				"family": "Singh",
				"given": "Narinder Pal"
			},
			{
				"family": "Banerjee",
				"given": "Bonny"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/27JLH9FU",
		"type": "paper-conference",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"note": "issue: 17",
		"page": "15695–15703",
		"source": "Google Scholar",
		"title": "Applied machine learning for games: A graduate school course",
		"title-short": "Applied machine learning for games",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/17849",
		"volume": "35",
		"author": [
			{
				"family": "Zeng",
				"given": "Yilei"
			},
			{
				"family": "Shah",
				"given": "Aayush"
			},
			{
				"family": "Thai",
				"given": "Jameson"
			},
			{
				"family": "Zyda",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2ZF77PUD",
		"type": "paper-conference",
		"container-title": "International Conference on Machine Learning",
		"page": "8096–8108",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Interactive learning from activity description",
		"URL": "https://proceedings.mlr.press/v139/nguyen21e.html",
		"author": [
			{
				"family": "Nguyen",
				"given": "Khanh X."
			},
			{
				"family": "Misra",
				"given": "Dipendra"
			},
			{
				"family": "Schapire",
				"given": "Robert"
			},
			{
				"family": "Dudík",
				"given": "Miroslav"
			},
			{
				"family": "Shafto",
				"given": "Patrick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/UCKZ56EI",
		"type": "paper-conference",
		"container-title": "International Conference on Machine Learning",
		"page": "9118–9147",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Language models as zero-shot planners: Extracting actionable knowledge for embodied agents",
		"title-short": "Language models as zero-shot planners",
		"URL": "https://proceedings.mlr.press/v162/huang22a.html",
		"author": [
			{
				"family": "Huang",
				"given": "Wenlong"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Pathak",
				"given": "Deepak"
			},
			{
				"family": "Mordatch",
				"given": "Igor"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RPBL3UAF",
		"type": "article-journal",
		"container-title": "Neural Processing Letters",
		"issue": "5",
		"note": "publisher: Springer",
		"page": "3979–3998",
		"source": "Google Scholar",
		"title": "Improving target-driven visual navigation with attention on 3D spatial relationships",
		"URL": "https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11063-022-10796-8&casa_token=vEiE7vIjGh0AAAAA:VFfphjp2A65WaFRJvRGlR0Bt6M64YKWVlKkoYlOjgZpCmTXFTlG7Awnn_rc55K04Yb85uvEFP63B0AGPjow",
		"volume": "54",
		"author": [
			{
				"family": "Lyu",
				"given": "Yunlian"
			},
			{
				"family": "Shi",
				"given": "Yimin"
			},
			{
				"family": "Zhang",
				"given": "Xianggang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RFJ768C8",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2022",
		"event-place": "Cham",
		"ISBN": "978-3-031-20058-8",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-031-20059-5_39",
		"page": "682-699",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "FedVLN: Privacy-Preserving Federated Vision-and-Language Navigation",
		"title-short": "FedVLN",
		"URL": "https://link.springer.com/10.1007/978-3-031-20059-5_39",
		"volume": "13696",
		"editor": [
			{
				"family": "Avidan",
				"given": "Shai"
			},
			{
				"family": "Brostow",
				"given": "Gabriel"
			},
			{
				"family": "Cissé",
				"given": "Moustapha"
			},
			{
				"family": "Farinella",
				"given": "Giovanni Maria"
			},
			{
				"family": "Hassner",
				"given": "Tal"
			}
		],
		"author": [
			{
				"family": "Zhou",
				"given": "Kaiwen"
			},
			{
				"family": "Wang",
				"given": "Xin Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EKDFL98L",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "2783–2795",
		"source": "Google Scholar",
		"title": "Goal-aware cross-entropy for multi-target reinforcement learning",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/165a59f7cf3b5c4396ba65953d679f17-Abstract.html",
		"volume": "34",
		"author": [
			{
				"family": "Kim",
				"given": "Kibeom"
			},
			{
				"family": "Lee",
				"given": "Min Whoo"
			},
			{
				"family": "Kim",
				"given": "Yoonsung"
			},
			{
				"family": "Ryu",
				"given": "JeHwan"
			},
			{
				"family": "Lee",
				"given": "Minsu"
			},
			{
				"family": "Zhang",
				"given": "Byoung-Tak"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SCVX6I8Q",
		"type": "paper-conference",
		"container-title": "ViGIL@ NeurIPS",
		"source": "Google Scholar",
		"title": "Natural Language Grounded Multitask Navigation.",
		"URL": "https://vigilworkshop.github.io/static/papers-2019/13.pdf",
		"author": [
			{
				"family": "Wang",
				"given": "Xin"
			},
			{
				"family": "Jain",
				"given": "Vihan"
			},
			{
				"family": "Ie",
				"given": "Eugene"
			},
			{
				"family": "Wang",
				"given": "William Yang"
			},
			{
				"family": "Kozareva",
				"given": "Zornitsa"
			},
			{
				"family": "Ravi",
				"given": "Sujith"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YXUJWZND",
		"type": "paper-conference",
		"container-title": "2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
		"page": "5877–5884",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Learning to act with affordance-aware multimodal neural slam",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9981261/?casa_token=t3yhSAJsSekAAAAA:c9jBz53ixcMhKcYzWz4Pz6xctZSZPTYlGecAEmXpuZtHjio4vv4eQxyHH3VpmJTC6ycS4iJqZ0tq",
		"author": [
			{
				"family": "Jia",
				"given": "Zhiwei"
			},
			{
				"family": "Lin",
				"given": "Kaixiang"
			},
			{
				"family": "Zhao",
				"given": "Yizhou"
			},
			{
				"family": "Gao",
				"given": "Qiaozi"
			},
			{
				"family": "Thattai",
				"given": "Govind"
			},
			{
				"family": "Sukhatme",
				"given": "Gaurav S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZFXVM254",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2020",
		"event-place": "Cham",
		"ISBN": "978-3-030-58538-9",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-030-58539-6_2",
		"page": "17-36",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "SoundSpaces: Audio-Visual Navigation in 3D Environments",
		"title-short": "SoundSpaces",
		"URL": "https://link.springer.com/10.1007/978-3-030-58539-6_2",
		"volume": "12351",
		"editor": [
			{
				"family": "Vedaldi",
				"given": "Andrea"
			},
			{
				"family": "Bischof",
				"given": "Horst"
			},
			{
				"family": "Brox",
				"given": "Thomas"
			},
			{
				"family": "Frahm",
				"given": "Jan-Michael"
			}
		],
		"author": [
			{
				"family": "Chen",
				"given": "Changan"
			},
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Schissler",
				"given": "Carl"
			},
			{
				"family": "Gari",
				"given": "Sebastia Vicenc Amengual"
			},
			{
				"family": "Al-Halah",
				"given": "Ziad"
			},
			{
				"family": "Ithapu",
				"given": "Vamsi Krishna"
			},
			{
				"family": "Robinson",
				"given": "Philip"
			},
			{
				"family": "Grauman",
				"given": "Kristen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DSFZ4ERM",
		"type": "article-journal",
		"container-title": "Journal of Artificial Intelligence Research",
		"page": "459–515",
		"source": "Google Scholar",
		"title": "Core challenges in embodied vision-language planning",
		"URL": "https://www.jair.org/index.php/jair/article/view/13646",
		"volume": "74",
		"author": [
			{
				"family": "Francis",
				"given": "Jonathan"
			},
			{
				"family": "Kitamura",
				"given": "Nariaki"
			},
			{
				"family": "Labelle",
				"given": "Felix"
			},
			{
				"family": "Lu",
				"given": "Xiaopeng"
			},
			{
				"family": "Navarro",
				"given": "Ingrid"
			},
			{
				"family": "Oh",
				"given": "Jean"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/R2FLMMEK",
		"type": "article-journal",
		"container-title": "Advanced Robotics",
		"DOI": "10.1080/01691864.2019.1632223",
		"ISSN": "0169-1864, 1568-5535",
		"issue": "15-16",
		"journalAbbreviation": "Advanced Robotics",
		"language": "en",
		"page": "700-730",
		"source": "DOI.org (Crossref)",
		"title": "Survey on frontiers of language and robotics",
		"URL": "https://www.tandfonline.com/doi/full/10.1080/01691864.2019.1632223",
		"volume": "33",
		"author": [
			{
				"family": "Taniguchi",
				"given": "T."
			},
			{
				"family": "Mochihashi",
				"given": "D."
			},
			{
				"family": "Nagai",
				"given": "T."
			},
			{
				"family": "Uchida",
				"given": "S."
			},
			{
				"family": "Inoue",
				"given": "N."
			},
			{
				"family": "Kobayashi",
				"given": "I."
			},
			{
				"family": "Nakamura",
				"given": "T."
			},
			{
				"family": "Hagiwara",
				"given": "Y."
			},
			{
				"family": "Iwahashi",
				"given": "N."
			},
			{
				"family": "Inamura",
				"given": "T."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					8,
					18
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZTL947L4",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
		"page": "14738–14748",
		"source": "Google Scholar",
		"title": "Pathdreamer: A world model for indoor navigation",
		"title-short": "Pathdreamer",
		"URL": "http://openaccess.thecvf.com/content/ICCV2021/html/Koh_Pathdreamer_A_World_Model_for_Indoor_Navigation_ICCV_2021_paper.html",
		"author": [
			{
				"family": "Koh",
				"given": "Jing Yu"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			},
			{
				"family": "Yang",
				"given": "Yinfei"
			},
			{
				"family": "Baldridge",
				"given": "Jason"
			},
			{
				"family": "Anderson",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XRKVPI78",
		"type": "article",
		"abstract": "Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The active exploration tasks require the robot to extensively explore an unknown environment within a limited number of actions. Meanwhile, the embodied question answering tasks necessitate that the robot answers questions based on observations acquired during prior explorations.",
		"note": "arXiv:2304.09349 [cs]",
		"number": "arXiv:2304.09349",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "LLM as A Robotic Brain: Unifying Egocentric Memory and Control",
		"title-short": "LLM as A Robotic Brain",
		"URL": "http://arxiv.org/abs/2304.09349",
		"author": [
			{
				"family": "Mai",
				"given": "Jinjie"
			},
			{
				"family": "Chen",
				"given": "Jun"
			},
			{
				"family": "Li",
				"given": "Bing"
			},
			{
				"family": "Qian",
				"given": "Guocheng"
			},
			{
				"family": "Elhoseiny",
				"given": "Mohamed"
			},
			{
				"family": "Ghanem",
				"given": "Bernard"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/STM5ZW39",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Language-modulated Actions using Deep Reinforcement Learning for Safer Human-Robot Interaction",
		"URL": "http://www.socrates-project.eu/internal-documents/SSR_2018/SSR_2018_paper_22.pdf",
		"author": [
			{
				"family": "Zamani",
				"given": "Mohammad Ali"
			},
			{
				"family": "Magg",
				"given": "Sven"
			},
			{
				"family": "Weber",
				"given": "Cornelius"
			},
			{
				"family": "Wermter",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LHK8US8I",
		"type": "article",
		"abstract": "Text-based games offer a challenging test bed to evaluate virtual agents at language understanding, multi-step problem-solving, and common-sense reasoning. However, speed is a major limitation of current text-based games, capping at 300 steps per second, mainly due to the use of legacy tooling. In this work we present TextWorldExpress, a high-performance simulator that includes implementations of three common text game benchmarks that increases simulation throughput by approximately three orders of magnitude, reaching over one million steps per second on common desktop hardware. This significantly reduces experiment runtime, enabling billion-step-scale experiments in about one day.",
		"note": "arXiv:2208.01174 [cs]",
		"number": "arXiv:2208.01174",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "TextWorldExpress: Simulating Text Games at One Million Steps Per Second",
		"title-short": "TextWorldExpress",
		"URL": "http://arxiv.org/abs/2208.01174",
		"author": [
			{
				"family": "Jansen",
				"given": "Peter A."
			},
			{
				"family": "Côté",
				"given": "Marc-Alexandre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/L25ZCU5N",
		"type": "article",
		"abstract": "Federated embodied agent learning protects the data privacy of individual visual environments by keeping data locally at each client (the individual environment) during training. However, since the local data is inaccessible to the server under federated learning, attackers may easily poison the training data of the local client to build a backdoor in the agent without notice. Deploying such an agent raises the risk of potential harm to humans, as the attackers may easily navigate and control the agent as they wish via the backdoor. Towards Byzantine-robust federated embodied agent learning, in this paper, we study the attack and defense for the task of vision-and-language navigation (VLN), where the agent is required to follow natural language instructions to navigate indoor environments. First, we introduce a simple but effective attack strategy, Navigation as Wish (NAW), in which the malicious client manipulates local trajectory data to implant a backdoor into the global model. Results on two VLN datasets (R2R and RxR) show that NAW can easily navigate the deployed VLN agent regardless of the language instruction, without affecting its performance on normal test sets. Then, we propose a new Prompt-Based Aggregation (PBA) to defend against the NAW attack in federated VLN, which provides the server with a ''prompt'' of the vision-and-language alignment variance between the benign and malicious clients so that they can be distinguished during training. We validate the effectiveness of the PBA method on protecting the global model from the NAW attack, which outperforms other state-of-the-art defense methods by a large margin in the defense metrics on R2R and RxR.",
		"note": "arXiv:2211.14769 [cs]",
		"number": "arXiv:2211.14769",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Navigation as Attackers Wish? Towards Building Byzantine-Robust Embodied Agents under Federated Learning",
		"title-short": "Navigation as Attackers Wish?",
		"URL": "http://arxiv.org/abs/2211.14769",
		"author": [
			{
				"family": "Zhang",
				"given": "Yunchao"
			},
			{
				"family": "Di",
				"given": "Zonglin"
			},
			{
				"family": "Zhou",
				"given": "Kaiwen"
			},
			{
				"family": "Xie",
				"given": "Cihang"
			},
			{
				"family": "Wang",
				"given": "Xin Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					14
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/73WD3JT2",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
		"page": "16898–16907",
		"source": "Google Scholar",
		"title": "Visual navigation with spatial attention",
		"URL": "http://openaccess.thecvf.com/content/CVPR2021/html/Mayo_Visual_Navigation_With_Spatial_Attention_CVPR_2021_paper.html",
		"author": [
			{
				"family": "Mayo",
				"given": "Bar"
			},
			{
				"family": "Hazan",
				"given": "Tamir"
			},
			{
				"family": "Tal",
				"given": "Ayellet"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/V6WCWK9R",
		"type": "article",
		"abstract": "Vision-and-Language Navigation (VLN) aims to develop intelligent agents to navigate in unseen environments only through language and vision supervision. In the recently proposed continuous settings (continuous VLN), the agent must act in a free 3D space and faces tougher challenges like real-time execution, complex instruction understanding, and long action sequence prediction. For a better performance in continuous VLN, we design a multi-level instruction understanding procedure and propose a novel model, Multi-Level Attention Network (MLANet). The first step of MLANet is to generate sub-instructions efficiently. We design a Fast Sub-instruction Algorithm (FSA) to segment the raw instruction into sub-instructions and generate a new sub-instruction dataset named ``FSASub\". FSA is annotation-free and faster than the current method by 70 times, thus fitting the real-time requirement in continuous VLN. To solve the complex instruction understanding problem, MLANet needs a global perception of the instruction and observations. We propose a Multi-Level Attention (MLA) module to fuse vision, low-level semantics, and high-level semantics, which produce features containing a dynamic and global comprehension of the task. MLA also mitigates the adverse effects of noise words, thus ensuring a robust understanding of the instruction. To correctly predict actions in long trajectories, MLANet needs to focus on what sub-instruction is being executed every step. We propose a Peak Attention Loss (PAL) to improve the flexible and adaptive selection of the current sub-instruction. PAL benefits the navigation agent by concentrating its attention on the local information, thus helping the agent predict the most appropriate actions. We train and test MLANet in the standard benchmark. Experiment results show MLANet outperforms baselines by a significant margin.",
		"note": "arXiv:2303.01396 [cs]",
		"number": "arXiv:2303.01396",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "MLANet: Multi-Level Attention Network with Sub-instruction for Continuous Vision-and-Language Navigation",
		"title-short": "MLANet",
		"URL": "http://arxiv.org/abs/2303.01396",
		"author": [
			{
				"family": "He",
				"given": "Zongtao"
			},
			{
				"family": "Wang",
				"given": "Liuyi"
			},
			{
				"family": "Li",
				"given": "Shu"
			},
			{
				"family": "Yan",
				"given": "Qingqing"
			},
			{
				"family": "Liu",
				"given": "Chengju"
			},
			{
				"family": "Chen",
				"given": "Qijun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JJFHVU7W",
		"type": "article-journal",
		"container-title": "Mathematics",
		"issue": "19",
		"note": "publisher: MDPI",
		"page": "4192",
		"source": "Google Scholar",
		"title": "Self-Organizing Memory Based on Adaptive Resonance Theory for Vision and Language Navigation",
		"URL": "https://www.mdpi.com/2227-7390/11/19/4192",
		"volume": "11",
		"author": [
			{
				"family": "Wu",
				"given": "Wansen"
			},
			{
				"family": "Hu",
				"given": "Yue"
			},
			{
				"family": "Xu",
				"given": "Kai"
			},
			{
				"family": "Qin",
				"given": "Long"
			},
			{
				"family": "Yin",
				"given": "Quanjun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GIUSPXKD",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2022",
		"event-place": "Cham",
		"ISBN": "978-3-031-19841-0",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-031-19842-7_21",
		"page": "355-373",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Housekeep: Tidying Virtual Households Using Commonsense Reasoning",
		"title-short": "Housekeep",
		"URL": "https://link.springer.com/10.1007/978-3-031-19842-7_21",
		"volume": "13699",
		"editor": [
			{
				"family": "Avidan",
				"given": "Shai"
			},
			{
				"family": "Brostow",
				"given": "Gabriel"
			},
			{
				"family": "Cissé",
				"given": "Moustapha"
			},
			{
				"family": "Farinella",
				"given": "Giovanni Maria"
			},
			{
				"family": "Hassner",
				"given": "Tal"
			}
		],
		"author": [
			{
				"family": "Kant",
				"given": "Yash"
			},
			{
				"family": "Ramachandran",
				"given": "Arun"
			},
			{
				"family": "Yenamandra",
				"given": "Sriram"
			},
			{
				"family": "Gilitschenski",
				"given": "Igor"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			},
			{
				"family": "Szot",
				"given": "Andrew"
			},
			{
				"family": "Agrawal",
				"given": "Harsh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8T4FPIWC",
		"type": "article",
		"abstract": "We seek to create agents that both act and communicate with other agents in pursuit of a goal. Towards this end, we extend LIGHT (Urbanek et al. 2019) -- a large-scale crowd-sourced fantasy text-game -- with a dataset of quests. These contain natural language motivations paired with in-game goals and human demonstrations; completing a quest might require dialogue or actions (or both). We introduce a reinforcement learning system that (1) incorporates large-scale language modeling-based and commonsense reasoning-based pre-training to imbue the agent with relevant priors; and (2) leverages a factorized action space of action commands and dialogue, balancing between the two. We conduct zero-shot evaluations using held-out human expert demonstrations, showing that our agents are able to act consistently and talk naturally with respect to their motivations.",
		"note": "arXiv:2010.00685 [cs]",
		"number": "arXiv:2010.00685",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "How to Motivate Your Dragon: Teaching Goal-Driven Agents to Speak and Act in Fantasy Worlds",
		"title-short": "How to Motivate Your Dragon",
		"URL": "http://arxiv.org/abs/2010.00685",
		"author": [
			{
				"family": "Ammanabrolu",
				"given": "Prithviraj"
			},
			{
				"family": "Urbanek",
				"given": "Jack"
			},
			{
				"family": "Li",
				"given": "Margaret"
			},
			{
				"family": "Szlam",
				"given": "Arthur"
			},
			{
				"family": "Rocktäschel",
				"given": "Tim"
			},
			{
				"family": "Weston",
				"given": "Jason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					5,
					25
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7M7URYFT",
		"type": "article",
		"abstract": "Generalist robot manipulators need to learn a wide variety of manipulation skills across diverse environments. Current robot training pipelines rely on humans to provide kinesthetic demonstrations or to program simulation environments and to code up reward functions for reinforcement learning. Such human involvement is an important bottleneck towards scaling up robot learning across diverse tasks and environments. We propose Generation to Simulation (Gen2Sim), a method for scaling up robot skill learning in simulation by automating generation of 3D assets, task descriptions, task decompositions and reward functions using large pre-trained generative models of language and vision. We generate 3D assets for simulation by lifting open-world 2D object-centric images to 3D using image diffusion models and querying LLMs to determine plausible physics parameters. Given URDF files of generated and human-developed assets, we chain-of-thought prompt LLMs to map these to relevant task descriptions, temporal decompositions, and corresponding python reward functions for reinforcement learning. We show Gen2Sim succeeds in learning policies for diverse long horizon tasks, where reinforcement learning with non temporally decomposed reward functions fails. Gen2Sim provides a viable path for scaling up reinforcement learning for robot manipulators in simulation, both by diversifying and expanding task and environment development, and by facilitating the discovery of reinforcement-learned behaviors through temporal task decomposition in RL. Our work contributes hundreds of simulated assets, tasks and demonstrations, taking a step towards fully autonomous robotic manipulation skill acquisition in simulation.",
		"note": "arXiv:2310.18308 [cs]",
		"number": "arXiv:2310.18308",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Gen2Sim: Scaling up Robot Learning in Simulation with Generative Models",
		"title-short": "Gen2Sim",
		"URL": "http://arxiv.org/abs/2310.18308",
		"author": [
			{
				"family": "Katara",
				"given": "Pushkal"
			},
			{
				"family": "Xian",
				"given": "Zhou"
			},
			{
				"family": "Fragkiadaki",
				"given": "Katerina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/T9BT2LKB",
		"type": "article",
		"abstract": "Pre-trained and frozen LLMs can effectively map simple scene re-arrangement instructions to programs over a robot's visuomotor functions through appropriate few-shot example prompting. To parse open-domain natural language and adapt to a user's idiosyncratic procedures, not known during prompt engineering time, fixed prompts fall short. In this paper, we introduce HELPER, an embodied agent equipped with an external memory of language-program pairs that parses free-form human-robot dialogue into action programs through retrieval-augmented LLM prompting: relevant memories are retrieved based on the current dialogue, instruction, correction or VLM description, and used as in-context prompt examples for LLM querying. The memory is expanded during deployment to include pairs of user's language and action plans, to assist future inferences and personalize them to the user's language and routines. HELPER sets a new state-of-the-art in the TEACh benchmark in both Execution from Dialog History (EDH) and Trajectory from Dialogue (TfD), with 1.7x improvement over the previous SOTA for TfD. Our models, code and video results can be found in our project's website: https://helper-agent-llm.github.io.",
		"note": "arXiv:2310.15127 [cs]",
		"number": "arXiv:2310.15127",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models",
		"URL": "http://arxiv.org/abs/2310.15127",
		"author": [
			{
				"family": "Sarch",
				"given": "Gabriel"
			},
			{
				"family": "Wu",
				"given": "Yue"
			},
			{
				"family": "Tarr",
				"given": "Michael J."
			},
			{
				"family": "Fragkiadaki",
				"given": "Katerina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					23
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5W65NCFK",
		"type": "paper-conference",
		"container-title": "Proceedings of Second International Combined Workshop on Spatial Language Understanding and Grounded Communication for Robotics",
		"page": "11–21",
		"source": "Google Scholar",
		"title": "Learning to Read Maps: Understanding Natural Language Instructions from Unseen Maps",
		"title-short": "Learning to Read Maps",
		"URL": "https://aclanthology.org/2021.splurobonlp-1.2/",
		"author": [
			{
				"family": "Katsakioris",
				"given": "Miltiadis Marios"
			},
			{
				"family": "Konstas",
				"given": "Ioannis"
			},
			{
				"family": "Mignotte",
				"given": "Pierre Yves"
			},
			{
				"family": "Hastie",
				"given": "Helen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/I5MBZE5Q",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Embodied Symbiotic Assistants that See, Act, Infer and Chat",
		"URL": "https://assets.amazon.science/1f/f0/68cdf857406eaca75a4baf2b5fcc/symbiote-simbotreport.pdf",
		"author": [
			{
				"family": "Cao",
				"given": "Yuchen"
			},
			{
				"family": "Pande",
				"given": "Nilay"
			},
			{
				"family": "Jain",
				"given": "Ayush"
			},
			{
				"family": "Sharma",
				"given": "Shikhar"
			},
			{
				"family": "Sarch",
				"given": "Gabriel"
			},
			{
				"family": "Gkanatsios",
				"given": "Nikolaos"
			},
			{
				"family": "Zhou",
				"given": "Xian"
			},
			{
				"family": "Fragkiadaki",
				"given": "Katerina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8KWRG35L",
		"type": "article",
		"abstract": "We introduce BridgeData V2, a large and diverse dataset of robotic manipulation behaviors designed to facilitate research on scalable robot learning. BridgeData V2 contains 60,096 trajectories collected across 24 environments on a publicly available low-cost robot. BridgeData V2 provides extensive task and environment variability, leading to skills that can generalize across environments, domains, and institutions, making the dataset a useful resource for a broad range of researchers. Additionally, the dataset is compatible with a wide variety of open-vocabulary, multi-task learning methods conditioned on goal images or natural language instructions. In our experiments, we train 6 state-of-the-art imitation learning and offline reinforcement learning methods on our dataset, and find that they succeed on a suite of tasks requiring varying amounts of generalization. We also demonstrate that the performance of these methods improves with more data and higher capacity models, and that training on a greater variety of skills leads to improved generalization. By publicly sharing BridgeData V2 and our pre-trained models, we aim to accelerate research in scalable robot learning methods. Project page at https://rail-berkeley.github.io/bridgedata",
		"note": "arXiv:2308.12952 [cs]",
		"number": "arXiv:2308.12952",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "BridgeData V2: A Dataset for Robot Learning at Scale",
		"title-short": "BridgeData V2",
		"URL": "http://arxiv.org/abs/2308.12952",
		"author": [
			{
				"family": "Walke",
				"given": "Homer"
			},
			{
				"family": "Black",
				"given": "Kevin"
			},
			{
				"family": "Lee",
				"given": "Abraham"
			},
			{
				"family": "Kim",
				"given": "Moo Jin"
			},
			{
				"family": "Du",
				"given": "Max"
			},
			{
				"family": "Zheng",
				"given": "Chongyi"
			},
			{
				"family": "Zhao",
				"given": "Tony"
			},
			{
				"family": "Hansen-Estruch",
				"given": "Philippe"
			},
			{
				"family": "Vuong",
				"given": "Quan"
			},
			{
				"family": "He",
				"given": "Andre"
			},
			{
				"family": "Myers",
				"given": "Vivek"
			},
			{
				"family": "Fang",
				"given": "Kuan"
			},
			{
				"family": "Finn",
				"given": "Chelsea"
			},
			{
				"family": "Levine",
				"given": "Sergey"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					21
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5R62BJTM",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "14911–14920",
		"source": "Google Scholar",
		"title": "Adaptive Zone-Aware Hierarchical Planner for Vision-Language Navigation",
		"URL": "http://openaccess.thecvf.com/content/CVPR2023/html/Gao_Adaptive_Zone-Aware_Hierarchical_Planner_for_Vision-Language_Navigation_CVPR_2023_paper.html",
		"author": [
			{
				"family": "Gao",
				"given": "Chen"
			},
			{
				"family": "Peng",
				"given": "Xingyu"
			},
			{
				"family": "Yan",
				"given": "Mi"
			},
			{
				"family": "Wang",
				"given": "He"
			},
			{
				"family": "Yang",
				"given": "Lirong"
			},
			{
				"family": "Ren",
				"given": "Haibing"
			},
			{
				"family": "Li",
				"given": "Hongsheng"
			},
			{
				"family": "Liu",
				"given": "Si"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/I4K8XMB9",
		"type": "article",
		"abstract": "We present the IGLU Gridworld: a reinforcement learning environment for building and evaluating language conditioned embodied agents in a scalable way. The environment features visual agent embodiment, interactive learning through collaboration, language conditioned RL, and combinatorically hard task (3d blocks building) space.",
		"note": "arXiv:2206.00142 [cs]",
		"number": "arXiv:2206.00142",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "IGLU Gridworld: Simple and Fast Environment for Embodied Dialog Agents",
		"title-short": "IGLU Gridworld",
		"URL": "http://arxiv.org/abs/2206.00142",
		"author": [
			{
				"family": "Zholus",
				"given": "Artem"
			},
			{
				"family": "Skrynnik",
				"given": "Alexey"
			},
			{
				"family": "Mohanty",
				"given": "Shrestha"
			},
			{
				"family": "Volovikova",
				"given": "Zoya"
			},
			{
				"family": "Kiseleva",
				"given": "Julia"
			},
			{
				"family": "Szlam",
				"given": "Artur"
			},
			{
				"family": "Coté",
				"given": "Marc-Alexandre"
			},
			{
				"family": "Panov",
				"given": "Aleksandr I."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					31
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZK8MSUCA",
		"type": "paper-conference",
		"container-title": "2023 IEEE International Conference on Robotics and Automation (ICRA)",
		"page": "7635–7641",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Embodied Referring Expression for Manipulation Question Answering in Interactive Environment",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10160748/?casa_token=cuhePHwV_0IAAAAA:_KSLdtiuDNVGOXTKDkJG9Kim2-9-XZwoG52psmKMJIgHSEP365k7177Uon8HwWRnxJcj4VCCrNOy",
		"author": [
			{
				"family": "Sima",
				"given": "Qie"
			},
			{
				"family": "Tan",
				"given": "Sinan"
			},
			{
				"family": "Liu",
				"given": "Huaping"
			},
			{
				"family": "Sun",
				"given": "Fuchun"
			},
			{
				"family": "Xu",
				"given": "Weifeng"
			},
			{
				"family": "Fu",
				"given": "Ling"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9PSRERQI",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "35880–35893",
		"source": "Google Scholar",
		"title": "Palm up: Playing in the Latent Manifold for Unsupervised Pretraining",
		"title-short": "Palm up",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/e92381dba235a8309f08ce46376189a9-Abstract-Conference.html",
		"volume": "35",
		"author": [
			{
				"family": "Liu",
				"given": "Hao"
			},
			{
				"family": "Zahavy",
				"given": "Tom"
			},
			{
				"family": "Mnih",
				"given": "Volodymyr"
			},
			{
				"family": "Singh",
				"given": "Satinder"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NDNY7ESJ",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
		"page": "10968–10980",
		"source": "Google Scholar",
		"title": "Bird's-Eye-View Scene Graph for Vision-Language Navigation",
		"URL": "http://openaccess.thecvf.com/content/ICCV2023/html/Liu_Birds-Eye-View_Scene_Graph_for_Vision-Language_Navigation_ICCV_2023_paper.html",
		"author": [
			{
				"family": "Liu",
				"given": "Rui"
			},
			{
				"family": "Wang",
				"given": "Xiaohan"
			},
			{
				"family": "Wang",
				"given": "Wenguan"
			},
			{
				"family": "Yang",
				"given": "Yi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/L8GAFD9Y",
		"type": "article",
		"abstract": "Pre-trained large language models have shown successful progress in many language understanding benchmarks. This work explores the capability of these models to predict actionable plans in real-world environments. Given a text instruction, we show that language priors encoded in pre-trained language models allow us to infer fine-grained subgoal sequences. In contrast to recent methods which make strong assumptions about subgoal supervision, our experiments show that language models can infer detailed subgoal sequences from few training sequences without any fine-tuning. We further propose a simple strategy to re-rank language model predictions based on interaction and feedback from the environment. Combined with pre-trained navigation and visual reasoning components, our approach demonstrates competitive performance on subgoal prediction and task completion in the ALFRED benchmark compared to prior methods that assume more subgoal supervision.",
		"note": "arXiv:2205.14288 [cs]",
		"number": "arXiv:2205.14288",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Few-shot Subgoal Planning with Language Models",
		"URL": "http://arxiv.org/abs/2205.14288",
		"author": [
			{
				"family": "Logeswaran",
				"given": "Lajanugen"
			},
			{
				"family": "Fu",
				"given": "Yao"
			},
			{
				"family": "Lee",
				"given": "Moontae"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RZI8NRN2",
		"type": "article-journal",
		"container-title": "AGILE: GIScience Series",
		"note": "publisher: Copernicus Publications Göttingen, Germany",
		"page": "2",
		"source": "Google Scholar",
		"title": "From Floorplan to Navigation Concepts: Automatic Generation of Text-based Games",
		"title-short": "From Floorplan to Navigation Concepts",
		"URL": "https://agile-giss.copernicus.org/articles/4/2/2023/",
		"volume": "4",
		"author": [
			{
				"family": "Arabsheibani",
				"given": "Reza"
			},
			{
				"family": "Hamzei",
				"given": "Ehsan"
			},
			{
				"family": "Amoozandeh",
				"given": "Kimia"
			},
			{
				"family": "Winter",
				"given": "Stephan"
			},
			{
				"family": "Tomko",
				"given": "Martin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9UTX6PH2",
		"type": "article",
		"abstract": "High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. Experimentally, various RL algorithms obtain significant improvement in performance and training speed when assisted by our design.",
		"note": "arXiv:2302.04449 [cs]",
		"number": "arXiv:2302.04449",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals",
		"title-short": "Read and Reap the Rewards",
		"URL": "http://arxiv.org/abs/2302.04449",
		"author": [
			{
				"family": "Wu",
				"given": "Yue"
			},
			{
				"family": "Fan",
				"given": "Yewen"
			},
			{
				"family": "Liang",
				"given": "Paul Pu"
			},
			{
				"family": "Azaria",
				"given": "Amos"
			},
			{
				"family": "Li",
				"given": "Yuanzhi"
			},
			{
				"family": "Mitchell",
				"given": "Tom M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					26
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KHX9IWLX",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "A Curiosity-Driven Approach for Generating Textual Descriptions of Environments",
		"URL": "https://www2.eecs.berkeley.edu/Pubs/TechRpts/2021/EECS-2021-142.pdf",
		"author": [
			{
				"family": "Zhang",
				"given": "Xinyun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BI87GSQY",
		"type": "article",
		"abstract": "Large Language Models (LLMs) have demonstrated impressive planning abilities in single-agent embodied tasks across various domains. However, their capacity for planning and communication in multi-agent cooperation remains unclear, even though these are crucial skills for intelligent embodied agents. In this paper, we present a novel framework that utilizes LLMs for multi-agent cooperation and tests it in various embodied environments. Our framework enables embodied agents to plan, communicate, and cooperate with other embodied agents or humans to accomplish long-horizon tasks efficiently. We demonstrate that recent LLMs, such as GPT-4, can surpass strong planning-based methods and exhibit emergent effective communication using our framework without requiring fine-tuning or few-shot prompting. We also discover that LLM-based agents that communicate in natural language can earn more trust and cooperate more effectively with humans. Our research underscores the potential of LLMs for embodied AI and lays the foundation for future research in multi-agent cooperation. Videos can be found on the project website https://vis-www.cs.umass.edu/Co-LLM-Agents/.",
		"note": "arXiv:2307.02485 [cs]",
		"number": "arXiv:2307.02485",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Building Cooperative Embodied Agents Modularly with Large Language Models",
		"URL": "http://arxiv.org/abs/2307.02485",
		"author": [
			{
				"family": "Zhang",
				"given": "Hongxin"
			},
			{
				"family": "Du",
				"given": "Weihua"
			},
			{
				"family": "Shan",
				"given": "Jiaming"
			},
			{
				"family": "Zhou",
				"given": "Qinhong"
			},
			{
				"family": "Du",
				"given": "Yilun"
			},
			{
				"family": "Tenenbaum",
				"given": "Joshua B."
			},
			{
				"family": "Shu",
				"given": "Tianmin"
			},
			{
				"family": "Gan",
				"given": "Chuang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JHYWLABJ",
		"type": "article-journal",
		"container-title": "IEEE Design & Test",
		"issue": "3",
		"note": "publisher: IEEE",
		"page": "37–44",
		"source": "Google Scholar",
		"title": "A hardware accelerator for language-guided reinforcement learning",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9367213/?casa_token=_XYCtxxb1NAAAAAA:-xPG-A6X99EqNbTwUmOhC8u3jbusosLmFkl_uegPcjuTo-FswvTw-Pc1NzSqkePFidk-FgBlAvCW",
		"volume": "39",
		"author": [
			{
				"family": "Shiri",
				"given": "Aidin"
			},
			{
				"family": "Mazumder",
				"given": "Arnab Neelim"
			},
			{
				"family": "Prakash",
				"given": "Bharat"
			},
			{
				"family": "Homayoun",
				"given": "Houman"
			},
			{
				"family": "Waytowich",
				"given": "Nicholas R."
			},
			{
				"family": "Mohsenin",
				"given": "Tinoosh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KSGXPZJY",
		"type": "article",
		"abstract": "Embodiment is an important characteristic for all intelligent agents (creatures and robots), while existing scene description tasks mainly focus on analyzing images passively and the semantic understanding of the scenario is separated from the interaction between the agent and the environment. In this work, we propose the Embodied Scene Description, which exploits the embodiment ability of the agent to find an optimal viewpoint in its environment for scene description tasks. A learning framework with the paradigms of imitation learning and reinforcement learning is established to teach the intelligent agent to generate corresponding sensorimotor activities. The proposed framework is tested on both the AI2Thor dataset and a real world robotic platform demonstrating the effectiveness and extendability of the developed method.",
		"note": "arXiv:2004.14638 [cs]",
		"number": "arXiv:2004.14638",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards Embodied Scene Description",
		"URL": "http://arxiv.org/abs/2004.14638",
		"author": [
			{
				"family": "Tan",
				"given": "Sinan"
			},
			{
				"family": "Liu",
				"given": "Huaping"
			},
			{
				"family": "Guo",
				"given": "Di"
			},
			{
				"family": "Zhang",
				"given": "Xinyu"
			},
			{
				"family": "Sun",
				"given": "Fuchun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					5,
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/D6SRYN2H",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Emerging Topics in Computational Intelligence",
		"issue": "2",
		"note": "publisher: IEEE",
		"page": "230–244",
		"source": "Google Scholar",
		"title": "A survey of embodied ai: From simulators to research tasks",
		"title-short": "A survey of embodied ai",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9687596/?casa_token=C9FgGA1swOMAAAAA:X7eIpKlLaAMzuDwPd9HIE-aJV8avujDFH367NERkPv-2sMmjYPtz5XDjFdo_g96oJ4aRjHMw0uaS",
		"volume": "6",
		"author": [
			{
				"family": "Duan",
				"given": "Jiafei"
			},
			{
				"family": "Yu",
				"given": "Samson"
			},
			{
				"family": "Tan",
				"given": "Hui Li"
			},
			{
				"family": "Zhu",
				"given": "Hongyuan"
			},
			{
				"family": "Tan",
				"given": "Cheston"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/H2477EVL",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Pontificia Universidad Catolica de Chile (Chile)",
		"source": "Google Scholar",
		"title": "Object attention and contextualization for vision and language navigation",
		"URL": "https://search.proquest.com/openview/001768f003c4e96a812edc5657d6dbf5/1?pq-origsite=gscholar&cbl=2026366&diss=y",
		"author": [
			{
				"family": "Earle",
				"given": "Benjamín"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VHDJQ85R",
		"type": "article",
		"abstract": "Despite recent breakthroughs in reinforcement learning (RL) and imitation learning (IL), existing algorithms fail to generalize beyond the training environments. In reality, humans can adapt to new tasks quickly by leveraging prior knowledge about the world such as language descriptions. To facilitate the research on language-guided agents with domain adaption, we propose a novel zero-shot compositional policy learning task, where the environments are characterized as a composition of different attributes. Since there are no public environments supporting this study, we introduce a new research platform BabyAI++ in which the dynamics of environments are disentangled from visual appearance. At each episode, BabyAI++ provides varied vision-dynamics combinations along with corresponding descriptive texts. To evaluate the adaption capability of learned agents, a set of vision-dynamics pairings are held-out for testing on BabyAI++. Unsurprisingly, we find that current language-guided RL/IL techniques overfit to the training environments and suffer from a huge performance drop when facing unseen combinations. In response, we propose a multi-modal fusion method with an attention mechanism to perform visual language-grounding. Extensive experiments show strong evidence that language grounding is able to improve the generalization of agents across environments with varied dynamics.",
		"note": "arXiv:2004.07200 [cs, stat]",
		"number": "arXiv:2004.07200",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Zero-Shot Compositional Policy Learning via Language Grounding",
		"URL": "http://arxiv.org/abs/2004.07200",
		"author": [
			{
				"family": "Cao",
				"given": "Tianshi"
			},
			{
				"family": "Wang",
				"given": "Jingkang"
			},
			{
				"family": "Zhang",
				"given": "Yining"
			},
			{
				"family": "Manivasagam",
				"given": "Sivabalan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					17
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DQWDAL3F",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "36858–36874",
		"source": "Google Scholar",
		"title": "Towards versatile embodied navigation",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/ef4f2a0232a246b8a502135175e08953-Abstract-Conference.html",
		"volume": "35",
		"author": [
			{
				"family": "Wang",
				"given": "Hanqing"
			},
			{
				"family": "Liang",
				"given": "Wei"
			},
			{
				"family": "Gool",
				"given": "Luc V."
			},
			{
				"family": "Wang",
				"given": "Wenguan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SV4T6NG9",
		"type": "article",
		"abstract": "Often times, we specify tasks for a robot using temporal language that can also span different levels of abstraction. The example command ``go to the kitchen before going to the second floor'' contains spatial abstraction, given that ``floor'' consists of individual rooms that can also be referred to in isolation (\"kitchen\", for example). There is also a temporal ordering of events, defined by the word \"before\". Previous works have used Linear Temporal Logic (LTL) to interpret temporal language (such as \"before\"), and Abstract Markov Decision Processes (AMDPs) to interpret hierarchical abstractions (such as \"kitchen\" and \"second floor\"), separately. To handle both types of commands at once, we introduce the Abstract Product Markov Decision Process (AP-MDP), a novel approach capable of representing non-Markovian reward functions at different levels of abstractions. The AP-MDP framework translates LTL into its corresponding automata, creates a product Markov Decision Process (MDP) of the LTL specification and the environment MDP, and decomposes the problem into subproblems to enable efficient planning with abstractions. AP-MDP performs faster than a non-hierarchical method of solving LTL problems in over 95% of tasks, and this number only increases as the size of the environment domain increases. We also present a neural sequence-to-sequence model trained to translate language commands into LTL expression, and a new corpus of non-Markovian language commands spanning different levels of abstraction. We test our framework with the collected language commands on a drone, demonstrating that our approach enables a robot to efficiently solve temporal commands at different levels of abstraction.",
		"note": "arXiv:1905.12096 [cs]",
		"number": "arXiv:1905.12096",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Planning with State Abstractions for Non-Markovian Task Specifications",
		"URL": "http://arxiv.org/abs/1905.12096",
		"author": [
			{
				"family": "Oh",
				"given": "Yoonseon"
			},
			{
				"family": "Patel",
				"given": "Roma"
			},
			{
				"family": "Nguyen",
				"given": "Thao"
			},
			{
				"family": "Huang",
				"given": "Baichuan"
			},
			{
				"family": "Pavlick",
				"given": "Ellie"
			},
			{
				"family": "Tellex",
				"given": "Stefanie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					5,
					28
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7N9Q5PV5",
		"type": "article",
		"abstract": "As a fundamental problem for Artificial Intelligence, multi-agent system (MAS) is making rapid progress, mainly driven by multi-agent reinforcement learning (MARL) techniques. However, previous MARL methods largely focused on grid-world like or game environments; MAS in visually rich environments has remained less explored. To narrow this gap and emphasize the crucial role of perception in MAS, we propose a large-scale 3D dataset, CollaVN, for multi-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed to cooperatively navigate across photo-realistic environments to reach target locations. Diverse MAVN variants are explored to make our problem more general. Moreover, a memory-augmented communication framework is proposed. Each agent is equipped with a private, external memory to persistently store communication information. This allows agents to make better use of their past communication information, enabling more efficient collaboration and robust long-term planning. In our experiments, several baselines and evaluation metrics are designed. We also empirically verify the efficacy of our proposed MARL approach across different MAVN task settings.",
		"note": "arXiv:2107.01151 [cs]",
		"number": "arXiv:2107.01151",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Collaborative Visual Navigation",
		"URL": "http://arxiv.org/abs/2107.01151",
		"author": [
			{
				"family": "Wang",
				"given": "Haiyang"
			},
			{
				"family": "Wang",
				"given": "Wenguan"
			},
			{
				"family": "Zhu",
				"given": "Xizhou"
			},
			{
				"family": "Dai",
				"given": "Jifeng"
			},
			{
				"family": "Wang",
				"given": "Liwei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6VXLRFEU",
		"type": "article-journal",
		"container-title": "environment",
		"page": "103",
		"source": "Google Scholar",
		"title": "Audio-visual embodied navigation",
		"URL": "https://www.researchgate.net/profile/Sebastia-Amengual-Gari-2/publication/338158203_Audio-Visual_Embodied_Navigation/links/5e29fb2ba6fdcc70a14651eb/Audio-Visual-Embodied-Navigation.pdf",
		"volume": "97",
		"author": [
			{
				"family": "Chen",
				"given": "Changan"
			},
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Schissler",
				"given": "Carl"
			},
			{
				"family": "Gari",
				"given": "Sebastia Vicenc Amengual"
			},
			{
				"family": "Al-Halah",
				"given": "Ziad"
			},
			{
				"family": "Ithapu",
				"given": "Vamsi Krishna"
			},
			{
				"family": "Robinson",
				"given": "Philip"
			},
			{
				"family": "Grauman",
				"given": "Kristen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HB4UBBJU",
		"type": "article",
		"abstract": "Our goal is to create an interactive natural language interface that efficiently and reliably learns from users to complete tasks in simulated robotics settings. We introduce a neural semantic parsing system that learns new high-level abstractions through decomposition: users interactively teach the system by breaking down high-level utterances describing novel behavior into low-level steps that it can understand. Unfortunately, existing methods either rely on grammars which parse sentences with limited flexibility, or neural sequence-to-sequence models that do not learn efficiently or reliably from individual examples. Our approach bridges this gap, demonstrating the flexibility of modern neural systems, as well as the one-shot reliable generalization of grammar-based methods. Our crowdsourced interactive experiments suggest that over time, users complete complex tasks more efficiently while using our system by leveraging what they just taught. At the same time, getting users to trust the system enough to be incentivized to teach high-level utterances is still an ongoing challenge. We end with a discussion of some of the obstacles we need to overcome to fully realize the potential of the interactive paradigm.",
		"note": "arXiv:2010.05190 [cs]",
		"number": "arXiv:2010.05190",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning Adaptive Language Interfaces through Decomposition",
		"URL": "http://arxiv.org/abs/2010.05190",
		"author": [
			{
				"family": "Karamcheti",
				"given": "Siddharth"
			},
			{
				"family": "Sadigh",
				"given": "Dorsa"
			},
			{
				"family": "Liang",
				"given": "Percy"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NABSDHCD",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
		"page": "1675–1685",
		"source": "Google Scholar",
		"title": "Env-qa: A video question answering benchmark for comprehensive understanding of dynamic environments",
		"title-short": "Env-qa",
		"URL": "http://openaccess.thecvf.com/content/ICCV2021/html/Gao_Env-QA_A_Video_Question_Answering_Benchmark_for_Comprehensive_Understanding_of_ICCV_2021_paper.html",
		"author": [
			{
				"family": "Gao",
				"given": "Difei"
			},
			{
				"family": "Wang",
				"given": "Ruiping"
			},
			{
				"family": "Bai",
				"given": "Ziyi"
			},
			{
				"family": "Chen",
				"given": "Xilin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IDTGSX6Y",
		"type": "paper-conference",
		"container-title": "Proceedings of the Thirteenth Language Resources and Evaluation Conference",
		"page": "6232–6239",
		"source": "Google Scholar",
		"title": "Task-Driven and Experience-Based Question Answering Corpus for In-Home Robot Application in the House3D Virtual Environment",
		"URL": "https://aclanthology.org/2022.lrec-1.670/",
		"author": [
			{
				"family": "Xu",
				"given": "Zhuoqun"
			},
			{
				"family": "Ouyang",
				"given": "Liubo"
			},
			{
				"family": "Liu",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YU6ZPSDS",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Cognitively Coherent Human-Computer Communication",
		"URL": "https://apps.dtic.mil/sti/pdfs/AD1186569.pdf",
		"author": [
			{
				"family": "Hockenmaier",
				"given": "Julia"
			},
			{
				"family": "URBANA-CHAMPAIGN",
				"given": "ILLINOIS UNIV AT"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EDFSCB8D",
		"type": "paper-conference",
		"container-title": "ViGIL@ NeurIPS",
		"source": "Google Scholar",
		"title": "Learning from Observation-Only Demonstration for Task-Oriented Language Grounding via Self-Examination.",
		"URL": "https://tsujuifu.github.io/pubs/neuripsw19_ood.pdf",
		"author": [
			{
				"family": "Fu",
				"given": "Tsu-Jui"
			},
			{
				"family": "Tsuboi",
				"given": "Yuta"
			},
			{
				"family": "Kobayashi",
				"given": "Sosuke"
			},
			{
				"family": "Kikuchi",
				"given": "Yuta"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RE9AHP6U",
		"type": "article",
		"abstract": "Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.",
		"note": "arXiv:2205.06175 [cs]",
		"number": "arXiv:2205.06175",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Generalist Agent",
		"URL": "http://arxiv.org/abs/2205.06175",
		"author": [
			{
				"family": "Reed",
				"given": "Scott"
			},
			{
				"family": "Zolna",
				"given": "Konrad"
			},
			{
				"family": "Parisotto",
				"given": "Emilio"
			},
			{
				"family": "Colmenarejo",
				"given": "Sergio Gomez"
			},
			{
				"family": "Novikov",
				"given": "Alexander"
			},
			{
				"family": "Barth-Maron",
				"given": "Gabriel"
			},
			{
				"family": "Gimenez",
				"given": "Mai"
			},
			{
				"family": "Sulsky",
				"given": "Yury"
			},
			{
				"family": "Kay",
				"given": "Jackie"
			},
			{
				"family": "Springenberg",
				"given": "Jost Tobias"
			},
			{
				"family": "Eccles",
				"given": "Tom"
			},
			{
				"family": "Bruce",
				"given": "Jake"
			},
			{
				"family": "Razavi",
				"given": "Ali"
			},
			{
				"family": "Edwards",
				"given": "Ashley"
			},
			{
				"family": "Heess",
				"given": "Nicolas"
			},
			{
				"family": "Chen",
				"given": "Yutian"
			},
			{
				"family": "Hadsell",
				"given": "Raia"
			},
			{
				"family": "Vinyals",
				"given": "Oriol"
			},
			{
				"family": "Bordbar",
				"given": "Mahyar"
			},
			{
				"family": "Freitas",
				"given": "Nando",
				"non-dropping-particle": "de"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QQ77IENL",
		"type": "article-journal",
		"container-title": "Frontiers in psychology",
		"note": "publisher: Frontiers Media SA",
		"page": "716671",
		"source": "Google Scholar",
		"title": "The embodied crossmodal self forms language and interaction: a computational cognitive review",
		"title-short": "The embodied crossmodal self forms language and interaction",
		"URL": "https://www.frontiersin.org/articles/10.3389/fpsyg.2021.716671/full",
		"volume": "12",
		"author": [
			{
				"family": "Röder",
				"given": "Frank"
			},
			{
				"family": "Özdemir",
				"given": "Ozan"
			},
			{
				"family": "Nguyen",
				"given": "Phuong DH"
			},
			{
				"family": "Wermter",
				"given": "Stefan"
			},
			{
				"family": "Eppe",
				"given": "Manfred"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7SK97H5B",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Mohit Shridhar1 Jesse Thomason1 Daniel Gordon1 Yonatan Bisk1, 2, 3",
		"URL": "https://www.researchgate.net/profile/Mohit-Shridhar/publication/337756597_ALFRED_A_Benchmark_for_Interpreting_Grounded_Instructions_for_Everyday_Tasks/links/5df13bc592851c836475e05b/ALFRED-A-Benchmark-for-Interpreting-Grounded-Instructions-for-Everyday-Tasks.pdf",
		"author": [
			{
				"family": "Han",
				"given": "Winson"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Zettlemoyer",
				"given": "Luke"
			},
			{
				"family": "Fox",
				"given": "Dieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EUDQU29D",
		"type": "article-journal",
		"container-title": "Frontiers in Robotics and AI",
		"note": "publisher: Frontiers Media SA",
		"source": "Google Scholar",
		"title": "Learning to reason over scene graphs: a case study of finetuning GPT-2 into a robot language model for grounded task planning",
		"title-short": "Learning to reason over scene graphs",
		"URL": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10464606/",
		"volume": "10",
		"author": [
			{
				"family": "Chalvatzaki",
				"given": "Georgia"
			},
			{
				"family": "Younes",
				"given": "Ali"
			},
			{
				"family": "Nandha",
				"given": "Daljeet"
			},
			{
				"family": "Le",
				"given": "An Thai"
			},
			{
				"family": "Ribeiro",
				"given": "Leonardo FR"
			},
			{
				"family": "Gurevych",
				"given": "Iryna"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A4F39SS7",
		"type": "article",
		"abstract": "The ability to pick up on language signals in an ongoing interaction is crucial for future machine learning models to collaborate and interact with humans naturally. In this paper, we present an initial study that evaluates intra-episodic feedback given in a collaborative setting. We use a referential language game as a controllable example of a task-oriented collaborative joint activity. A teacher utters a referring expression generated by a well-known symbolic algorithm (the \"Incremental Algorithm\") as an initial instruction and then monitors the follower's actions to possibly intervene with intra-episodic feedback (which does not explicitly have to be requested). We frame this task as a reinforcement learning problem with sparse rewards and learn a follower policy for a heuristic teacher. Our results show that intra-episodic feedback allows the follower to generalize on aspects of scene complexity and performs better than providing only the initial statement.",
		"note": "arXiv:2305.12880 [cs]",
		"number": "arXiv:2305.12880",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Yes, this Way! Learning to Ground Referring Expressions into Actions with Intra-episodic Feedback from Supportive Teachers",
		"URL": "http://arxiv.org/abs/2305.12880",
		"author": [
			{
				"family": "Sadler",
				"given": "Philipp"
			},
			{
				"family": "Hakimov",
				"given": "Sherzod"
			},
			{
				"family": "Schlangen",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					22
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/M7YLWNCW",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
		"page": "3164–3174",
		"source": "Google Scholar",
		"title": "Robothor: An open simulation-to-real embodied ai platform",
		"title-short": "Robothor",
		"URL": "http://openaccess.thecvf.com/content_CVPR_2020/html/Deitke_RoboTHOR_An_Open_Simulation-to-Real_Embodied_AI_Platform_CVPR_2020_paper.html",
		"author": [
			{
				"family": "Deitke",
				"given": "Matt"
			},
			{
				"family": "Han",
				"given": "Winson"
			},
			{
				"family": "Herrasti",
				"given": "Alvaro"
			},
			{
				"family": "Kembhavi",
				"given": "Aniruddha"
			},
			{
				"family": "Kolve",
				"given": "Eric"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Salvador",
				"given": "Jordi"
			},
			{
				"family": "Schwenk",
				"given": "Dustin"
			},
			{
				"family": "VanderBilt",
				"given": "Eli"
			},
			{
				"family": "Wallingford",
				"given": "Matthew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ADUNQT58",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Massachusetts Institute of Technology",
		"source": "Google Scholar",
		"title": "VirtualHome: simulating household activities via programs",
		"title-short": "VirtualHome",
		"URL": "https://dspace.mit.edu/handle/1721.1/118051",
		"author": [
			{
				"family": "Puig Fernandez",
				"given": "Xavier Francesc Xavier"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SUFK44V9",
		"type": "paper-conference",
		"container-title": "Conference on Robot Learning",
		"page": "81–98",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Guided feature transformation (gft): A neural language grounding module for embodied agents",
		"title-short": "Guided feature transformation (gft)",
		"URL": "http://proceedings.mlr.press/v87/yu18a.html",
		"author": [
			{
				"family": "Yu",
				"given": "Haonan"
			},
			{
				"family": "Lian",
				"given": "Xiaochen"
			},
			{
				"family": "Zhang",
				"given": "Haichao"
			},
			{
				"family": "Xu",
				"given": "Wei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GGIWXLIH",
		"type": "article-journal",
		"container-title": "Autonomous robots",
		"note": "publisher: Springer",
		"page": "1–23",
		"source": "Google Scholar",
		"title": "Embodied scene description",
		"URL": "https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10514-021-10014-9&casa_token=oqNa7gU94jgAAAAA:H5JsCy-19YLYj7EXGe__qyPrPmfHaL6jbg0WCb604yFCjQZoQi3YAX-eCZj3NscWp01456NhOh96ZS8jMv0",
		"author": [
			{
				"family": "Tan",
				"given": "Sinan"
			},
			{
				"family": "Guo",
				"given": "Di"
			},
			{
				"family": "Liu",
				"given": "Huaping"
			},
			{
				"family": "Zhang",
				"given": "Xinyu"
			},
			{
				"family": "Sun",
				"given": "Fuchun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZKFVTXAB",
		"type": "article",
		"abstract": "Breakthroughs in machine learning in the last decade have led to `digital intelligence', i.e. machine learning models capable of learning from vast amounts of labeled data to perform several digital tasks such as speech recognition, face recognition, machine translation and so on. The goal of this thesis is to make progress towards designing algorithms capable of `physical intelligence', i.e. building intelligent autonomous navigation agents capable of learning to perform complex navigation tasks in the physical world involving visual perception, natural language understanding, reasoning, planning, and sequential decision making. Despite several advances in classical navigation methods in the last few decades, current navigation agents struggle at long-term semantic navigation tasks. In the first part of the thesis, we discuss our work on short-term navigation using end-to-end reinforcement learning to tackle challenges such as obstacle avoidance, semantic perception, language grounding, and reasoning. In the second part, we present a new class of navigation methods based on modular learning and structured explicit map representations, which leverage the strengths of both classical and end-to-end learning methods, to tackle long-term navigation tasks. We show that these methods are able to effectively tackle challenges such as localization, mapping, long-term planning, exploration and learning semantic priors. These modular learning methods are capable of long-term spatial and semantic understanding and achieve state-of-the-art results on various navigation tasks.",
		"note": "arXiv:2106.13415 [cs]",
		"number": "arXiv:2106.13415",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Building Intelligent Autonomous Navigation Agents",
		"URL": "http://arxiv.org/abs/2106.13415",
		"author": [
			{
				"family": "Chaplot",
				"given": "Devendra Singh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					25
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BD5BI89T",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF international conference on computer vision",
		"page": "2769–2779",
		"source": "Google Scholar",
		"title": "Bayesian relational memory for semantic visual navigation",
		"URL": "http://openaccess.thecvf.com/content_ICCV_2019/html/Wu_Bayesian_Relational_Memory_for_Semantic_Visual_Navigation_ICCV_2019_paper.html",
		"author": [
			{
				"family": "Wu",
				"given": "Yi"
			},
			{
				"family": "Wu",
				"given": "Yuxin"
			},
			{
				"family": "Tamar",
				"given": "Aviv"
			},
			{
				"family": "Russell",
				"given": "Stuart"
			},
			{
				"family": "Gkioxari",
				"given": "Georgia"
			},
			{
				"family": "Tian",
				"given": "Yuandong"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EL2T4ATM",
		"type": "article",
		"abstract": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.",
		"note": "arXiv:2307.15217 [cs]",
		"number": "arXiv:2307.15217",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
		"URL": "http://arxiv.org/abs/2307.15217",
		"author": [
			{
				"family": "Casper",
				"given": "Stephen"
			},
			{
				"family": "Davies",
				"given": "Xander"
			},
			{
				"family": "Shi",
				"given": "Claudia"
			},
			{
				"family": "Gilbert",
				"given": "Thomas Krendl"
			},
			{
				"family": "Scheurer",
				"given": "Jérémy"
			},
			{
				"family": "Rando",
				"given": "Javier"
			},
			{
				"family": "Freedman",
				"given": "Rachel"
			},
			{
				"family": "Korbak",
				"given": "Tomasz"
			},
			{
				"family": "Lindner",
				"given": "David"
			},
			{
				"family": "Freire",
				"given": "Pedro"
			},
			{
				"family": "Wang",
				"given": "Tony"
			},
			{
				"family": "Marks",
				"given": "Samuel"
			},
			{
				"family": "Segerie",
				"given": "Charbel-Raphaël"
			},
			{
				"family": "Carroll",
				"given": "Micah"
			},
			{
				"family": "Peng",
				"given": "Andi"
			},
			{
				"family": "Christoffersen",
				"given": "Phillip"
			},
			{
				"family": "Damani",
				"given": "Mehul"
			},
			{
				"family": "Slocum",
				"given": "Stewart"
			},
			{
				"family": "Anwar",
				"given": "Usman"
			},
			{
				"family": "Siththaranjan",
				"given": "Anand"
			},
			{
				"family": "Nadeau",
				"given": "Max"
			},
			{
				"family": "Michaud",
				"given": "Eric J."
			},
			{
				"family": "Pfau",
				"given": "Jacob"
			},
			{
				"family": "Krasheninnikov",
				"given": "Dmitrii"
			},
			{
				"family": "Chen",
				"given": "Xin"
			},
			{
				"family": "Langosco",
				"given": "Lauro"
			},
			{
				"family": "Hase",
				"given": "Peter"
			},
			{
				"family": "Bıyık",
				"given": "Erdem"
			},
			{
				"family": "Dragan",
				"given": "Anca"
			},
			{
				"family": "Krueger",
				"given": "David"
			},
			{
				"family": "Sadigh",
				"given": "Dorsa"
			},
			{
				"family": "Hadfield-Menell",
				"given": "Dylan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AVJXI7EV",
		"type": "thesis",
		"genre": "PhD Thesis",
		"source": "Google Scholar",
		"title": "Reinforcement Learning Agents that Discover Structured Representations",
		"URL": "https://deepblue.lib.umich.edu/handle/2027.42/177937",
		"author": [
			{
				"family": "Carvalho",
				"given": "Wilka"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LBWHJJV3",
		"type": "article",
		"abstract": "With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.",
		"note": "arXiv:2307.13854 [cs]",
		"number": "arXiv:2307.13854",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
		"title-short": "WebArena",
		"URL": "http://arxiv.org/abs/2307.13854",
		"author": [
			{
				"family": "Zhou",
				"given": "Shuyan"
			},
			{
				"family": "Xu",
				"given": "Frank F."
			},
			{
				"family": "Zhu",
				"given": "Hao"
			},
			{
				"family": "Zhou",
				"given": "Xuhui"
			},
			{
				"family": "Lo",
				"given": "Robert"
			},
			{
				"family": "Sridhar",
				"given": "Abishek"
			},
			{
				"family": "Cheng",
				"given": "Xianyi"
			},
			{
				"family": "Ou",
				"given": "Tianyue"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Fried",
				"given": "Daniel"
			},
			{
				"family": "Alon",
				"given": "Uri"
			},
			{
				"family": "Neubig",
				"given": "Graham"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					24
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RE8IFS66",
		"type": "article",
		"abstract": "Despite recent progress, learning new tasks through language instructions remains an extremely challenging problem. On the ALFRED benchmark for task learning, the published state-of-the-art system only achieves a task success rate of less than 10% in an unseen environment, compared to the human performance of over 90%. To address this issue, this paper takes a closer look at task learning. In a departure from a widely applied end-to-end architecture, we decomposed task learning into three sub-problems: sub-goal planning, scene navigation, and object manipulation; and developed a model HiTUT (stands for Hierarchical Tasks via Unified Transformers) that addresses each sub-problem in a unified manner to learn a hierarchical task structure. On the ALFRED benchmark, HiTUT has achieved the best performance with a remarkably higher generalization ability. In the unseen environment, HiTUT achieves over 160% performance gain in success rate compared to the previous state of the art. The explicit representation of task structures also enables an in-depth understanding of the nature of the problem and the ability of the agent, which provides insight for future benchmark development and evaluation.",
		"note": "arXiv:2106.03427 [cs]",
		"number": "arXiv:2106.03427",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring",
		"URL": "http://arxiv.org/abs/2106.03427",
		"author": [
			{
				"family": "Zhang",
				"given": "Yichi"
			},
			{
				"family": "Chai",
				"given": "Joyce"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					6,
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VZ5TFQAC",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "18343–18362",
		"source": "Google Scholar",
		"title": "Minedojo: Building open-ended embodied agents with internet-scale knowledge",
		"title-short": "Minedojo",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/74a67268c5cc5910f64938cac4526a90-Abstract-Datasets_and_Benchmarks.html",
		"volume": "35",
		"author": [
			{
				"family": "Fan",
				"given": "Linxi"
			},
			{
				"family": "Wang",
				"given": "Guanzhi"
			},
			{
				"family": "Jiang",
				"given": "Yunfan"
			},
			{
				"family": "Mandlekar",
				"given": "Ajay"
			},
			{
				"family": "Yang",
				"given": "Yuncong"
			},
			{
				"family": "Zhu",
				"given": "Haoyi"
			},
			{
				"family": "Tang",
				"given": "Andrew"
			},
			{
				"family": "Huang",
				"given": "De-An"
			},
			{
				"family": "Zhu",
				"given": "Yuke"
			},
			{
				"family": "Anandkumar",
				"given": "Anima"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/PEZJVRJN",
		"type": "article-journal",
		"abstract": "This paper presents a dataset of natural language instructions for object reference in manipulation scenarios. It comprises 1582 individual written instructions, which were collected via online crowdsourcing. This dataset is particularly useful for researchers who work in natural language processing, human–robot interaction, and robotic manipulation. In addition to serving as a rich corpus of domain-specific language, it provides a benchmark of image–instruction pairs to be used in system evaluations and uncovers inherent challenges in tabletop object specification. Example code is provided for easy access via Python.",
		"container-title": "The International Journal of Robotics Research",
		"DOI": "10.1177/0278364918760992",
		"ISSN": "0278-3649, 1741-3176",
		"issue": "6",
		"journalAbbreviation": "The International Journal of Robotics Research",
		"language": "en",
		"page": "558-565",
		"source": "DOI.org (Crossref)",
		"title": "Natural language instructions for human–robot collaborative manipulation",
		"URL": "http://journals.sagepub.com/doi/10.1177/0278364918760992",
		"volume": "37",
		"author": [
			{
				"family": "Scalise",
				"given": "Rosario"
			},
			{
				"family": "Li",
				"given": "Shen"
			},
			{
				"family": "Admoni",
				"given": "Henny"
			},
			{
				"family": "Rosenthal",
				"given": "Stephanie"
			},
			{
				"family": "Srinivasa",
				"given": "Siddhartha S"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FI8DZJBE",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition",
		"page": "6732–6740",
		"source": "Google Scholar",
		"title": "The regretful agent: Heuristic-aided navigation through progress estimation",
		"title-short": "The regretful agent",
		"URL": "http://openaccess.thecvf.com/content_CVPR_2019/html/Ma_The_Regretful_Agent_Heuristic-Aided_Navigation_Through_Progress_Estimation_CVPR_2019_paper.html",
		"author": [
			{
				"family": "Ma",
				"given": "Chih-Yao"
			},
			{
				"family": "Wu",
				"given": "Zuxuan"
			},
			{
				"family": "AlRegib",
				"given": "Ghassan"
			},
			{
				"family": "Xiong",
				"given": "Caiming"
			},
			{
				"family": "Kira",
				"given": "Zsolt"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/T6GAFPMA",
		"type": "article",
		"abstract": "Reinforcement learning agents deployed in the real world often have to cope with partially observable environments. Therefore, most agents employ memory mechanisms to approximate the state of the environment. Recently, there have been impressive success stories in mastering partially observable environments, mostly in the realm of computer games like Dota 2, StarCraft II, or MineCraft. However, existing methods lack interpretability in the sense that it is not comprehensible for humans what the agent stores in its memory. In this regard, we propose a novel memory mechanism that represents past events in human language. Our method uses CLIP to associate visual inputs with language tokens. Then we feed these tokens to a pretrained language model that serves the agent as memory and provides it with a coherent and human-readable representation of the past. We train our memory mechanism on a set of partially observable environments and find that it excels on tasks that require a memory component, while mostly attaining performance on-par with strong baselines on tasks that do not. On a challenging continuous recognition task, where memorizing the past is crucial, our memory mechanism converges two orders of magnitude faster than prior methods. Since our memory mechanism is human-readable, we can peek at an agent's memory and check whether crucial pieces of information have been stored. This significantly enhances troubleshooting and paves the way toward more interpretable agents.",
		"note": "arXiv:2306.09312 [cs, stat]",
		"number": "arXiv:2306.09312",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Semantic HELM: A Human-Readable Memory for Reinforcement Learning",
		"title-short": "Semantic HELM",
		"URL": "http://arxiv.org/abs/2306.09312",
		"author": [
			{
				"family": "Paischer",
				"given": "Fabian"
			},
			{
				"family": "Adler",
				"given": "Thomas"
			},
			{
				"family": "Hofmarcher",
				"given": "Markus"
			},
			{
				"family": "Hochreiter",
				"given": "Sepp"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LIUCTI2T",
		"type": "paper-conference",
		"container-title": "2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
		"page": "8593–8600",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Grounding linguistic commands to navigable regions",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9636172/?casa_token=kyAfsAXra20AAAAA:SeKiHjHjYuRhMrtPwgUNFV97itpaYIAwCZNOjhIsQhFiO5z6Z6OM6AvApEK2U7r1W3MLnISaIMbZ",
		"author": [
			{
				"family": "Rufus",
				"given": "Nivedita"
			},
			{
				"family": "Jain",
				"given": "Kanishk"
			},
			{
				"family": "Nair",
				"given": "Unni Krishnan R."
			},
			{
				"family": "Gandhi",
				"given": "Vineet"
			},
			{
				"family": "Krishna",
				"given": "K. Madhava"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CIWJSUKM",
		"type": "article",
		"abstract": "Given a simple request like Put a washed apple in the kitchen fridge, humans can reason in purely abstract terms by imagining action sequences and scoring their likelihood of success, prototypicality, and efficiency, all without moving a muscle. Once we see the kitchen in question, we can update our abstract plans to fit the scene. Embodied agents require the same abilities, but existing work does not yet provide the infrastructure necessary for both reasoning abstractly and executing concretely. We address this limitation by introducing ALFWorld, a simulator that enables agents to learn abstract, text based policies in TextWorld (C\\^ot\\'e et al., 2018) and then execute goals from the ALFRED benchmark (Shridhar et al., 2020) in a rich visual environment. ALFWorld enables the creation of a new BUTLER agent whose abstract knowledge, learned in TextWorld, corresponds directly to concrete, visually grounded actions. In turn, as we demonstrate empirically, this fosters better agent generalization than training only in the visually grounded environment. BUTLER's simple, modular design factors the problem to allow researchers to focus on models for improving every piece of the pipeline (language understanding, planning, navigation, and visual scene understanding).",
		"note": "arXiv:2010.03768 [cs]",
		"number": "arXiv:2010.03768",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ALFWorld: Aligning Text and Embodied Environments for Interactive Learning",
		"title-short": "ALFWorld",
		"URL": "http://arxiv.org/abs/2010.03768",
		"author": [
			{
				"family": "Shridhar",
				"given": "Mohit"
			},
			{
				"family": "Yuan",
				"given": "Xingdi"
			},
			{
				"family": "Côté",
				"given": "Marc-Alexandre"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Trischler",
				"given": "Adam"
			},
			{
				"family": "Hausknecht",
				"given": "Matthew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					3,
					14
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2ZVLVLUU",
		"type": "article-journal",
		"container-title": "Neural Processing Letters",
		"note": "publisher: Springer",
		"page": "1–22",
		"source": "Google Scholar",
		"title": "Double Graph Attention Networks for Visual Semantic Navigation",
		"URL": "https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11063-023-11190-8&casa_token=IUXmBgJ2L7wAAAAA:_M361Y2jhvxzCxUiymKTAdYoXSFCPt8RQizi-dzmRXB5tXt2XKJ5f7FpWJP6nrEETIyi1_mSSfSsuF8-rqQ",
		"author": [
			{
				"family": "Lyu",
				"given": "Yunlian"
			},
			{
				"family": "Talebi",
				"given": "Mohammad Sadegh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9872VFWY",
		"type": "article",
		"abstract": "Vision-and-language navigation (VLN) agents are trained to navigate in real-world environments by following natural language instructions. A major challenge in VLN is the limited availability of training data, which hinders the models' ability to generalize effectively. Previous approaches have attempted to address this issue by introducing additional supervision during training, often requiring costly human-annotated data that restricts scalability. In this paper, we introduce a masked path modeling (MPM) objective, which pretrains an agent using self-collected data for downstream navigation tasks. Our proposed method involves allowing the agent to actively explore navigation environments without a specific goal and collect the paths it traverses. Subsequently, we train the agent on this collected data to reconstruct the original path given a randomly masked subpath. This way, the agent can actively accumulate a diverse and substantial amount of data while learning conditional action generation. To evaluate the effectiveness of our technique, we conduct experiments on various VLN datasets and demonstrate the versatility of MPM across different levels of instruction complexity. Our results exhibit significant improvements in success rates, with enhancements of 1.32\\%, 1.05\\%, and 1.19\\% on the val-unseen split of the Room-to-Room, Room-for-Room, and Room-across-Room datasets, respectively. Furthermore, we conduct an analysis that highlights the potential for additional improvements when the agent is allowed to explore unseen environments prior to testing.",
		"note": "arXiv:2305.14268 [cs]",
		"number": "arXiv:2305.14268",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Masked Path Modeling for Vision-and-Language Navigation",
		"URL": "http://arxiv.org/abs/2305.14268",
		"author": [
			{
				"family": "Dou",
				"given": "Zi-Yi"
			},
			{
				"family": "Gao",
				"given": "Feng"
			},
			{
				"family": "Peng",
				"given": "Nanyun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					23
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZIMSKYBP",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Pontificia Universidad Catolica de Chile (Chile)",
		"source": "Google Scholar",
		"title": "Enhanced Vision-Language Navigation by Using Scene Recognition Auxiliary Task",
		"URL": "https://search.proquest.com/openview/5b7bfc2f21fdaf26112ce54ffcea312e/1?pq-origsite=gscholar&cbl=2026366&diss=y",
		"author": [
			{
				"family": "Valenzuela",
				"given": "Raimundo Manterola"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FZIN2TZE",
		"type": "paper-conference",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"note": "issue: 11",
		"page": "13192–13200",
		"source": "Google Scholar",
		"title": "On grounded planning for embodied tasks with language models",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/26549",
		"volume": "37",
		"author": [
			{
				"family": "Lin",
				"given": "Bill Yuchen"
			},
			{
				"family": "Huang",
				"given": "Chengsong"
			},
			{
				"family": "Liu",
				"given": "Qian"
			},
			{
				"family": "Gu",
				"given": "Wenda"
			},
			{
				"family": "Sommerer",
				"given": "Sam"
			},
			{
				"family": "Ren",
				"given": "Xiang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/U5BJ52DJ",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Superlative Learning in Semantic Goal Navigation",
		"URL": "https://damongeorge.github.io/src/docs/Deep_Learning_Final_Paper.pdf",
		"author": [
			{
				"family": "George",
				"given": "Damon"
			},
			{
				"family": "Krantz",
				"given": "Jacob"
			},
			{
				"family": "Valencia",
				"given": "Joseph"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A2CTSUPH",
		"type": "thesis",
		"genre": "PhD Thesis",
		"source": "Google Scholar",
		"title": "Towards Improving Deep Vision and Language Navigation",
		"URL": "https://deepblue.lib.umich.edu/handle/2027.42/172615",
		"author": [
			{
				"family": "Banerjee",
				"given": "Shurjo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VPMAP452",
		"type": "paper-conference",
		"container-title": "Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition@ CoRL2023",
		"source": "Google Scholar",
		"title": "Gen2Sim: Scaling up Simulation with Generative Models for Robotic Skill Learning",
		"title-short": "Gen2Sim",
		"URL": "https://openreview.net/forum?id=vE4B7yKbk1",
		"author": [
			{
				"family": "Katara",
				"given": "Pushkal"
			},
			{
				"family": "Xian",
				"given": "Zhou"
			},
			{
				"family": "Fragkiadaki",
				"given": "Katerina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/H9YIJIR9",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "14829–14838",
		"source": "Google Scholar",
		"title": "Simple but effective: Clip embeddings for embodied ai",
		"title-short": "Simple but effective",
		"URL": "http://openaccess.thecvf.com/content/CVPR2022/html/Khandelwal_Simple_but_Effective_CLIP_Embeddings_for_Embodied_AI_CVPR_2022_paper.html",
		"author": [
			{
				"family": "Khandelwal",
				"given": "Apoorv"
			},
			{
				"family": "Weihs",
				"given": "Luca"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Kembhavi",
				"given": "Aniruddha"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JAX48GHW",
		"type": "article",
		"abstract": "In this work, we present an alternative approach to making an agent compositional through the use of a diagnostic classifier. Because of the need for explainable agents in automated decision processes, we attempt to interpret the latent space from an RL agent to identify its current objective in a complex language instruction. Results show that the classification process causes changes in the hidden states which makes them more easily interpretable, but also causes a shift in zero-shot performance to novel instructions. Lastly, we limit the supervisory signal on the classification, and observe a similar but less notable effect.",
		"note": "arXiv:2001.04418 [cs]",
		"number": "arXiv:2001.04418",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Exploiting Language Instructions for Interpretable and Compositional Reinforcement Learning",
		"URL": "http://arxiv.org/abs/2001.04418",
		"author": [
			{
				"family": "Meer",
				"given": "Michiel",
				"non-dropping-particle": "van der"
			},
			{
				"family": "Pirotta",
				"given": "Matteo"
			},
			{
				"family": "Bruni",
				"given": "Elia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					1,
					13
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3VVJM3LS",
		"type": "paper-conference",
		"container-title": "2021 International Joint Conference on Neural Networks (IJCNN)",
		"page": "1–8",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Vision-and-Dialog Navigation by Fusing Cross-modal features",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9533776/?casa_token=cge61ky13fUAAAAA:8XRa06eC0jjP_xQ4pQuNB4DjnXR1IsT5YcTw-XNq6EWYiPs--k3kVfRNVqqLPnKdi7sAQ1-0X9N8",
		"author": [
			{
				"family": "Nie",
				"given": "Hongxu"
			},
			{
				"family": "Dong",
				"given": "Min"
			},
			{
				"family": "Bi",
				"given": "Sheng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GWUVVWWL",
		"type": "article",
		"abstract": "In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN",
		"note": "arXiv:2311.00530 [cs]",
		"number": "arXiv:2311.00530",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "The Development of LLMs for Embodied Navigation",
		"URL": "http://arxiv.org/abs/2311.00530",
		"author": [
			{
				"family": "Lin",
				"given": "Jinzhou"
			},
			{
				"family": "Gao",
				"given": "Han"
			},
			{
				"family": "Xu",
				"given": "Rongtao"
			},
			{
				"family": "Wang",
				"given": "Changwei"
			},
			{
				"family": "Guo",
				"given": "Li"
			},
			{
				"family": "Xu",
				"given": "Shibiao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SN6BP8MC",
		"type": "article",
		"abstract": "The AI community has been pursuing algorithms known as artificial general intelligence (AGI) that apply to any kind of real-world problem. Recently, chat systems powered by large language models (LLMs) emerge and rapidly become a promising direction to achieve AGI in natural language processing (NLP), but the path towards AGI in computer vision (CV) remains unclear. One may owe the dilemma to the fact that visual signals are more complex than language signals, yet we are interested in finding concrete reasons, as well as absorbing experiences from GPT and LLMs to solve the problem. In this paper, we start with a conceptual definition of AGI and briefly review how NLP solves a wide range of tasks via a chat system. The analysis inspires us that unification is the next important goal of CV. But, despite various efforts in this direction, CV is still far from a system like GPT that naturally integrates all tasks. We point out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world. We then imagine a pipeline that puts a CV algorithm (i.e., an agent) in world-scale, interactable environments, pre-trains it to predict future frames with respect to its action, and then fine-tunes it with instruction to accomplish various tasks. We expect substantial research and engineering efforts to push the idea forward and scale it up, for which we share our perspectives on future research directions.",
		"note": "arXiv:2306.08641 [cs]",
		"number": "arXiv:2306.08641",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models",
		"title-short": "Towards AGI in Computer Vision",
		"URL": "http://arxiv.org/abs/2306.08641",
		"author": [
			{
				"family": "Xie",
				"given": "Lingxi"
			},
			{
				"family": "Wei",
				"given": "Longhui"
			},
			{
				"family": "Zhang",
				"given": "Xiaopeng"
			},
			{
				"family": "Bi",
				"given": "Kaifeng"
			},
			{
				"family": "Gu",
				"given": "Xiaotao"
			},
			{
				"family": "Chang",
				"given": "Jianlong"
			},
			{
				"family": "Tian",
				"given": "Qi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					14
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XRH7E7P3",
		"type": "paper-conference",
		"container-title": "2021 International Joint Conference on Neural Networks (IJCNN)",
		"page": "1–8",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Multi-grounding navigator for self-supervised vision-and-language navigation",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9533628/?casa_token=YZavST_BX20AAAAA:hXGAmUNQrjUOoysQPcQiujrVYFI0oh2o0M2GJbhUi3u3vt7XULko-ijdvsAd91oSoBf4oPgvNNY3",
		"author": [
			{
				"family": "Wu",
				"given": "Zongkai"
			},
			{
				"family": "Liu",
				"given": "Zihan"
			},
			{
				"family": "Wang",
				"given": "Donglin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8S2T6QQ7",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2020",
		"event-place": "Cham",
		"ISBN": "978-3-030-58570-9",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-030-58571-6_2",
		"page": "19-34",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Learning Object Relation Graph and Tentative Policy for Visual Navigation",
		"URL": "https://link.springer.com/10.1007/978-3-030-58571-6_2",
		"volume": "12352",
		"editor": [
			{
				"family": "Vedaldi",
				"given": "Andrea"
			},
			{
				"family": "Bischof",
				"given": "Horst"
			},
			{
				"family": "Brox",
				"given": "Thomas"
			},
			{
				"family": "Frahm",
				"given": "Jan-Michael"
			}
		],
		"author": [
			{
				"family": "Du",
				"given": "Heming"
			},
			{
				"family": "Yu",
				"given": "Xin"
			},
			{
				"family": "Zheng",
				"given": "Liang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4XIA6BKP",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "13328–13339",
		"source": "Google Scholar",
		"title": "Curriculum learning for vision-and-language navigation",
		"URL": "https://proceedings.neurips.cc/paper/2021/hash/6f0442558302a6ededff195daf67f79b-Abstract.html",
		"volume": "34",
		"author": [
			{
				"family": "Zhang",
				"given": "Jiwen"
			},
			{
				"family": "Fan",
				"given": "Jianqing"
			},
			{
				"family": "Peng",
				"given": "Jiajie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FUUW6S26",
		"type": "article-journal",
		"container-title": "Proceedings of robotics: science and systems, New York City, NY, USA",
		"page": "1–14",
		"source": "Google Scholar",
		"title": "Embodied multi-agent task planning from ambiguous instruction",
		"URL": "https://scholar.archive.org/work/mowmk7pjr5gd5jk25jb5pvxoky/access/wayback/http://www.roboticsproceedings.org/rss18/p032.pdf",
		"author": [
			{
				"family": "Liu",
				"given": "Xinzhu"
			},
			{
				"family": "Li",
				"given": "Xinghang"
			},
			{
				"family": "Guo",
				"given": "Di"
			},
			{
				"family": "Tan",
				"given": "Sinan"
			},
			{
				"family": "Liu",
				"given": "Huaping"
			},
			{
				"family": "Sun",
				"given": "Fuchun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8GA63LPQ",
		"type": "article",
		"abstract": "Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as a road-map for identifying gaps in current methodologies. We release our benchmark at github.com/microsoft/SmartPlay",
		"note": "arXiv:2310.01557 [cs]",
		"number": "arXiv:2310.01557",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SmartPlay : A Benchmark for LLMs as Intelligent Agents",
		"title-short": "SmartPlay",
		"URL": "http://arxiv.org/abs/2310.01557",
		"author": [
			{
				"family": "Wu",
				"given": "Yue"
			},
			{
				"family": "Tang",
				"given": "Xuan"
			},
			{
				"family": "Mitchell",
				"given": "Tom M."
			},
			{
				"family": "Li",
				"given": "Yuanzhi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					4
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/TIP459M8",
		"type": "article",
		"abstract": "We introduce EvalAI, an open source platform for evaluating and comparing machine learning (ML) and artificial intelligence algorithms (AI) at scale. EvalAI is built to provide a scalable solution to the research community to fulfill the critical need of evaluating machine learning models and agents acting in an environment against annotations or with a human-in-the-loop. This will help researchers, students, and data scientists to create, collaborate, and participate in AI challenges organized around the globe. By simplifying and standardizing the process of benchmarking these models, EvalAI seeks to lower the barrier to entry for participating in the global scientific effort to push the frontiers of machine learning and artificial intelligence, thereby increasing the rate of measurable progress in this domain.",
		"note": "arXiv:1902.03570 [cs]",
		"number": "arXiv:1902.03570",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "EvalAI: Towards Better Evaluation Systems for AI Agents",
		"title-short": "EvalAI",
		"URL": "http://arxiv.org/abs/1902.03570",
		"author": [
			{
				"family": "Yadav",
				"given": "Deshraj"
			},
			{
				"family": "Jain",
				"given": "Rishabh"
			},
			{
				"family": "Agrawal",
				"given": "Harsh"
			},
			{
				"family": "Chattopadhyay",
				"given": "Prithvijit"
			},
			{
				"family": "Singh",
				"given": "Taranjeet"
			},
			{
				"family": "Jain",
				"given": "Akash"
			},
			{
				"family": "Singh",
				"given": "Shiv Baran"
			},
			{
				"family": "Lee",
				"given": "Stefan"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					2,
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4KY2IGAQ",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Multimedia",
		"note": "publisher: IEEE",
		"page": "4426–4440",
		"source": "Google Scholar",
		"title": "Referring expression comprehension: A survey of methods and datasets",
		"title-short": "Referring expression comprehension",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9285213/?casa_token=K_-kqBIL7WkAAAAA:v_D9uwy6ytK-Vc_fzu79Hd5PXJhXlDANi5d0NoVTtXFUwsD_521ZUlL1r3zGBxZtEC_acoawamLV",
		"volume": "23",
		"author": [
			{
				"family": "Qiao",
				"given": "Yanyuan"
			},
			{
				"family": "Deng",
				"given": "Chaorui"
			},
			{
				"family": "Wu",
				"given": "Qi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/Y6JHNZXF",
		"type": "article",
		"abstract": "Deep reinforcement learning agents need to be trained over millions of episodes to decently solve navigation tasks grounded to instructions. Furthermore, their ability to generalize to novel combinations of instructions is unclear. Interestingly however, children can decompose language-based instructions and navigate to the referred object, even if they have not seen the combination of queries prior. Hence, we created three 3D environments to investigate how deep RL agents learn and compose color-shape based combinatorial instructions to solve novel combinations in a spatial navigation task. First, we explore if agents can perform compositional learning, and whether they can leverage on frozen text encoders (e.g. CLIP, BERT) to learn word combinations in fewer episodes. Next, we demonstrate that when agents are pretrained on the shape or color concepts separately, they show a 20 times decrease in training episodes needed to solve unseen combinations of instructions. Lastly, we show that agents pretrained on concept and compositional learning achieve significantly higher reward when evaluated zero-shot on novel color-shape1-shape2 visual object combinations. Overall, our results highlight the foundations needed to increase an agent's proficiency in composing word groups through reinforcement learning and its ability for zero-shot generalization to new combinations.",
		"note": "arXiv:2309.04504 [cs]",
		"number": "arXiv:2309.04504",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Compositional Learning of Visually-Grounded Concepts Using Reinforcement",
		"URL": "http://arxiv.org/abs/2309.04504",
		"author": [
			{
				"family": "Lin",
				"given": "Zijun"
			},
			{
				"family": "Azaman",
				"given": "Haidi"
			},
			{
				"family": "Kumar",
				"given": "M. Ganesh"
			},
			{
				"family": "Tan",
				"given": "Cheston"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					8
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/TGVT2BEM",
		"type": "paper-conference",
		"container-title": "Conference on Robot Learning",
		"page": "1314–1327",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Navigation agents for the visually impaired: A sidewalk simulator and experiments",
		"title-short": "Navigation agents for the visually impaired",
		"URL": "http://proceedings.mlr.press/v100/weiss20a.html",
		"author": [
			{
				"family": "Weiss",
				"given": "Martin"
			},
			{
				"family": "Chamorro",
				"given": "Simon"
			},
			{
				"family": "Girgis",
				"given": "Roger"
			},
			{
				"family": "Luck",
				"given": "Margaux"
			},
			{
				"family": "Kahou",
				"given": "Samira E."
			},
			{
				"family": "Cohen",
				"given": "Joseph P."
			},
			{
				"family": "Nowrouzezahrai",
				"given": "Derek"
			},
			{
				"family": "Precup",
				"given": "Doina"
			},
			{
				"family": "Golemo",
				"given": "Florian"
			},
			{
				"family": "Pal",
				"given": "Chris"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2U37SGDY",
		"type": "article",
		"abstract": "With strong representation capabilities, pretrained vision-language models are widely used in vision and language navigation (VLN). However, most of them are trained on web-crawled general-purpose datasets, which incurs a considerable domain gap when used for VLN tasks. Another challenge for VLN is how the agent understands the contextual relations between actions on a trajectory and performs cross-modal alignment sequentially. In this paper, we propose a novel Prompt-bAsed coNtext- and Domain-Aware (PANDA) pretraining framework to address these problems. It performs prompting in two stages. In the domain-aware stage, we apply a low-cost prompt tuning paradigm to learn soft visual prompts from an in-domain dataset for equipping the pretrained models with object-level and scene-level cross-modal alignment in VLN tasks. Furthermore, in the context-aware stage, we design a set of hard context prompts to capture the sequence-level semantics and instill both out-of-context and contextual knowledge in the instruction into cross-modal representations. They enable further tuning of the pretrained models via contrastive learning. Experimental results on both R2R and REVERIE show the superiority of PANDA compared to previous state-of-the-art methods.",
		"note": "arXiv:2309.03661 [cs]",
		"number": "arXiv:2309.03661",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Prompt-based Context- and Domain-aware Pretraining for Vision and Language Navigation",
		"URL": "http://arxiv.org/abs/2309.03661",
		"author": [
			{
				"family": "Liu",
				"given": "Ting"
			},
			{
				"family": "Wu",
				"given": "Wansen"
			},
			{
				"family": "Hu",
				"given": "Yue"
			},
			{
				"family": "Wang",
				"given": "Youkai"
			},
			{
				"family": "Xu",
				"given": "Kai"
			},
			{
				"family": "Yin",
				"given": "Quanjun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EK4RBB7N",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Vision-and-Language Navigation: Literature Review",
		"title-short": "Vision-and-Language Navigation",
		"URL": "https://g-jing.github.io/files/VLN_review.pdf",
		"author": [
			{
				"family": "Gu",
				"given": "Jing"
			},
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Wang",
				"given": "Xin Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6IYEQK4A",
		"type": "thesis",
		"genre": "Master's Thesis",
		"publisher": "University of Twente",
		"source": "Google Scholar",
		"title": "Language-Based Augmentation to Address Shortcut Learning in Object-Goal Navigation",
		"URL": "http://essay.utwente.nl/94505/",
		"author": [
			{
				"family": "Hoftijzer",
				"given": "D. M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AKA2G7IV",
		"type": "article",
		"abstract": "The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.",
		"note": "arXiv:2102.03406 [cs]",
		"number": "arXiv:2102.03406",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Symbolic Behaviour in Artificial Intelligence",
		"URL": "http://arxiv.org/abs/2102.03406",
		"author": [
			{
				"family": "Santoro",
				"given": "Adam"
			},
			{
				"family": "Lampinen",
				"given": "Andrew"
			},
			{
				"family": "Mathewson",
				"given": "Kory"
			},
			{
				"family": "Lillicrap",
				"given": "Timothy"
			},
			{
				"family": "Raposo",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					1,
					21
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DM4AJNN3",
		"type": "paper-conference",
		"container-title": "2023 IEEE International Conference on Robotics and Automation (ICRA)",
		"page": "4113–4120",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Ground then navigate: Language-guided navigation in dynamic scenes",
		"title-short": "Ground then navigate",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10160614/?casa_token=Iob6o-uYGI4AAAAA:4RhiAeawzG9Qi4fzeTB8pzozk92G8QqHhsooHchRuSoPRiUrGGq4pe5OQxwjxq-5dsxjyB6EOazW",
		"author": [
			{
				"family": "Jain",
				"given": "Kanishk"
			},
			{
				"family": "Chhangani",
				"given": "Varun"
			},
			{
				"family": "Tiwari",
				"given": "Amogh"
			},
			{
				"family": "Krishna",
				"given": "K. Madhava"
			},
			{
				"family": "Gandhi",
				"given": "Vineet"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BQXCLERI",
		"type": "article",
		"abstract": "Planning is an important capability of artificial agents that perform long-horizon tasks in real-world environments. In this work, we explore the use of pre-trained language models (PLMs) to reason about plan sequences from text instructions in embodied visual environments. Prior PLM based approaches for planning either assume observations are available in the form of text (e.g., provided by a captioning model), reason about plans from the instruction alone, or incorporate information about the visual environment in limited ways (such as a pre-trained affordance function). In contrast, we show that PLMs can accurately plan even when observations are directly encoded as input prompts for the PLM. We show that this simple approach outperforms prior approaches in experiments on the ALFWorld and VirtualHome benchmarks.",
		"note": "arXiv:2303.09031 [cs]",
		"number": "arXiv:2303.09031",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Picture is Worth a Thousand Words: Language Models Plan from Pixels",
		"title-short": "A Picture is Worth a Thousand Words",
		"URL": "http://arxiv.org/abs/2303.09031",
		"author": [
			{
				"family": "Liu",
				"given": "Anthony Z."
			},
			{
				"family": "Logeswaran",
				"given": "Lajanugen"
			},
			{
				"family": "Sohn",
				"given": "Sungryull"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					15
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XI5Z32GT",
		"type": "article",
		"abstract": "We propose a new task to benchmark scene understanding of embodied agents: Situated Question Answering in 3D Scenes (SQA3D). Given a scene context (e.g., 3D scan), SQA3D requires the tested agent to first understand its situation (position, orientation, etc.) in the 3D scene as described by text, then reason about its surrounding environment and answer a question under that situation. Based upon 650 scenes from ScanNet, we provide a dataset centered around 6.8k unique situations, along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations. These questions examine a wide spectrum of reasoning capabilities for an intelligent agent, ranging from spatial relation comprehension to commonsense understanding, navigation, and multi-hop reasoning. SQA3D imposes a significant challenge to current multi-modal especially 3D reasoning models. We evaluate various state-of-the-art approaches and find that the best one only achieves an overall score of 47.20%, while amateur human participants can reach 90.06%. We believe SQA3D could facilitate future embodied AI research with stronger situation understanding and reasoning capability.",
		"note": "arXiv:2210.07474 [cs]",
		"number": "arXiv:2210.07474",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SQA3D: Situated Question Answering in 3D Scenes",
		"title-short": "SQA3D",
		"URL": "http://arxiv.org/abs/2210.07474",
		"author": [
			{
				"family": "Ma",
				"given": "Xiaojian"
			},
			{
				"family": "Yong",
				"given": "Silong"
			},
			{
				"family": "Zheng",
				"given": "Zilong"
			},
			{
				"family": "Li",
				"given": "Qing"
			},
			{
				"family": "Liang",
				"given": "Yitao"
			},
			{
				"family": "Zhu",
				"given": "Song-Chun"
			},
			{
				"family": "Huang",
				"given": "Siyuan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8X9CUKU8",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF winter conference on applications of computer vision",
		"page": "3733–3742",
		"source": "Google Scholar",
		"title": "Optimistic agent: Accurate graph-based value estimation for more successful visual navigation",
		"title-short": "Optimistic agent",
		"URL": "http://openaccess.thecvf.com/content/WACV2021/html/Moghaddam_Optimistic_Agent_Accurate_Graph-Based_Value_Estimation_for_More_Successful_Visual_WACV_2021_paper.html",
		"author": [
			{
				"family": "Moghaddam",
				"given": "Mahdi Kazemi"
			},
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Abbasnejad",
				"given": "Ehsan"
			},
			{
				"family": "Shi",
				"given": "Javen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BQPE47ZU",
		"type": "paper-conference",
		"container-title": "2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)",
		"page": "1025–1031",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Early fusion for goal directed robotic vision",
		"URL": "https://ieeexplore.ieee.org/abstract/document/8968165/?casa_token=TJpT6RySnmEAAAAA:E6z6aFhyc3SG8Ed-9dsYl1hjk7Igr1tskiJzyC5fO2uw6wdNxYqKa2c5SLPeXkxnLIXXuGXc95aH",
		"author": [
			{
				"family": "Walsman",
				"given": "Aaron"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Gabriel",
				"given": "Saadia"
			},
			{
				"family": "Misra",
				"given": "Dipendra"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			},
			{
				"family": "Choi",
				"given": "Yejin"
			},
			{
				"family": "Fox",
				"given": "Dieter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GDKZBXL7",
		"type": "paper-conference",
		"container-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"source": "Google Scholar",
		"title": "Semantic HELM: A Human-Readable Memory for Reinforcement Learning",
		"title-short": "Semantic HELM",
		"URL": "https://openreview.net/forum?id=ebMPmx5mr7",
		"author": [
			{
				"family": "Paischer",
				"given": "Fabian"
			},
			{
				"family": "Adler",
				"given": "Thomas"
			},
			{
				"family": "Hofmarcher",
				"given": "Markus"
			},
			{
				"family": "Hochreiter",
				"given": "Sepp"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XEQJULAI",
		"type": "document",
		"source": "Google Scholar",
		"title": "Machine Learning Adversaries in Video Games: Using reinforcement learning in the Unity Engine to create compelling enemy characters",
		"title-short": "Machine Learning Adversaries in Video Games",
		"URL": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:1583775",
		"author": [
			{
				"family": "Nämerforslund",
				"given": "Tim"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WGG9YYZG",
		"type": "paper-conference",
		"container-title": "Proceedings of the 29th ACM International Conference on Multimedia",
		"DOI": "10.1145/3474085.3475575",
		"event-place": "Virtual Event China",
		"event-title": "MM '21: ACM Multimedia Conference",
		"ISBN": "978-1-4503-8651-7",
		"language": "en",
		"page": "4343-4352",
		"publisher": "ACM",
		"publisher-place": "Virtual Event China",
		"source": "DOI.org (Crossref)",
		"title": "ION: Instance-level Object Navigation",
		"title-short": "ION",
		"URL": "https://dl.acm.org/doi/10.1145/3474085.3475575",
		"author": [
			{
				"family": "Li",
				"given": "Weijie"
			},
			{
				"family": "Song",
				"given": "Xinhang"
			},
			{
				"family": "Bai",
				"given": "Yubing"
			},
			{
				"family": "Zhang",
				"given": "Sixian"
			},
			{
				"family": "Jiang",
				"given": "Shuqiang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					10,
					17
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZYYT47WJ",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Deep reinforcement learning for multi-modal embodied navigation",
		"URL": "https://papyrus.bib.umontreal.ca/xmlui/handle/1866/25106",
		"author": [
			{
				"family": "Weiss",
				"given": "Martin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/S9LCNMVA",
		"type": "article",
		"abstract": "In this paper, we present a state-of-the-art model and introduce a new dataset for grounded language learning. Our goal is to develop a model that can learn to follow new instructions given prior instruction-perception-action examples. We based our work on the SAIL dataset which consists of navigational instructions and actions in a maze-like environment. The new model we propose achieves the best results to date on the SAIL dataset by using an improved perceptual component that can represent relative positions of objects. We also analyze the problems with the SAIL dataset regarding its size and balance. We argue that performance on a small, fixed-size dataset is no longer a good measure to differentiate state-of-the-art models. We introduce SAILx, a synthetic dataset generator, and perform experiments where the size and balance of the dataset are controlled.",
		"note": "arXiv:1805.07952 [cs]",
		"number": "arXiv:1805.07952",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A new dataset and model for learning to understand navigational instructions",
		"URL": "http://arxiv.org/abs/1805.07952",
		"author": [
			{
				"family": "Can",
				"given": "Ozan Arkan"
			},
			{
				"family": "Yuret",
				"given": "Deniz"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					21
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4LS53R4Z",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
		"page": "2151–2162",
		"source": "Google Scholar",
		"title": "Guided Motion Diffusion for Controllable Human Motion Synthesis",
		"URL": "http://openaccess.thecvf.com/content/ICCV2023/html/Karunratanakul_Guided_Motion_Diffusion_for_Controllable_Human_Motion_Synthesis_ICCV_2023_paper.html",
		"author": [
			{
				"family": "Karunratanakul",
				"given": "Korrawe"
			},
			{
				"family": "Preechakul",
				"given": "Konpat"
			},
			{
				"family": "Suwajanakorn",
				"given": "Supasorn"
			},
			{
				"family": "Tang",
				"given": "Siyu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WQSL8KBK",
		"type": "article",
		"abstract": "Neural network-based systems can now learn to locate the referents of words and phrases in images, answer questions about visual scenes, and execute symbolic instructions as first-person actors in partially-observable worlds. To achieve this so-called grounded language learning, models must overcome challenges that infants face when learning their first words. While it is notable that models with no meaningful prior knowledge overcome these obstacles, researchers currently lack a clear understanding of how they do so, a problem that we attempt to address in this paper. For maximum control and generality, we focus on a simple neural network-based language learning agent, trained via policy-gradient methods, which can interpret single-word instructions in a simulated 3D world. Whilst the goal is not to explicitly model infant word learning, we take inspiration from experimental paradigms in developmental psychology and apply some of these to the artificial agent, exploring the conditions under which established human biases and learning effects emerge. We further propose a novel method for visualising semantic representations in the agent.",
		"note": "arXiv:1710.09867 [cs]",
		"number": "arXiv:1710.09867",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Understanding Early Word Learning in Situated Artificial Agents",
		"URL": "http://arxiv.org/abs/1710.09867",
		"author": [
			{
				"family": "Hill",
				"given": "Felix"
			},
			{
				"family": "Clark",
				"given": "Stephen"
			},
			{
				"family": "Hermann",
				"given": "Karl Moritz"
			},
			{
				"family": "Blunsom",
				"given": "Phil"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QWMMA2E2",
		"type": "article-journal",
		"container-title": "International Journal of Computer Integrated Manufacturing",
		"DOI": "10.1080/0951192X.2018.1466396",
		"ISSN": "0951-192X, 1362-3052",
		"issue": "9",
		"journalAbbreviation": "International Journal of Computer Integrated Manufacturing",
		"language": "en",
		"page": "907-920",
		"source": "DOI.org (Crossref)",
		"title": "A new approach to plan manual assembly",
		"URL": "https://www.tandfonline.com/doi/full/10.1080/0951192X.2018.1466396",
		"volume": "31",
		"author": [
			{
				"family": "Manns",
				"given": "Martin"
			},
			{
				"family": "Fischer",
				"given": "Klaus"
			},
			{
				"family": "Du",
				"given": "Han"
			},
			{
				"family": "Slusallek",
				"given": "Philip"
			},
			{
				"family": "Alexopoulos",
				"given": "Kosmas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					9,
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/K83PWWRU",
		"type": "article",
		"abstract": "When generating technical instructions, it is often convenient to describe complex objects in the world at different levels of abstraction. A novice user might need an object explained piece by piece, while for an expert, talking about the complex object (e.g. a wall or railing) directly may be more succinct and efficient. We show how to generate building instructions at different levels of abstraction in Minecraft. We introduce the use of hierarchical planning to this end, a method from AI planning which can capture the structure of complex objects neatly. A crowdsourcing evaluation shows that the choice of abstraction level matters to users, and that an abstraction strategy which balances low-level and high-level object descriptions compares favorably to ones which don't.",
		"note": "arXiv:2010.03982 [cs]",
		"number": "arXiv:2010.03982",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Generating Instructions at Different Levels of Abstraction",
		"URL": "http://arxiv.org/abs/2010.03982",
		"author": [
			{
				"family": "Köhn",
				"given": "Arne"
			},
			{
				"family": "Wichlacz",
				"given": "Julia"
			},
			{
				"family": "Torralba",
				"given": "Álvaro"
			},
			{
				"family": "Höller",
				"given": "Daniel"
			},
			{
				"family": "Hoffmann",
				"given": "Jörg"
			},
			{
				"family": "Koller",
				"given": "Alexander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					10,
					8
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/UF8AAPND",
		"type": "article",
		"abstract": "While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenarios. These capabilities extend to mobile robots in the real world - navigating to landmarks referring to visual, audio, and spatial concepts. Videos and code are available at: https://avlmaps.github.io.",
		"note": "arXiv:2303.07522 [cs]",
		"number": "arXiv:2303.07522",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Audio Visual Language Maps for Robot Navigation",
		"URL": "http://arxiv.org/abs/2303.07522",
		"author": [
			{
				"family": "Huang",
				"given": "Chenguang"
			},
			{
				"family": "Mees",
				"given": "Oier"
			},
			{
				"family": "Zeng",
				"given": "Andy"
			},
			{
				"family": "Burgard",
				"given": "Wolfram"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8M29KTEX",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "13767–13777",
		"source": "Google Scholar",
		"title": "Galactic: Scaling End-to-End Reinforcement Learning for Rearrangement at 100k Steps-Per-Second",
		"title-short": "Galactic",
		"URL": "http://openaccess.thecvf.com/content/CVPR2023/html/Berges_Galactic_Scaling_End-to-End_Reinforcement_Learning_for_Rearrangement_at_100k_Steps-per-Second_CVPR_2023_paper.html",
		"author": [
			{
				"family": "Berges",
				"given": "Vincent-Pierre"
			},
			{
				"family": "Szot",
				"given": "Andrew"
			},
			{
				"family": "Chaplot",
				"given": "Devendra Singh"
			},
			{
				"family": "Gokaslan",
				"given": "Aaron"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			},
			{
				"family": "Undersander",
				"given": "Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZAL7B3YQ",
		"type": "article",
		"abstract": "We present CHALET, a 3D house simulator with support for navigation and manipulation. CHALET includes 58 rooms and 10 house configuration, and allows to easily create new house and room layouts. CHALET supports a range of common household activities, including moving objects, toggling appliances, and placing objects inside closeable containers. The environment and actions available are designed to create a challenging domain to train and evaluate autonomous agents, including for tasks that combine language, vision, and planning in a dynamic environment.",
		"note": "arXiv:1801.07357 [cs]",
		"number": "arXiv:1801.07357",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CHALET: Cornell House Agent Learning Environment",
		"title-short": "CHALET",
		"URL": "http://arxiv.org/abs/1801.07357",
		"author": [
			{
				"family": "Yan",
				"given": "Claudia"
			},
			{
				"family": "Misra",
				"given": "Dipendra"
			},
			{
				"family": "Bennnett",
				"given": "Andrew"
			},
			{
				"family": "Walsman",
				"given": "Aaron"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					9,
					16
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VPDEF8BQ",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2022",
		"event-place": "Cham",
		"ISBN": "978-3-031-19841-0",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-031-19842-7_28",
		"page": "480-496",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "TIDEE: Tidying Up Novel Rooms Using Visuo-Semantic Commonsense Priors",
		"title-short": "TIDEE",
		"URL": "https://link.springer.com/10.1007/978-3-031-19842-7_28",
		"volume": "13699",
		"editor": [
			{
				"family": "Avidan",
				"given": "Shai"
			},
			{
				"family": "Brostow",
				"given": "Gabriel"
			},
			{
				"family": "Cissé",
				"given": "Moustapha"
			},
			{
				"family": "Farinella",
				"given": "Giovanni Maria"
			},
			{
				"family": "Hassner",
				"given": "Tal"
			}
		],
		"author": [
			{
				"family": "Sarch",
				"given": "Gabriel"
			},
			{
				"family": "Fang",
				"given": "Zhaoyuan"
			},
			{
				"family": "Harley",
				"given": "Adam W."
			},
			{
				"family": "Schydlo",
				"given": "Paul"
			},
			{
				"family": "Tarr",
				"given": "Michael J."
			},
			{
				"family": "Gupta",
				"given": "Saurabh"
			},
			{
				"family": "Fragkiadaki",
				"given": "Katerina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NB7DEWH2",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Hierarchical Modular Framework for Long Horizon Instruction Following",
		"URL": "https://openreview.net/forum?id=s-b95PMK4E6",
		"author": [
			{
				"family": "Bhambri",
				"given": "Suvaansh"
			},
			{
				"family": "Kim",
				"given": "Byeonghwi"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Choi",
				"given": "Jonghyun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5JF2WYP3",
		"type": "article",
		"abstract": "Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.",
		"note": "arXiv:1810.08272 [cs]",
		"number": "arXiv:1810.08272",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "BabyAI: A Platform to Study the Sample Efficiency of Grounded Language Learning",
		"title-short": "BabyAI",
		"URL": "http://arxiv.org/abs/1810.08272",
		"author": [
			{
				"family": "Chevalier-Boisvert",
				"given": "Maxime"
			},
			{
				"family": "Bahdanau",
				"given": "Dzmitry"
			},
			{
				"family": "Lahlou",
				"given": "Salem"
			},
			{
				"family": "Willems",
				"given": "Lucas"
			},
			{
				"family": "Saharia",
				"given": "Chitwan"
			},
			{
				"family": "Nguyen",
				"given": "Thien Huu"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/N2FZYSF6",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "University of California, San Diego",
		"source": "Google Scholar",
		"title": "Navigation in Everyday Settings Using Semantics and Language",
		"URL": "https://search.proquest.com/openview/1d712879384b26897e01c3c1c71c5377/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Madhavan",
				"given": "Srirangan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8PJX68EG",
		"type": "article",
		"abstract": "We explore the use of language as a perceptual representation for vision-and-language navigation. Our approach uses off-the-shelf vision systems (for image captioning and object detection) to convert an agent's egocentric panoramic view at each time step into natural language descriptions. We then finetune a pretrained language model to select an action, based on the current view and the trajectory history, that would best fulfill the navigation instructions. In contrast to the standard setup which adapts a pretrained language model to work directly with continuous visual features from pretrained vision models, our approach instead uses (discrete) language as the perceptual representation. We explore two use cases of our language-based navigation (LangNav) approach on the R2R vision-and-language navigation benchmark: generating synthetic trajectories from a prompted large language model (GPT-4) with which to finetune a smaller language model; and sim-to-real transfer where we transfer a policy learned on a simulated environment (ALFRED) to a real-world environment (R2R). Our approach is found to improve upon strong baselines that rely on visual features in settings where only a few gold trajectories (10-100) are available, demonstrating the potential of using language as a perceptual representation for navigation tasks.",
		"note": "arXiv:2310.07889 [cs]",
		"number": "arXiv:2310.07889",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "LangNav: Language as a Perceptual Representation for Navigation",
		"title-short": "LangNav",
		"URL": "http://arxiv.org/abs/2310.07889",
		"author": [
			{
				"family": "Pan",
				"given": "Bowen"
			},
			{
				"family": "Panda",
				"given": "Rameswar"
			},
			{
				"family": "Jin",
				"given": "SouYoung"
			},
			{
				"family": "Feris",
				"given": "Rogerio"
			},
			{
				"family": "Oliva",
				"given": "Aude"
			},
			{
				"family": "Isola",
				"given": "Phillip"
			},
			{
				"family": "Kim",
				"given": "Yoon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KCECFWVY",
		"type": "article",
		"abstract": "We study the problem of continually training an instruction-following agent through feedback provided by users during collaborative interactions. During interaction, human users instruct an agent using natural language, and provide realtime binary feedback as they observe the agent's instruction execution. We cast learning as a contextual bandit problem, converting the user feedback to immediate reward. We evaluate through multiple rounds of human-agent interactions, demonstrating 15.4% absolute improvement in instruction execution over time. We also show our approach is robust to several design variations, and that the feedback signal is roughly equivalent to the learning signal of supervised demonstration data.",
		"note": "arXiv:2212.09710 [cs]",
		"number": "arXiv:2212.09710",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Continual Learning for Instruction Following from Realtime Feedback",
		"URL": "http://arxiv.org/abs/2212.09710",
		"author": [
			{
				"family": "Suhr",
				"given": "Alane"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4K8AIMIE",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Ta-Chung Chi Carnegie Mellon University tachungc@ andrew. cmu. edu",
		"URL": "http://alborz-geramifard.com/workshops/neurips19-Conversational-AI/Papers/4.pdf",
		"author": [
			{
				"family": "Kim",
				"given": "Seokhwan"
			},
			{
				"family": "Alexa",
				"given": "A. I."
			},
			{
				"family": "Shen",
				"given": "Minmin"
			},
			{
				"family": "Hakkani-tur",
				"given": "Dilek"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AM7TDBA2",
		"type": "article",
		"abstract": "The domain of Embodied AI, in which agents learn to complete tasks through interaction with their environment from egocentric observations, has experienced substantial growth with the advent of deep reinforcement learning and increased interest from the computer vision, NLP, and robotics communities. This growth has been facilitated by the creation of a large number of simulated environments (such as AI2-THOR, Habitat and CARLA), tasks (like point navigation, instruction following, and embodied question answering), and associated leaderboards. While this diversity has been beneficial and organic, it has also fragmented the community: a huge amount of effort is required to do something as simple as taking a model trained in one environment and testing it in another. This discourages good science. We introduce AllenAct, a modular and flexible learning framework designed with a focus on the unique requirements of Embodied AI research. AllenAct provides first-class support for a growing collection of embodied environments, tasks and algorithms, provides reproductions of state-of-the-art models and includes extensive documentation, tutorials, start-up code, and pre-trained models. We hope that our framework makes Embodied AI more accessible and encourages new researchers to join this exciting area. The framework can be accessed at: https://allenact.org/",
		"note": "arXiv:2008.12760 [cs]",
		"number": "arXiv:2008.12760",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "AllenAct: A Framework for Embodied AI Research",
		"title-short": "AllenAct",
		"URL": "http://arxiv.org/abs/2008.12760",
		"author": [
			{
				"family": "Weihs",
				"given": "Luca"
			},
			{
				"family": "Salvador",
				"given": "Jordi"
			},
			{
				"family": "Kotar",
				"given": "Klemen"
			},
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Zeng",
				"given": "Kuo-Hao"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Kembhavi",
				"given": "Aniruddha"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					8,
					28
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GFH2VGQK",
		"type": "paper-conference",
		"container-title": "2nd Workshop on Language and Robot Learning: Language as Grounding",
		"source": "Google Scholar",
		"title": "Transcribe3D: Grounding LLMs Using Transcribed Information for 3D Referential Reasoning with Self-Corrected Finetuning",
		"title-short": "Transcribe3D",
		"URL": "https://openreview.net/forum?noteId=BqwDAp4cY1",
		"author": [
			{
				"family": "Fang",
				"given": "Jiading"
			},
			{
				"family": "Tan",
				"given": "Xiangshan"
			},
			{
				"family": "Lin",
				"given": "Shengjie"
			},
			{
				"family": "Mei",
				"given": "Hongyuan"
			},
			{
				"family": "Walter",
				"given": "Matthew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/N62WPHXT",
		"type": "article-journal",
		"abstract": "Abstract\n            Spoken language is one of the most efficientways to instruct robots about performing domestic tasks. However, the state of the environment has to be considered to plan and execute actions successfully. We propose a system that learns to recognise the user’s intention and map it to a goal. A reinforcement learning (RL) system then generates a sequence of actions toward this goal considering the state of the environment. A novel contribution in this paper is the use of symbolic representations for both input and output of a neural Deep Q-network (DQN), which enables it to be used in a hybrid system. To show the effectiveness of our approach, the Tell-Me-Dave corpus is used to train an intention detection model and in a second step an RL agent generates the sequences of actions towards the detected objective, represented by a set of state predicates. We show that the system can successfully recognise command sequences fromthis corpus aswell as train the deep- RL network with symbolic input.We further show that the performance can be significantly increased by exploiting the symbolic representation to generate intermediate rewards.",
		"container-title": "Paladyn, Journal of Behavioral Robotics",
		"DOI": "10.1515/pjbr-2018-0026",
		"ISSN": "2081-4836",
		"issue": "1",
		"language": "en",
		"page": "358-373",
		"source": "DOI.org (Crossref)",
		"title": "Deep reinforcement learning using compositional representations for performing instructions",
		"URL": "https://www.degruyter.com/document/doi/10.1515/pjbr-2018-0026/html",
		"volume": "9",
		"author": [
			{
				"family": "Zamani",
				"given": "Mohammad Ali"
			},
			{
				"family": "Magg",
				"given": "Sven"
			},
			{
				"family": "Weber",
				"given": "Cornelius"
			},
			{
				"family": "Wermter",
				"given": "Stefan"
			},
			{
				"family": "Fu",
				"given": "Di"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2C5SJGVV",
		"type": "paper-conference",
		"container-title": "2019 11th International Conference on Knowledge and Systems Engineering (KSE)",
		"page": "1–7",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Reinforcement learning based navigation with semantic knowledge of indoor environments",
		"URL": "https://ieeexplore.ieee.org/abstract/document/8919366/",
		"author": [
			{
				"family": "Nguyen",
				"given": "Tai-Long"
			},
			{
				"family": "Nguyen",
				"given": "Do-Van"
			},
			{
				"family": "Le",
				"given": "Thanh-Ha"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3CU9E596",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
		"page": "1594–1603",
		"source": "Google Scholar",
		"title": "Self-motivated communication agent for real-world vision-dialog navigation",
		"URL": "http://openaccess.thecvf.com/content/ICCV2021/html/Zhu_Self-Motivated_Communication_Agent_for_Real-World_Vision-Dialog_Navigation_ICCV_2021_paper.html",
		"author": [
			{
				"family": "Zhu",
				"given": "Yi"
			},
			{
				"family": "Weng",
				"given": "Yue"
			},
			{
				"family": "Zhu",
				"given": "Fengda"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			},
			{
				"family": "Ye",
				"given": "Qixiang"
			},
			{
				"family": "Lu",
				"given": "Yutong"
			},
			{
				"family": "Jiao",
				"given": "Jianbin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/N75HHFQU",
		"type": "article",
		"abstract": "We study a collaborative scenario where a user not only instructs a system to complete tasks, but also acts alongside it. This allows the user to adapt to the system abilities by changing their language or deciding to simply accomplish some tasks themselves, and requires the system to effectively recover from errors as the user strategically assigns it new goals. We build a game environment to study this scenario, and learn to map user instructions to system actions. We introduce a learning approach focused on recovery from cascading errors between instructions, and modeling methods to explicitly reason about instructions with multiple goals. We evaluate with a new evaluation protocol using recorded interactions and online games with human users, and observe how users adapt to the system abilities.",
		"note": "arXiv:1910.03655 [cs]",
		"number": "arXiv:1910.03655",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Executing Instructions in Situated Collaborative Interactions",
		"URL": "http://arxiv.org/abs/1910.03655",
		"author": [
			{
				"family": "Suhr",
				"given": "Alane"
			},
			{
				"family": "Yan",
				"given": "Claudia"
			},
			{
				"family": "Schluger",
				"given": "Charlotte"
			},
			{
				"family": "Yu",
				"given": "Stanley"
			},
			{
				"family": "Khader",
				"given": "Hadi"
			},
			{
				"family": "Mouallem",
				"given": "Marwa"
			},
			{
				"family": "Zhang",
				"given": "Iris"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					22
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HD4UEUXJ",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
		"note": "publisher: IEEE",
		"source": "Google Scholar",
		"title": "Towards Deviation-Robust Agent Navigation Via Perturbation-Aware Contrastive Learning",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10120966/",
		"author": [
			{
				"family": "Lin",
				"given": "Bingqian"
			},
			{
				"family": "Long",
				"given": "Yanxin"
			},
			{
				"family": "Zhu",
				"given": "Yi"
			},
			{
				"family": "Zhu",
				"given": "Fengda"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			},
			{
				"family": "Ye",
				"given": "Qixiang"
			},
			{
				"family": "Lin",
				"given": "Liang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/W57UQRT8",
		"type": "article-journal",
		"container-title": "Frontiers in artificial intelligence",
		"note": "publisher: Frontiers Media SA",
		"page": "550030",
		"source": "Google Scholar",
		"title": "Explainable ai and reinforcement learning—a systematic review of current approaches and trends",
		"URL": "https://www.frontiersin.org/articles/10.3389/frai.2021.550030/full",
		"volume": "4",
		"author": [
			{
				"family": "Wells",
				"given": "Lindsay"
			},
			{
				"family": "Bednarz",
				"given": "Tomasz"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4TUMDSSZ",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "3064–3073",
		"source": "Google Scholar",
		"title": "Room-and-object aware knowledge reasoning for remote embodied referring expression",
		"URL": "http://openaccess.thecvf.com/content/CVPR2021/html/Gao_Room-and-Object_Aware_Knowledge_Reasoning_for_Remote_Embodied_Referring_Expression_CVPR_2021_paper.html",
		"author": [
			{
				"family": "Gao",
				"given": "Chen"
			},
			{
				"family": "Chen",
				"given": "Jinyu"
			},
			{
				"family": "Liu",
				"given": "Si"
			},
			{
				"family": "Wang",
				"given": "Luting"
			},
			{
				"family": "Zhang",
				"given": "Qiong"
			},
			{
				"family": "Wu",
				"given": "Qi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RTJWMD2F",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
		"note": "publisher: IEEE",
		"source": "Google Scholar",
		"title": "Room-Object Entity Prompting and Reasoning for Embodied Referring Expression",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10292872/",
		"author": [
			{
				"family": "Gao",
				"given": "Chen"
			},
			{
				"family": "Liu",
				"given": "Si"
			},
			{
				"family": "Chen",
				"given": "Jinyu"
			},
			{
				"family": "Wang",
				"given": "Luting"
			},
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Li",
				"given": "Bo"
			},
			{
				"family": "Tian",
				"given": "Qi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/UT8PDYNM",
		"type": "article",
		"abstract": "The question of whether deep neural networks are good at generalising beyond their immediate training experience is of critical importance for learning-based approaches to AI. Here, we consider tests of out-of-sample generalisation that require an agent to respond to never-seen-before instructions by manipulating and positioning objects in a 3D Unity simulated room. We first describe a comparatively generic agent architecture that exhibits strong performance on these tests. We then identify three aspects of the training regime and environment that make a significant difference to its performance: (a) the number of object/word experiences in the training set; (b) the visual invariances afforded by the agent's perspective, or frame of reference; and (c) the variety of visual input inherent in the perceptual aspect of the agent's perception. Our findings indicate that the degree of generalisation that networks exhibit can depend critically on particulars of the environment in which a given task is instantiated. They further suggest that the propensity for neural networks to generalise in systematic ways may increase if, like human children, those networks have access to many frames of richly varying, multi-modal observations as they learn.",
		"note": "arXiv:1910.00571 [cs]",
		"number": "arXiv:1910.00571",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Environmental drivers of systematicity and generalization in a situated agent",
		"URL": "http://arxiv.org/abs/1910.00571",
		"author": [
			{
				"family": "Hill",
				"given": "Felix"
			},
			{
				"family": "Lampinen",
				"given": "Andrew"
			},
			{
				"family": "Schneider",
				"given": "Rosalia"
			},
			{
				"family": "Clark",
				"given": "Stephen"
			},
			{
				"family": "Botvinick",
				"given": "Matthew"
			},
			{
				"family": "McClelland",
				"given": "James L."
			},
			{
				"family": "Santoro",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					2,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3XKHLGMU",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
		"page": "1888–1897",
		"source": "Google Scholar",
		"title": "Factorizing perception and policy for interactive instruction following",
		"URL": "http://openaccess.thecvf.com/content/ICCV2021/html/Singh_Factorizing_Perception_and_Policy_for_Interactive_Instruction_Following_ICCV_2021_paper.html",
		"author": [
			{
				"family": "Singh",
				"given": "Kunal Pratap"
			},
			{
				"family": "Bhambri",
				"given": "Suvaansh"
			},
			{
				"family": "Kim",
				"given": "Byeonghwi"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Choi",
				"given": "Jonghyun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/582ZREWE",
		"type": "thesis",
		"genre": "PhD Thesis",
		"source": "Google Scholar",
		"title": "Efficient Deep Reinforcement Learning via Planning, Generalization, and Improved Exploration",
		"URL": "https://deepblue.lib.umich.edu/handle/2027.42/145829",
		"author": [
			{
				"family": "Oh",
				"given": "Junhyuk"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/G8FUZ7RI",
		"type": "article",
		"abstract": "Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game's original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent's current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM's answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context \"reasoning\" induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.",
		"note": "arXiv:2305.15486 [cs]",
		"number": "arXiv:2305.15486",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning",
		"title-short": "SPRING",
		"URL": "http://arxiv.org/abs/2305.15486",
		"author": [
			{
				"family": "Wu",
				"given": "Yue"
			},
			{
				"family": "Prabhumoye",
				"given": "Shrimai"
			},
			{
				"family": "Min",
				"given": "So Yeon"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			},
			{
				"family": "Azaria",
				"given": "Amos"
			},
			{
				"family": "Mitchell",
				"given": "Tom"
			},
			{
				"family": "Li",
				"given": "Yuanzhi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					29
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5GZZFLVF",
		"type": "article-journal",
		"note": "publisher: The Australian National University",
		"source": "Google Scholar",
		"title": "Vision and Language Learning: From Image Captioning and Visual Question Answering towards Embodied Agents",
		"title-short": "Vision and Language Learning",
		"URL": "https://openresearch-repository.anu.edu.au/handle/1885/164018",
		"author": [
			{
				"family": "Anderson",
				"given": "Peter James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DCVPWJQA",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "An End-to-end Transformer-based Model for Interactive Grounded Language Understanding",
		"URL": "http://sag.art.uniroma2.it/NL4AI/wp-content/uploads/2023/10/paper18.pdf",
		"author": [
			{
				"family": "Hromei",
				"given": "Claudiu D."
			},
			{
				"family": "Margiotta",
				"given": "Daniele"
			},
			{
				"family": "Croce",
				"given": "Danilo"
			},
			{
				"family": "Basili",
				"given": "Roberto"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/R3CIHVJG",
		"type": "paper-conference",
		"container-title": "International Conference on Machine Learning",
		"page": "11868–11890",
		"publisher": "PMLR",
		"source": "Google Scholar",
		"title": "Tell me why! explanations support learning relational and causal structure",
		"URL": "https://proceedings.mlr.press/v162/lampinen22a.html",
		"author": [
			{
				"family": "Lampinen",
				"given": "Andrew K."
			},
			{
				"family": "Roy",
				"given": "Nicholas"
			},
			{
				"family": "Dasgupta",
				"given": "Ishita"
			},
			{
				"family": "Chan",
				"given": "Stephanie CY"
			},
			{
				"family": "Tam",
				"given": "Allison"
			},
			{
				"family": "Mcclelland",
				"given": "James"
			},
			{
				"family": "Yan",
				"given": "Chen"
			},
			{
				"family": "Santoro",
				"given": "Adam"
			},
			{
				"family": "Rabinowitz",
				"given": "Neil C."
			},
			{
				"family": "Wang",
				"given": "Jane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4CYCSMW9",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Language-modulated Actions for Safer Human-Robot Interaction using Deep Reinforcement Learning",
		"URL": "https://www2.informatik.uni-hamburg.de/wtm/secure-project/secure-robots.eu/fileadmin/introduction/images/research_results/Zamani_files/Zamani-icra-phd-forum.pdf",
		"author": [
			{
				"family": "Zamani",
				"given": "Mohammad Ali"
			},
			{
				"family": "Magg",
				"given": "Sven"
			},
			{
				"family": "Weber",
				"given": "Cornelius"
			},
			{
				"family": "Wermter",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IRZPQFTW",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:1908.05135",
		"source": "Google Scholar",
		"title": "Mastering emergent language: learning to guide in simulated navigation",
		"title-short": "Mastering emergent language",
		"URL": "https://arxiv.org/abs/1908.05135",
		"author": [
			{
				"family": "Mul",
				"given": "Mathijs"
			},
			{
				"family": "Bouchacourt",
				"given": "Diane"
			},
			{
				"family": "Bruni",
				"given": "Elia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YGWYY2IZ",
		"type": "paper-conference",
		"container-title": "2023 20th International Conference on Ubiquitous Robots (UR)",
		"page": "503–509",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Good Time to Ask: A Learning Framework for Asking for Help in Embodied Visual Navigation",
		"title-short": "Good Time to Ask",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10202397/",
		"author": [
			{
				"family": "Zhang",
				"given": "Jenny"
			},
			{
				"family": "Yu",
				"given": "Samson"
			},
			{
				"family": "Duan",
				"given": "Jiafei"
			},
			{
				"family": "Tan",
				"given": "Cheston"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/B72BTQ4V",
		"type": "article",
		"abstract": "We describe an ongoing project in learning to perform primitive actions from demonstrations using an interactive interface. In our previous work, we have used demonstrations captured from humans performing actions as training samples for a neural network-based trajectory model of actions to be performed by a computational agent in novel setups. We found that our original framework had some limitations that we hope to overcome by incorporating communication between the human and the computational agent, using the interaction between them to fine-tune the model learned by the machine. We propose a framework that uses multimodal human-computer interaction to teach action concepts to machines, making use of both live demonstration and communication through natural language, as two distinct teaching modalities, while requiring few training samples.",
		"note": "arXiv:1810.00838 [cs]",
		"number": "arXiv:1810.00838",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Multimodal Interactive Learning of Primitive Actions",
		"URL": "http://arxiv.org/abs/1810.00838",
		"author": [
			{
				"family": "Do",
				"given": "Tuan"
			},
			{
				"family": "Krishnaswamy",
				"given": "Nikhil"
			},
			{
				"family": "Rim",
				"given": "Kyeongmin"
			},
			{
				"family": "Pustejovsky",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/H828MB7W",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "University of Piraeus (Greece)",
		"source": "Google Scholar",
		"title": "Behavior of Intelligent Agents Using Reinforcement Learning Through Unity's ML-Agent’s Toolkit",
		"URL": "https://search.proquest.com/openview/cf7c472f9bbcf6c5fc8d0db7aa605570/1?pq-origsite=gscholar&cbl=2026366&diss=y",
		"author": [
			{
				"family": "Christos",
				"given": "Davillas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/S6WDFHVX",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:2304.04321",
		"source": "Google Scholar",
		"title": "ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes",
		"title-short": "ARNOLD",
		"URL": "https://arxiv.org/abs/2304.04321",
		"author": [
			{
				"family": "Gong",
				"given": "Ran"
			},
			{
				"family": "Huang",
				"given": "Jiangyong"
			},
			{
				"family": "Zhao",
				"given": "Yizhou"
			},
			{
				"family": "Geng",
				"given": "Haoran"
			},
			{
				"family": "Gao",
				"given": "Xiaofeng"
			},
			{
				"family": "Wu",
				"given": "Qingyang"
			},
			{
				"family": "Ai",
				"given": "Wensi"
			},
			{
				"family": "Zhou",
				"given": "Ziheng"
			},
			{
				"family": "Terzopoulos",
				"given": "Demetri"
			},
			{
				"family": "Zhu",
				"given": "Song-Chun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EDMQ4XIW",
		"type": "paper-conference",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"note": "issue: 01",
		"page": "2911–2918",
		"source": "Google Scholar",
		"title": "Combining deep learning and qualitative spatial reasoning to learn complex structures from sparse examples with noise",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/4146",
		"volume": "33",
		"author": [
			{
				"family": "Krishnaswamy",
				"given": "Nikhil"
			},
			{
				"family": "Friedman",
				"given": "Scott"
			},
			{
				"family": "Pustejovsky",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ZUX79GXW",
		"type": "book",
		"publisher": "University of California, Los Angeles",
		"source": "Google Scholar",
		"title": "Multimodal Communication for Embodied Human-Robot Interaction with Natural Gestures",
		"URL": "https://search.proquest.com/openview/153a3a9a8da6a431798d07245431234f/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Wu",
				"given": "Qi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VTNF3LJR",
		"type": "article",
		"abstract": "Object rearrangement is the problem of enabling a robot to identify the correct object placement in a complex environment. Prior work on object rearrangement has explored a diverse set of techniques for following user instructions to achieve some desired goal state. Logical predicates, images of the goal scene, and natural language descriptions have all been used to instruct a robot in how to arrange objects. In this work, we argue that burdening the user with specifying goal scenes is not necessary in partially-arranged environments, such as common household settings. Instead, we show that contextual cues from partially arranged scenes (i.e., the placement of some number of pre-arranged objects in the environment) provide sufficient context to enable robots to perform object rearrangement \\textit{without any explicit user goal specification}. We introduce ConSOR, a Context-aware Semantic Object Rearrangement framework that utilizes contextual cues from a partially arranged initial state of the environment to complete the arrangement of new objects, without explicit goal specification from the user. We demonstrate that ConSOR strongly outperforms two baselines in generalizing to novel object arrangements and unseen object categories. The code and data can be found at https://github.com/kartikvrama/consor.",
		"note": "arXiv:2310.00371 [cs]",
		"number": "arXiv:2310.00371",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ConSOR: A Context-Aware Semantic Object Rearrangement Framework for Partially Arranged Scenes",
		"title-short": "ConSOR",
		"URL": "http://arxiv.org/abs/2310.00371",
		"author": [
			{
				"family": "Ramachandruni",
				"given": "Kartik"
			},
			{
				"family": "Zuo",
				"given": "Max"
			},
			{
				"family": "Chernova",
				"given": "Sonia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					30
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HRULQGBR",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition",
		"page": "4497–4506",
		"source": "Google Scholar",
		"title": "Manipulathor: A framework for visual object manipulation",
		"title-short": "Manipulathor",
		"URL": "http://openaccess.thecvf.com/content/CVPR2021/html/Ehsani_ManipulaTHOR_A_Framework_for_Visual_Object_Manipulation_CVPR_2021_paper.html",
		"author": [
			{
				"family": "Ehsani",
				"given": "Kiana"
			},
			{
				"family": "Han",
				"given": "Winson"
			},
			{
				"family": "Herrasti",
				"given": "Alvaro"
			},
			{
				"family": "VanderBilt",
				"given": "Eli"
			},
			{
				"family": "Weihs",
				"given": "Luca"
			},
			{
				"family": "Kolve",
				"given": "Eric"
			},
			{
				"family": "Kembhavi",
				"given": "Aniruddha"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4T886W5L",
		"type": "article",
		"abstract": "Physically rearranging objects is an important capability for embodied agents. Visual room rearrangement evaluates an agent's ability to rearrange objects in a room to a desired goal based solely on visual input. We propose a simple yet effective method for this problem: (1) search for and map which objects need to be rearranged, and (2) rearrange each object until the task is complete. Our approach consists of an off-the-shelf semantic segmentation model, voxel-based semantic map, and semantic search policy to efficiently find objects that need to be rearranged. On the AI2-THOR Rearrangement Challenge, our method improves on current state-of-the-art end-to-end reinforcement learning-based methods that learn visual rearrangement policies from 0.53% correct rearrangement to 16.56%, using only 2.7% as many samples from the environment.",
		"note": "arXiv:2206.13396 [cs]",
		"number": "arXiv:2206.13396",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Simple Approach for Visual Rearrangement: 3D Mapping and Semantic Search",
		"title-short": "A Simple Approach for Visual Rearrangement",
		"URL": "http://arxiv.org/abs/2206.13396",
		"author": [
			{
				"family": "Trabucco",
				"given": "Brandon"
			},
			{
				"family": "Sigurdsson",
				"given": "Gunnar"
			},
			{
				"family": "Piramuthu",
				"given": "Robinson"
			},
			{
				"family": "Sukhatme",
				"given": "Gaurav S."
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RE3CU5EH",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Université Grenoble Alpes [2020-....]",
		"source": "Google Scholar",
		"title": "Learning goal-oriented agents with limited supervision",
		"URL": "https://theses.hal.science/tel-04266339/",
		"author": [
			{
				"family": "Mezghani",
				"given": "Lina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/MZCGIDVC",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Carnegie Mellon University",
		"source": "Google Scholar",
		"title": "End-to-End Multimodal Learning for Situated Dialogue Systems",
		"URL": "https://search.proquest.com/openview/421a42f9d061ab5c946a763a2eee241d/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Chao",
				"given": "Guan-Lin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FZ7KX93U",
		"type": "article",
		"abstract": "Recent work has shown how predictive modeling can endow agents with rich knowledge of their surroundings, improving their ability to act in complex environments. We propose question-answering as a general paradigm to decode and understand the representations that such agents develop, applying our method to two recent approaches to predictive modeling -action-conditional CPC (Guo et al., 2018) and SimCore (Gregor et al., 2019). After training agents with these predictive objectives in a visually-rich, 3D environment with an assortment of objects, colors, shapes, and spatial configurations, we probe their internal state representations with synthetic (English) questions, without backpropagating gradients from the question-answering decoder into the agent. The performance of different agents when probed this way reveals that they learn to encode factual, and seemingly compositional, information about objects, properties and spatial relations from their physical environment. Our approach is intuitive, i.e. humans can easily interpret responses of the model as opposed to inspecting continuous vectors, and model-agnostic, i.e. applicable to any modeling approach. By revealing the implicit knowledge of objects, quantities, properties and relations acquired by agents as they learn, question-conditional agent probing can stimulate the design and development of stronger predictive learning objectives.",
		"note": "arXiv:2006.01016 [cs]",
		"number": "arXiv:2006.01016",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Probing Emergent Semantics in Predictive Agents via Question Answering",
		"URL": "http://arxiv.org/abs/2006.01016",
		"author": [
			{
				"family": "Das",
				"given": "Abhishek"
			},
			{
				"family": "Carnevale",
				"given": "Federico"
			},
			{
				"family": "Merzic",
				"given": "Hamza"
			},
			{
				"family": "Rimell",
				"given": "Laura"
			},
			{
				"family": "Schneider",
				"given": "Rosalia"
			},
			{
				"family": "Abramson",
				"given": "Josh"
			},
			{
				"family": "Hung",
				"given": "Alden"
			},
			{
				"family": "Ahuja",
				"given": "Arun"
			},
			{
				"family": "Clark",
				"given": "Stephen"
			},
			{
				"family": "Wayne",
				"given": "Gregory"
			},
			{
				"family": "Hill",
				"given": "Felix"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					6,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JTU7KMED",
		"type": "article-journal",
		"container-title": "Frontiers in Artificial Intelligence",
		"note": "publisher: Frontiers",
		"page": "774752",
		"source": "Google Scholar",
		"title": "Affordance embeddings for situated language understanding",
		"URL": "https://www.frontiersin.org/articles/10.3389/frai.2022.774752/full",
		"volume": "5",
		"author": [
			{
				"family": "Krishnaswamy",
				"given": "Nikhil"
			},
			{
				"family": "Pustejovsky",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/S6Z26JZZ",
		"type": "article",
		"abstract": "We build a virtual agent for learning language in a 2D maze-like world. The agent sees images of the surrounding environment, listens to a virtual teacher, and takes actions to receive rewards. It interactively learns the teacher's language from scratch based on two language use cases: sentence-directed navigation and question answering. It learns simultaneously the visual representations of the world, the language, and the action control. By disentangling language grounding from other computational routines and sharing a concept detection function between language grounding and prediction, the agent reliably interpolates and extrapolates to interpret sentences that contain new word combinations or new words missing from training sentences. The new words are transferred from the answers of language prediction. Such a language ability is trained and evaluated on a population of over 1.6 million distinct sentences consisting of 119 object words, 8 color words, 9 spatial-relation words, and 50 grammatical words. The proposed model significantly outperforms five comparison methods for interpreting zero-shot sentences. In addition, we demonstrate human-interpretable intermediate outputs of the model in the appendix.",
		"note": "arXiv:1802.01433 [cs]",
		"number": "arXiv:1802.01433",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Interactive Grounded Language Acquisition and Generalization in a 2D World",
		"URL": "http://arxiv.org/abs/1802.01433",
		"author": [
			{
				"family": "Yu",
				"given": "Haonan"
			},
			{
				"family": "Zhang",
				"given": "Haichao"
			},
			{
				"family": "Xu",
				"given": "Wei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					8,
					13
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7E2SUJ4Q",
		"type": "article",
		"abstract": "Skills learned through (deep) reinforcement learning often generalizes poorly across domains and re-training is necessary when presented with a new task. We present a framework that combines techniques in \\textit{formal methods} with \\textit{reinforcement learning} (RL). The methods we provide allows for convenient specification of tasks with logical expressions, learns hierarchical policies (meta-controller and low-level controllers) with well-defined intrinsic rewards, and construct new skills from existing ones with little to no additional exploration. We evaluate the proposed methods in a simple grid world simulation as well as a more complicated kitchen environment in AI2Thor",
		"note": "arXiv:1711.00129 [cs]",
		"number": "arXiv:1711.00129",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Automata-Guided Hierarchical Reinforcement Learning for Skill Composition",
		"URL": "http://arxiv.org/abs/1711.00129",
		"author": [
			{
				"family": "Li",
				"given": "Xiao"
			},
			{
				"family": "Ma",
				"given": "Yao"
			},
			{
				"family": "Belta",
				"given": "Calin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2Y8N77CZ",
		"type": "paper-conference",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"note": "issue: 7",
		"page": "7534–7541",
		"source": "Google Scholar",
		"title": "Learning Parameterized Task Structure for Generalization to Unseen Entities",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/20718",
		"volume": "36",
		"author": [
			{
				"family": "Liu",
				"given": "Anthony"
			},
			{
				"family": "Sohn",
				"given": "Sungryull"
			},
			{
				"family": "Qazwini",
				"given": "Mahdi"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EFT24XUK",
		"type": "article",
		"abstract": "We propose and address a novel few-shot RL problem, where a task is characterized by a subtask graph which describes a set of subtasks and their dependencies that are unknown to the agent. The agent needs to quickly adapt to the task over few episodes during adaptation phase to maximize the return in the test phase. Instead of directly learning a meta-policy, we develop a Meta-learner with Subtask Graph Inference(MSGI), which infers the latent parameter of the task by interacting with the environment and maximizes the return given the latent parameter. To facilitate learning, we adopt an intrinsic reward inspired by upper confidence bound (UCB) that encourages efficient exploration. Our experiment results on two grid-world domains and StarCraft II environments show that the proposed method is able to accurately infer the latent task parameter, and to adapt more efficiently than existing meta RL and hierarchical RL methods.",
		"note": "arXiv:2001.00248 [cs, stat]",
		"number": "arXiv:2001.00248",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Meta Reinforcement Learning with Autonomous Inference of Subtask Dependencies",
		"URL": "http://arxiv.org/abs/2001.00248",
		"author": [
			{
				"family": "Sohn",
				"given": "Sungryull"
			},
			{
				"family": "Woo",
				"given": "Hyunjae"
			},
			{
				"family": "Choi",
				"given": "Jongwook"
			},
			{
				"family": "Lee",
				"given": "Honglak"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					4,
					13
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WYJT7VRA",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "7655–7670",
		"source": "Google Scholar",
		"title": "Explainability Via Causal Self-Talk",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/324bb74b6d557428e21528379eeb7a0c-Abstract-Conference.html",
		"volume": "35",
		"author": [
			{
				"family": "Roy",
				"given": "Nicholas A."
			},
			{
				"family": "Kim",
				"given": "Junkyung"
			},
			{
				"family": "Rabinowitz",
				"given": "Neil"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NTG8D3WH",
		"type": "paper-conference",
		"container-title": "The Eleventh International Conference on Learning Representations",
		"source": "Google Scholar",
		"title": "A Simple Approach for Visual Room Rearrangement: 3D Mapping and Semantic Search",
		"title-short": "A Simple Approach for Visual Room Rearrangement",
		"URL": "https://openreview.net/forum?id=1C6nCCaRe6p",
		"author": [
			{
				"family": "Trabucco",
				"given": "Brandon"
			},
			{
				"family": "Sigurdsson",
				"given": "Gunnar A."
			},
			{
				"family": "Piramuthu",
				"given": "Robinson"
			},
			{
				"family": "Sukhatme",
				"given": "Gaurav S."
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AVIL5QXI",
		"type": "article",
		"abstract": "Recent advances in robot learning have shown promise in enabling robots to perform a variety of manipulation tasks and generalize to novel scenarios. One of the key contributing factors to this progress is the scale of robot data used to train the models. To obtain large-scale datasets, prior approaches have relied on either demonstrations requiring high human involvement or engineering-heavy autonomous data collection schemes, both of which are challenging to scale. To mitigate this issue, we propose an alternative route and leverage text-to-image foundation models widely used in computer vision and natural language processing to obtain meaningful data for robot learning without requiring additional robot data. We term our method Robot Learning with Semantically Imagened Experience (ROSIE). Specifically, we make use of the state of the art text-to-image diffusion models and perform aggressive data augmentation on top of our existing robotic manipulation datasets via inpainting various unseen objects for manipulation, backgrounds, and distractors with text guidance. Through extensive real-world experiments, we show that manipulation policies trained on data augmented this way are able to solve completely unseen tasks with new objects and can behave more robustly w.r.t. novel distractors. In addition, we find that we can improve the robustness and generalization of high-level robot learning tasks such as success detection through training with the diffusion-based data augmentation. The project's website and videos can be found at diffusion-rosie.github.io",
		"note": "arXiv:2302.11550 [cs]",
		"number": "arXiv:2302.11550",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Scaling Robot Learning with Semantically Imagined Experience",
		"URL": "http://arxiv.org/abs/2302.11550",
		"author": [
			{
				"family": "Yu",
				"given": "Tianhe"
			},
			{
				"family": "Xiao",
				"given": "Ted"
			},
			{
				"family": "Stone",
				"given": "Austin"
			},
			{
				"family": "Tompson",
				"given": "Jonathan"
			},
			{
				"family": "Brohan",
				"given": "Anthony"
			},
			{
				"family": "Wang",
				"given": "Su"
			},
			{
				"family": "Singh",
				"given": "Jaspiar"
			},
			{
				"family": "Tan",
				"given": "Clayton"
			},
			{
				"family": "M",
				"given": "Dee"
			},
			{
				"family": "Peralta",
				"given": "Jodilyn"
			},
			{
				"family": "Ichter",
				"given": "Brian"
			},
			{
				"family": "Hausman",
				"given": "Karol"
			},
			{
				"family": "Xia",
				"given": "Fei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					2,
					22
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8CRRR66T",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Imitation Learning of Robot Policies using Language, Vision and Motion",
		"URL": "https://openreview.net/forum?id=Bkg5LgrYwS",
		"author": [
			{
				"family": "Stepputtis",
				"given": "Simon"
			},
			{
				"family": "Campbell",
				"given": "Joseph"
			},
			{
				"family": "Phielipp",
				"given": "Mariano"
			},
			{
				"family": "Baral",
				"given": "Chitta"
			},
			{
				"family": "Amor",
				"given": "Heni Ben"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EWRLXA75",
		"type": "paper-conference",
		"container-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"source": "Google Scholar",
		"title": "Efficient Policy Adaptation with Contrastive Prompt Ensemble for Embodied Agents",
		"URL": "https://openreview.net/forum?id=Ny3GcHLyzj",
		"author": [
			{
				"family": "Choi",
				"given": "Wonje"
			},
			{
				"family": "Kim",
				"given": "Woo Kyung"
			},
			{
				"family": "Kim",
				"given": "SeungHyun"
			},
			{
				"family": "Woo",
				"given": "Honguk"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HNWMML63",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Training Machine Learning Agents in a 3D Game Engine",
		"URL": "https://scholarworks.uark.edu/csceuht/50/",
		"author": [
			{
				"family": "Calderon",
				"given": "Diego"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9J2P77LU",
		"type": "article",
		"abstract": "Our paper focuses on the research of the possibility for speech recognition intelligent agents to assist the elderly and disabled people's lives, to improve their life quality by utilizing cutting-edge technologies. After researching the attitude of elderly and disabled people toward the household agent, we propose a design framework: ESNI(Exploration, Segmentation, Navigation, Instruction) that apply to mobile agent, achieve some functionalities such as processing human commands, picking up a specified object, and moving an object to another location. The agent starts the exploration in an unseen environment, stores each item's information in the grid cells to his memory and analyzes the corresponding features for each section. We divided our indoor environment into 6 sections: Kitchen, Living room, Bedroom, Studio, Bathroom, Balcony. The agent uses algorithms to assign sections for each grid cell then generates a navigation trajectory base on the section segmentation. When the user gives a command to the agent, feature words will be extracted and processed into a sequence of sub-tasks.",
		"note": "arXiv:2203.16014 [cs]",
		"number": "arXiv:2203.16014",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ESNI: Domestic Robots Design for Elderly and Disabled People",
		"title-short": "ESNI",
		"URL": "http://arxiv.org/abs/2203.16014",
		"author": [
			{
				"family": "Chu",
				"given": "Junchi"
			},
			{
				"family": "Tang",
				"given": "Xueyun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3,
					29
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/C65H45GW",
		"type": "paper-conference",
		"publisher": "Proceedings of the 2023 IEEE/RSJ International Conference on Intelligent …",
		"source": "Google Scholar",
		"title": "Skill Generalization with Verbs",
		"URL": "https://cs.brown.edu/~gdk/pubs/skillgen_verbs.pdf",
		"author": [
			{
				"family": "Ma",
				"given": "Rachel"
			},
			{
				"family": "Lam",
				"given": "Lyndon"
			},
			{
				"family": "Spiegel",
				"given": "Benjamin A."
			},
			{
				"family": "Ganeshan",
				"given": "Aditya"
			},
			{
				"family": "Patel",
				"given": "Roma"
			},
			{
				"family": "Abbatematteo",
				"given": "Ben"
			},
			{
				"family": "Paulius",
				"given": "David"
			},
			{
				"family": "Tellex",
				"given": "Stefanie"
			},
			{
				"family": "Konidaris",
				"given": "George"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ETGBTHGX",
		"type": "article",
		"abstract": "Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause. We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctness metric of the executed task plans by 76.49% compared to SayCan. Our approach enables the robot to follow natural language commands and robustly recover from failures, which baseline approaches largely cannot resolve or address inefficiently.",
		"note": "arXiv:2211.09935 [cs]",
		"number": "arXiv:2211.09935",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CAPE: Corrective Actions from Precondition Errors using Large Language Models",
		"title-short": "CAPE",
		"URL": "http://arxiv.org/abs/2211.09935",
		"author": [
			{
				"family": "Raman",
				"given": "Shreyas Sundara"
			},
			{
				"family": "Cohen",
				"given": "Vanya"
			},
			{
				"family": "Paulius",
				"given": "David"
			},
			{
				"family": "Idrees",
				"given": "Ifrah"
			},
			{
				"family": "Rosen",
				"given": "Eric"
			},
			{
				"family": "Mooney",
				"given": "Ray"
			},
			{
				"family": "Tellex",
				"given": "Stefanie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					22
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SXH7XFQ9",
		"type": "paper-conference",
		"container-title": "2022 International Conference on Robotics and Automation (ICRA)",
		"page": "9390–9396",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Depth-Aware Vision-and-Language Navigation using Scene Query Attention Network",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9811921/",
		"author": [
			{
				"family": "Tan",
				"given": "Sinan"
			},
			{
				"family": "Ge",
				"given": "Mengmeng"
			},
			{
				"family": "Guo",
				"given": "Di"
			},
			{
				"family": "Liu",
				"given": "Huaping"
			},
			{
				"family": "Sun",
				"given": "Fuchun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DXNEMZ2M",
		"type": "article",
		"abstract": "In the pursuit of fully autonomous robotic systems capable of taking over tasks traditionally performed by humans, the complexity of open-world environments poses a considerable challenge. Addressing this imperative, this study contributes to the field of Large Language Models (LLMs) applied to task and motion planning for robots. We propose a system architecture that orchestrates a seamless interplay between multiple cognitive levels, encompassing reasoning, planning, and motion generation. At its core lies a novel replanning strategy that handles physically grounded, logical, and semantic errors in the generated plans. We demonstrate the efficacy of the proposed feedback architecture, particularly its impact on executability, correctness, and time complexity via empirical evaluation in the context of a simulation and two intricate real-world scenarios: blocks world, barman and pizza preparation.",
		"note": "arXiv:2310.07263 [cs]",
		"number": "arXiv:2310.07263",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CoPAL: Corrective Planning of Robot Actions with Large Language Models",
		"title-short": "CoPAL",
		"URL": "http://arxiv.org/abs/2310.07263",
		"author": [
			{
				"family": "Joublin",
				"given": "Frank"
			},
			{
				"family": "Ceravola",
				"given": "Antonello"
			},
			{
				"family": "Smirnov",
				"given": "Pavel"
			},
			{
				"family": "Ocker",
				"given": "Felix"
			},
			{
				"family": "Deigmoeller",
				"given": "Joerg"
			},
			{
				"family": "Belardinelli",
				"given": "Anna"
			},
			{
				"family": "Wang",
				"given": "Chao"
			},
			{
				"family": "Hasler",
				"given": "Stephan"
			},
			{
				"family": "Tanneberg",
				"given": "Daniel"
			},
			{
				"family": "Gienger",
				"given": "Michael"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LA6ENMTK",
		"type": "paper-conference",
		"container-title": "2023 IEEE International Conference on Robotics and Automation (ICRA)",
		"page": "9508–9514",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Learning visual-audio representations for voice-controlled robots",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10161461/",
		"author": [
			{
				"family": "Chang",
				"given": "Peixin"
			},
			{
				"family": "Liu",
				"given": "Shuijing"
			},
			{
				"family": "McPherson",
				"given": "D. Livingston"
			},
			{
				"family": "Driggs-Campbell",
				"given": "Katherine"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9AG7SS4E",
		"type": "paper-conference",
		"container-title": "Hawaii International Conference on System Sciences (HICSS)",
		"source": "Google Scholar",
		"title": "Mechanisms of Common Ground in Human-Agent Interaction: A Systematic Review of Conversational Agent Research",
		"title-short": "Mechanisms of Common Ground in Human-Agent Interaction",
		"URL": "https://www.alexandria.unisg.ch/bitstreams/46af363f-4f67-46dd-8f17-6ca4469f654d/download",
		"author": [
			{
				"family": "Tolzin",
				"given": "Antonia"
			},
			{
				"family": "Janson",
				"given": "Andreas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/63LTYZYS",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Institut polytechnique de Paris",
		"source": "Google Scholar",
		"title": "Exploration of reinforcement learning algorithms for autonomous vehicle visual perception and control",
		"URL": "https://www.theses.fr/2021IPPAE007",
		"author": [
			{
				"family": "Carton",
				"given": "Florence"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/33J8YRTJ",
		"type": "article",
		"abstract": "Large Language Models (LLMs) have the capacity of performing complex scheduling in a multi-agent system and can coordinate these agents into completing sophisticated tasks that require extensive collaboration. However, despite the introduction of numerous gaming frameworks, the community has insufficient benchmarks towards building general multi-agents collaboration infrastructure that encompass both LLM and human-NPCs collaborations. In this work, we propose a novel infrastructure - MindAgent - to evaluate planning and coordination emergent capabilities for gaming interaction. In particular, our infrastructure leverages existing gaming framework, to i) require understanding of the coordinator for a multi-agent system, ii) collaborate with human players via un-finetuned proper instructions, and iii) establish an in-context learning on few-shot prompt with feedback. Furthermore, we introduce CUISINEWORLD, a new gaming scenario and related benchmark that dispatch a multi-agent collaboration efficiency and supervise multiple agents playing the game simultaneously. We conduct comprehensive evaluations with new auto-metric CoS for calculating the collaboration efficiency. Finally, our infrastructure can be deployed into real-world gaming scenarios in a customized VR version of CUISINEWORLD and adapted in existing broader Minecraft gaming domain. We hope our findings on LLMs and the new infrastructure for general-purpose scheduling and coordination can help shed light on how such skills can be obtained by learning from large language corpora.",
		"note": "arXiv:2309.09971 [cs]",
		"number": "arXiv:2309.09971",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "MindAgent: Emergent Gaming Interaction",
		"title-short": "MindAgent",
		"URL": "http://arxiv.org/abs/2309.09971",
		"author": [
			{
				"family": "Gong",
				"given": "Ran"
			},
			{
				"family": "Huang",
				"given": "Qiuyuan"
			},
			{
				"family": "Ma",
				"given": "Xiaojian"
			},
			{
				"family": "Vo",
				"given": "Hoi"
			},
			{
				"family": "Durante",
				"given": "Zane"
			},
			{
				"family": "Noda",
				"given": "Yusuke"
			},
			{
				"family": "Zheng",
				"given": "Zilong"
			},
			{
				"family": "Zhu",
				"given": "Song-Chun"
			},
			{
				"family": "Terzopoulos",
				"given": "Demetri"
			},
			{
				"family": "Fei-Fei",
				"given": "Li"
			},
			{
				"family": "Gao",
				"given": "Jianfeng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/299WN3LN",
		"type": "article",
		"abstract": "In order for robots to follow open-ended instructions like \"go open the brown cabinet over the sink\", they require an understanding of both the scene geometry and the semantics of their environment. Robotic systems often handle these through separate pipelines, sometimes using very different representation spaces, which can be suboptimal when the two objectives conflict. In this work, we present USA-Net, a simple method for constructing a world representation that encodes both the semantics and spatial affordances of a scene in a differentiable map. This allows us to build a gradient-based planner which can navigate to locations in the scene specified using open-ended vocabulary. We use this planner to consistently generate trajectories which are both shorter 5-10% shorter and 10-30% closer to our goal query in CLIP embedding space than paths from comparable grid-based planners which don't leverage gradient information. To our knowledge, this is the first end-to-end differentiable planner optimizes for both semantics and affordance in a single implicit map. Code and visuals are available at our website: https://usa.bolte.cc/",
		"note": "arXiv:2304.12164 [cs]",
		"number": "arXiv:2304.12164",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "USA-Net: Unified Semantic and Affordance Representations for Robot Memory",
		"title-short": "USA-Net",
		"URL": "http://arxiv.org/abs/2304.12164",
		"author": [
			{
				"family": "Bolte",
				"given": "Benjamin"
			},
			{
				"family": "Wang",
				"given": "Austin"
			},
			{
				"family": "Yang",
				"given": "Jimmy"
			},
			{
				"family": "Mukadam",
				"given": "Mustafa"
			},
			{
				"family": "Kalakrishnan",
				"given": "Mrinal"
			},
			{
				"family": "Paxton",
				"given": "Chris"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					24
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4DSK4KEV",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "25102–25116",
		"source": "Google Scholar",
		"title": "Gaudi: A neural architect for immersive 3d scene generation",
		"title-short": "Gaudi",
		"URL": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/a03037317560b8c5f2fb4b6466d4c439-Abstract-Conference.html",
		"volume": "35",
		"author": [
			{
				"family": "Bautista",
				"given": "Miguel Angel"
			},
			{
				"family": "Guo",
				"given": "Pengsheng"
			},
			{
				"family": "Abnar",
				"given": "Samira"
			},
			{
				"family": "Talbott",
				"given": "Walter"
			},
			{
				"family": "Toshev",
				"given": "Alexander"
			},
			{
				"family": "Chen",
				"given": "Zhuoyuan"
			},
			{
				"family": "Dinh",
				"given": "Laurent"
			},
			{
				"family": "Zhai",
				"given": "Shuangfei"
			},
			{
				"family": "Goh",
				"given": "Hanlin"
			},
			{
				"family": "Ulbricht",
				"given": "Daniel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/R8FU44RS",
		"type": "article",
		"abstract": "The human ability to learn, generalize, and control complex manipulation tasks through multi-modality feedback suggests a unique capability, which we refer to as dexterity intelligence. Understanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive proliferation of large language models (LLMs), their applications in the field of robotics have garnered increasing attention. LLMs possess the ability to process and generate natural language, facilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of robotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot interaction, and autonomy. Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning. We first provide an overview of the background and development of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and recent advancements in robotics models based on LLMs. We then delve into the various techniques used in the model, including those employed in perception, decision-making, control, and interaction. Finally, we explore the applications of LLMs in robotics and some potential challenges they may face in the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics is one of the promising but challenging paths to achieve this.",
		"note": "arXiv:2311.07226 [cs]",
		"number": "arXiv:2311.07226",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Large Language Models for Robotics: A Survey",
		"title-short": "Large Language Models for Robotics",
		"URL": "http://arxiv.org/abs/2311.07226",
		"author": [
			{
				"family": "Zeng",
				"given": "Fanlong"
			},
			{
				"family": "Gan",
				"given": "Wensheng"
			},
			{
				"family": "Wang",
				"given": "Yongheng"
			},
			{
				"family": "Liu",
				"given": "Ning"
			},
			{
				"family": "Yu",
				"given": "Philip S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					13
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NCC7LAGJ",
		"type": "thesis",
		"genre": "PhD Thesis",
		"source": "Google Scholar",
		"title": "Towards Optimistic, Imaginative, and Harmonious Reinforcement Learning in Single-Agent and Multi-Agent Environments",
		"URL": "https://digital.library.adelaide.edu.au/dspace/handle/2440/137158",
		"author": [
			{
				"family": "Kazemi Moghaddam",
				"given": "Mohammad Mahdi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/M48GQ3TR",
		"type": "paper-conference",
		"container-title": "2017 IEEE Intelligent Vehicles Symposium (IV)",
		"page": "341–346",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "Baidu driving dataset and end-to-end reactive control model",
		"URL": "https://ieeexplore.ieee.org/abstract/document/7995742/",
		"author": [
			{
				"family": "Yu",
				"given": "Hao"
			},
			{
				"family": "Yang",
				"given": "Shu"
			},
			{
				"family": "Gu",
				"given": "Weihao"
			},
			{
				"family": "Zhang",
				"given": "Shaoyu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3HYNZUMT",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "6309–6318",
		"source": "Google Scholar",
		"title": "Multi-target embodied question answering",
		"URL": "http://openaccess.thecvf.com/content_CVPR_2019/html/Yu_Multi-Target_Embodied_Question_Answering_CVPR_2019_paper.html",
		"author": [
			{
				"family": "Yu",
				"given": "Licheng"
			},
			{
				"family": "Chen",
				"given": "Xinlei"
			},
			{
				"family": "Gkioxari",
				"given": "Georgia"
			},
			{
				"family": "Bansal",
				"given": "Mohit"
			},
			{
				"family": "Berg",
				"given": "Tamara L."
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XWYHMDIQ",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Visualization and Computer Graphics",
		"note": "publisher: IEEE",
		"source": "Google Scholar",
		"title": "A review of interaction techniques for immersive environments",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9773967/",
		"author": [
			{
				"family": "Spittle",
				"given": "Becky"
			},
			{
				"family": "Frutos-Pascual",
				"given": "Maite"
			},
			{
				"family": "Creed",
				"given": "Chris"
			},
			{
				"family": "Williams",
				"given": "Ian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DGZC77ZF",
		"type": "article",
		"abstract": "Embodied AI has seen steady progress across a diverse set of independent tasks. While these varied tasks have different end goals, the basic skills required to complete them successfully overlap significantly. In this paper, our goal is to leverage these shared skills to learn to perform multiple tasks jointly. We propose Atomic Skill Completion (ASC), an approach for multi-task training for Embodied AI, where a set of atomic skills shared across multiple tasks are composed together to perform the tasks. The key to the success of this approach is a pre-training scheme that decouples learning of the skills from the high-level tasks making joint training effective. We use ASC to train agents within the AI2-THOR environment to perform four interactive tasks jointly and find it to be remarkably effective. In a multi-task setting, ASC improves success rates by a factor of 2x on Seen scenes and 4x on Unseen scenes compared to no pre-training. Importantly, ASC enables us to train a multi-task agent that has a 52% higher Success Rate than training 4 independent single task agents. Finally, our hierarchical agents are more interpretable than traditional black-box architectures.",
		"note": "arXiv:2202.06987 [cs]",
		"number": "arXiv:2202.06987",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ASC me to Do Anything: Multi-task Training for Embodied AI",
		"title-short": "ASC me to Do Anything",
		"URL": "http://arxiv.org/abs/2202.06987",
		"author": [
			{
				"family": "Lu",
				"given": "Jiasen"
			},
			{
				"family": "Salvador",
				"given": "Jordi"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Kembhavi",
				"given": "Aniruddha"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					2,
					14
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QYRUCG82",
		"type": "paper-conference",
		"container-title": "LREC proceedings",
		"source": "Google Scholar",
		"title": "The VoxWorld platform for multimodal embodied agents",
		"URL": "https://par.nsf.gov/biblio/10379209",
		"volume": "13",
		"author": [
			{
				"family": "Krshnaswamy",
				"given": "Nikhil"
			},
			{
				"family": "Pickard",
				"given": "William"
			},
			{
				"family": "Cates",
				"given": "Brittany"
			},
			{
				"family": "Blanchard",
				"given": "Nathaniel"
			},
			{
				"family": "Pustejovsky",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EIHP5B4Z",
		"type": "article-journal",
		"container-title": "Distill",
		"issue": "7",
		"page": "e11",
		"source": "Google Scholar",
		"title": "Feature-wise transformations",
		"URL": "https://staging.distill.pub/2018/feature-wise-transformations/?utm_campaign=The+Batch&%3Butm_source=hs_email&%3Butm_medium=email&%3Butm_content=2&%3B_hsenc=p2ANqtz-_y7LKn2OW8eVKFWN6aYCjxUI-sOF4aNoqsVlfHqHvZqO66RnPZbAPo4wwMyW2fo5iNqSLEHOGgkqNU2QwzSqK0HJUNdw&ref=dl-staging-website.ghost.io",
		"volume": "3",
		"author": [
			{
				"family": "Dumoulin",
				"given": "Vincent"
			},
			{
				"family": "Perez",
				"given": "Ethan"
			},
			{
				"family": "Schucher",
				"given": "Nathan"
			},
			{
				"family": "Strub",
				"given": "Florian"
			},
			{
				"family": "Vries",
				"given": "Harm",
				"dropping-particle": "de"
			},
			{
				"family": "Courville",
				"given": "Aaron"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CHXMBY7K",
		"type": "article",
		"abstract": "Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is projected. As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the foundation of the image formation process. In this work, we formulate the newt ask of language-guided 3D-aware editing, where objects in an image should be edited according to a language instruction in context of the underlying 3D scene. To promote progress towards this goal, we release OBJECT: a dataset consisting of 400K editing examples created from procedurally generated 3D scenes. Each example consists of an input image, editing instruction in language, and the edited image. We also introduce 3DIT : single and multi-task models for four editing tasks. Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects, surfaces, lighting conditions, shadows, and physically-plausible object configurations. Surprisingly, training on only synthetic scenes from OBJECT, editing capabilities of 3DIT generalize to real-world images.",
		"note": "arXiv:2307.11073 [cs]",
		"number": "arXiv:2307.11073",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "OBJECT 3DIT: Language-guided 3D-aware Image Editing",
		"title-short": "OBJECT 3DIT",
		"URL": "http://arxiv.org/abs/2307.11073",
		"author": [
			{
				"family": "Michel",
				"given": "Oscar"
			},
			{
				"family": "Bhattad",
				"given": "Anand"
			},
			{
				"family": "VanderBilt",
				"given": "Eli"
			},
			{
				"family": "Krishna",
				"given": "Ranjay"
			},
			{
				"family": "Kembhavi",
				"given": "Aniruddha"
			},
			{
				"family": "Gupta",
				"given": "Tanmay"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6BE49UV9",
		"type": "book",
		"publisher": "McGill University (Canada)",
		"source": "Google Scholar",
		"title": "Language-conditional imitation learning",
		"URL": "https://search.proquest.com/openview/5946311700f98c378697dd64b3a1ece4/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Skirzynski",
				"given": "Julian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4NS9C5SQ",
		"type": "article",
		"abstract": "While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is desirable to preserve the generality of LMs during finetuning, which facilitates generalizing the embodied knowledge across tasks rather than being tied to specific simulations. We thus further introduce the classical (EWC) for selective weight updates, combined with low-rank adapters (LoRA) for training efficiency. Extensive experiments show our approach substantially improves base LMs on 18 downstream tasks by 64.28% on average. In particular, the small LMs (1.3B, 6B, and 13B) enhanced by our approach match or even outperform much larger LMs (e.g., ChatGPT).",
		"note": "arXiv:2305.10626 [cs]",
		"number": "arXiv:2305.10626",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Language Models Meet World Models: Embodied Experiences Enhance Language Models",
		"title-short": "Language Models Meet World Models",
		"URL": "http://arxiv.org/abs/2305.10626",
		"author": [
			{
				"family": "Xiang",
				"given": "Jiannan"
			},
			{
				"family": "Tao",
				"given": "Tianhua"
			},
			{
				"family": "Gu",
				"given": "Yi"
			},
			{
				"family": "Shu",
				"given": "Tianmin"
			},
			{
				"family": "Wang",
				"given": "Zirui"
			},
			{
				"family": "Yang",
				"given": "Zichao"
			},
			{
				"family": "Hu",
				"given": "Zhiting"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					28
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6MSMA2IZ",
		"type": "article-journal",
		"container-title": "Psychological review",
		"issue": "2",
		"note": "publisher: American Psychological Association",
		"page": "401",
		"source": "Google Scholar",
		"title": "Word meaning in minds and machines.",
		"URL": "https://psycnet.apa.org/record/2021-63913-001",
		"volume": "130",
		"author": [
			{
				"family": "Lake",
				"given": "Brenden M."
			},
			{
				"family": "Murphy",
				"given": "Gregory L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/Y9MHGBUG",
		"type": "article-journal",
		"container-title": "Autonomous Robots",
		"DOI": "10.1007/s10514-022-10043-y",
		"ISSN": "0929-5593, 1573-7527",
		"issue": "6",
		"journalAbbreviation": "Auton Robot",
		"language": "en",
		"page": "667-683",
		"source": "DOI.org (Crossref)",
		"title": "Hierarchical planning with state abstractions for temporal task specifications",
		"URL": "https://link.springer.com/10.1007/s10514-022-10043-y",
		"volume": "46",
		"author": [
			{
				"family": "Oh",
				"given": "Yoonseon"
			},
			{
				"family": "Patel",
				"given": "Roma"
			},
			{
				"family": "Nguyen",
				"given": "Thao"
			},
			{
				"family": "Huang",
				"given": "Baichuan"
			},
			{
				"family": "Berg",
				"given": "Matthew"
			},
			{
				"family": "Pavlick",
				"given": "Ellie"
			},
			{
				"family": "Tellex",
				"given": "Stefanie"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FGDE3SLZ",
		"type": "article",
		"abstract": "We introduce OPEND, a benchmark for learning how to use a hand to open cabinet doors or drawers in a photo-realistic and physics-reliable simulation environment driven by language instruction. To solve the task, we propose a multi-step planner composed of a deep neural network and rule-base controllers. The network is utilized to capture spatial relationships from images and understand semantic meaning from language instructions. Controllers efficiently execute the plan based on the spatial and semantic understanding. We evaluate our system by measuring its zero-shot performance in test data set. Experimental results demonstrate the effectiveness of decision planning by our multi-step planner for different hands, while suggesting that there is significant room for developing better models to address the challenge brought by language understanding, spatial reasoning, and long-term manipulation. We will release OPEND and host challenges to promote future research in this area.",
		"note": "arXiv:2212.05211 [cs]",
		"number": "arXiv:2212.05211",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "OpenD: A Benchmark for Language-Driven Door and Drawer Opening",
		"title-short": "OpenD",
		"URL": "http://arxiv.org/abs/2212.05211",
		"author": [
			{
				"family": "Zhao",
				"given": "Yizhou"
			},
			{
				"family": "Gao",
				"given": "Qiaozi"
			},
			{
				"family": "Qiu",
				"given": "Liang"
			},
			{
				"family": "Thattai",
				"given": "Govind"
			},
			{
				"family": "Sukhatme",
				"given": "Gaurav S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JWA924KL",
		"type": "article",
		"abstract": "Episodic training, where an agent's environment is reset after every success or failure, is the de facto standard when training embodied reinforcement learning (RL) agents. The underlying assumption that the environment can be easily reset is limiting both practically, as resets generally require human effort in the real world and can be computationally expensive in simulation, and philosophically, as we'd expect intelligent agents to be able to continuously learn without intervention. Work in learning without any resets, i.e{.} Reset-Free RL (RF-RL), is promising but is plagued by the problem of irreversible transitions (e.g{.} an object breaking) which halt learning. Moreover, the limited state diversity and instrument setup encountered during RF-RL means that works studying RF-RL largely do not require their models to generalize to new environments. In this work, we instead look to minimize, rather than completely eliminate, resets while building visual agents that can meaningfully generalize. As studying generalization has previously not been a focus of benchmarks designed for RF-RL, we propose a new Stretch Pick-and-Place benchmark designed for evaluating generalizations across goals, cosmetic variations, and structural changes. Moreover, towards building performant reset-minimizing RL agents, we propose unsupervised metrics to detect irreversible transitions and a single-policy training mechanism to enable generalization. Our proposed approach significantly outperforms prior episodic, reset-free, and reset-minimizing approaches achieving higher success rates with fewer resets in Stretch-P\\&P and another popular RF-RL benchmark. Finally, we find that our proposed approach can dramatically reduce the number of resets required for training other embodied tasks, in particular for RoboTHOR ObjectNav we obtain higher success rates than episodic approaches using 99.97\\% fewer resets.",
		"note": "arXiv:2303.17600 [cs]",
		"number": "arXiv:2303.17600",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "When Learning Is Out of Reach, Reset: Generalization in Autonomous Visuomotor Reinforcement Learning",
		"title-short": "When Learning Is Out of Reach, Reset",
		"URL": "http://arxiv.org/abs/2303.17600",
		"author": [
			{
				"family": "Zhang",
				"given": "Zichen"
			},
			{
				"family": "Weihs",
				"given": "Luca"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					30
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/T9VWAA8L",
		"type": "article-journal",
		"abstract": "Semantic navigation is necessary to deploy mobile robots in uncontrolled environments such as homes or hospitals. Many learning-based approaches have been proposed in response to the lack of semantic understanding of the classical pipeline for spatial navigation, which builds a geometric map using depth sensors and plans to reach point goals. Broadly, end-to-end learning approaches reactively map sensor inputs to actions with deep neural networks, whereas modular learning approaches enrich the classical pipeline with learning-based semantic sensing and exploration. However, learned visual navigation policies have predominantly been evaluated in sim, with little known about what works on a robot. We present a large-scale empirical study of semantic visual navigation methods comparing representative methods with classical, modular, and end-to-end learning approaches across six homes with no prior experience, maps, or instrumentation. We found that modular learning works well in the real world, attaining a 90% success rate. In contrast, end-to-end learning does not, dropping from 77% sim to a 23% real-world success rate because of a large image domain gap between sim and reality. For practitioners, we show that modular learning is a reliable approach to navigate to objects: Modularity and abstraction in policy design enable sim-to-real transfer. For researchers, we identify two key issues that prevent today’s simulators from being reliable evaluation benchmarks—a large sim-to-real gap in images and a disconnect between sim and real-world error modes—and propose concrete steps forward.\n          , \n            A real-world empirical study of robot navigation methods compared classical, end-to-end, and modular learning approaches.",
		"container-title": "Science Robotics",
		"DOI": "10.1126/scirobotics.adf6991",
		"ISSN": "2470-9476",
		"issue": "79",
		"journalAbbreviation": "Sci. Robot.",
		"language": "en",
		"page": "eadf6991",
		"source": "DOI.org (Crossref)",
		"title": "Navigating to objects in the real world",
		"URL": "https://www.science.org/doi/10.1126/scirobotics.adf6991",
		"volume": "8",
		"author": [
			{
				"family": "Gervet",
				"given": "Theophile"
			},
			{
				"family": "Chintala",
				"given": "Soumith"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			},
			{
				"family": "Malik",
				"given": "Jitendra"
			},
			{
				"family": "Chaplot",
				"given": "Devendra Singh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					28
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/Q6I9UCY6",
		"type": "article",
		"abstract": "Robots rely heavily on sensors, especially RGB and depth cameras, to perceive and interact with the world. RGB cameras record 2D images with rich semantic information while missing precise spatial information. On the other side, depth cameras offer critical 3D geometry data but capture limited semantics. Therefore, integrating both modalities is crucial for learning representations for robotic perception and control. However, current research predominantly focuses on only one of these modalities, neglecting the benefits of incorporating both. To this end, we present $\\textbf{Semantic-Geometric Representation} (\\textbf{SGR})$, a universal perception module for robotics that leverages the rich semantic information of large-scale pre-trained 2D models and inherits the merits of 3D spatial reasoning. Our experiments demonstrate that SGR empowers the agent to successfully complete a diverse range of simulated and real-world robotic manipulation tasks, outperforming state-of-the-art methods significantly in both single-task and multi-task settings. Furthermore, SGR possesses the capability to generalize to novel semantic attributes, setting it apart from the other methods. Project website: https://semantic-geometric-representation.github.io.",
		"note": "arXiv:2306.10474 [cs]",
		"number": "arXiv:2306.10474",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Universal Semantic-Geometric Representation for Robotic Manipulation",
		"URL": "http://arxiv.org/abs/2306.10474",
		"author": [
			{
				"family": "Zhang",
				"given": "Tong"
			},
			{
				"family": "Hu",
				"given": "Yingdong"
			},
			{
				"family": "Cui",
				"given": "Hanchen"
			},
			{
				"family": "Zhao",
				"given": "Hang"
			},
			{
				"family": "Gao",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					13
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/D3EZ998X",
		"type": "article",
		"abstract": "We introduce a large scale crowdsourced text adventure game as a research platform for studying grounded dialogue. In it, agents can perceive, emote, and act whilst conducting dialogue with other agents. Models and humans can both act as characters within the game. We describe the results of training state-of-the-art generative and retrieval models in this setting. We show that in addition to using past dialogue, these models are able to effectively use the state of the underlying world to condition their predictions. In particular, we show that grounding on the details of the local environment, including location descriptions, and the objects (and their affordances) and characters (and their previous actions) present within it allows better predictions of agent behavior and dialogue. We analyze the ingredients necessary for successful grounding in this setting, and how each of these factors relate to agents that can talk and act successfully.",
		"note": "arXiv:1903.03094 [cs]",
		"number": "arXiv:1903.03094",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning to Speak and Act in a Fantasy Text Adventure Game",
		"URL": "http://arxiv.org/abs/1903.03094",
		"author": [
			{
				"family": "Urbanek",
				"given": "Jack"
			},
			{
				"family": "Fan",
				"given": "Angela"
			},
			{
				"family": "Karamcheti",
				"given": "Siddharth"
			},
			{
				"family": "Jain",
				"given": "Saachi"
			},
			{
				"family": "Humeau",
				"given": "Samuel"
			},
			{
				"family": "Dinan",
				"given": "Emily"
			},
			{
				"family": "Rocktäschel",
				"given": "Tim"
			},
			{
				"family": "Kiela",
				"given": "Douwe"
			},
			{
				"family": "Szlam",
				"given": "Arthur"
			},
			{
				"family": "Weston",
				"given": "Jason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					3,
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/UZZ9NH7Y",
		"type": "paper-conference",
		"container-title": "2020 29th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)",
		"page": "1395–1402",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "A general methodology for teaching norms to social robots",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9223610/",
		"author": [
			{
				"family": "Malle",
				"given": "Bertram F."
			},
			{
				"family": "Rosen",
				"given": "Eric"
			},
			{
				"family": "Chi",
				"given": "Vivienne B."
			},
			{
				"family": "Berg",
				"given": "Matthew"
			},
			{
				"family": "Haas",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HID9HYZ2",
		"type": "article-journal",
		"container-title": "IEEE access",
		"note": "publisher: IEEE",
		"page": "4209–4251",
		"source": "Google Scholar",
		"title": "A metaverse: Taxonomy, components, applications, and open challenges",
		"title-short": "A metaverse",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9667507/",
		"volume": "10",
		"author": [
			{
				"family": "Park",
				"given": "Sang-Min"
			},
			{
				"family": "Kim",
				"given": "Young-Gab"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/I99AKMWQ",
		"type": "thesis",
		"genre": "PhD Thesis",
		"source": "Google Scholar",
		"title": "From Vision-Language Multimodal Learning Towards Embodied Agents",
		"URL": "https://ses.library.usyd.edu.au/handle/2123/31617",
		"author": [
			{
				"family": "Zheng",
				"given": "Qi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9I9WZLRP",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Lucida: Enhancing the creation of photography with an Augmented Intelligence Digital Photography Agent",
		"title-short": "Lucida",
		"URL": "https://prism.ucalgary.ca/bitstreams/97879a14-6cc2-4ef3-b08e-74bef4445579/download",
		"author": [
			{
				"family": "Wrobleski",
				"given": "Brad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LPNTVGDE",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Learning to Speak and Act in a Fantasy Text Adventure Game",
		"URL": "https://openreview.net/pdf?id=ShxS070y_V",
		"author": [
			{
				"family": "Fan",
				"given": "Jack Urbanek1 Angela"
			},
			{
				"family": "Humeau",
				"given": "Siddharth Karamcheti1 Saachi Jain1 Samuel"
			},
			{
				"family": "Rocktäschel",
				"given": "Emily Dinan1 Tim"
			},
			{
				"family": "Weston",
				"given": "Douwe Kiela1 Arthur Szlam1 Jason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GIE7DR6Y",
		"type": "article",
		"abstract": "Object goal navigation aims to steer an agent towards a target object based on observations of the agent. It is of pivotal importance to design effective visual representations of the observed scene in determining navigation actions. In this paper, we introduce a Visual Transformer Network (VTNet) for learning informative visual representation in navigation. VTNet is a highly effective structure that embodies two key properties for visual representations: First, the relationships among all the object instances in a scene are exploited; Second, the spatial locations of objects and image regions are emphasized so that directional navigation signals can be learned. Furthermore, we also develop a pre-training scheme to associate the visual representations with navigation signals, and thus facilitate navigation policy learning. In a nutshell, VTNet embeds object and region features with their location cues as spatial-aware descriptors and then incorporates all the encoded descriptors through attention operations to achieve informative representation for navigation. Given such visual representations, agents are able to explore the correlations between visual observations and navigation actions. For example, an agent would prioritize \"turning right\" over \"turning left\" when the visual representation emphasizes on the right side of activation map. Experiments in the artificial environment AI2-Thor demonstrate that VTNet significantly outperforms state-of-the-art methods in unseen testing environments.",
		"note": "arXiv:2105.09447 [cs]",
		"number": "arXiv:2105.09447",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "VTNet: Visual Transformer Network for Object Goal Navigation",
		"title-short": "VTNet",
		"URL": "http://arxiv.org/abs/2105.09447",
		"author": [
			{
				"family": "Du",
				"given": "Heming"
			},
			{
				"family": "Yu",
				"given": "Xin"
			},
			{
				"family": "Zheng",
				"given": "Liang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					5,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CDHBJ2E3",
		"type": "article-journal",
		"container-title": "Robotics and Autonomous Systems",
		"note": "publisher: Elsevier",
		"page": "104294",
		"source": "Google Scholar",
		"title": "A survey of Semantic Reasoning frameworks for robotic systems",
		"URL": "https://www.sciencedirect.com/science/article/pii/S092188902200183X",
		"volume": "159",
		"author": [
			{
				"family": "Liu",
				"given": "Weiyu"
			},
			{
				"family": "Daruna",
				"given": "Angel"
			},
			{
				"family": "Patel",
				"given": "Maithili"
			},
			{
				"family": "Ramachandruni",
				"given": "Kartik"
			},
			{
				"family": "Chernova",
				"given": "Sonia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IPT2JPVL",
		"type": "chapter",
		"container-title": "Intelligent Human Computer Interaction",
		"event-place": "Cham",
		"ISBN": "978-3-030-68451-8",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-030-68452-5_42",
		"page": "406-422",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "AARON: Assistive Augmented Reality Operations and Navigation System for NASA’s Exploration Extravehicular Mobility Unit (xEMU)",
		"title-short": "AARON",
		"URL": "https://link.springer.com/10.1007/978-3-030-68452-5_42",
		"volume": "12616",
		"editor": [
			{
				"family": "Singh",
				"given": "Madhusudan"
			},
			{
				"family": "Kang",
				"given": "Dae-Ki"
			},
			{
				"family": "Lee",
				"given": "Jong-Ha"
			},
			{
				"family": "Tiwary",
				"given": "Uma Shanker"
			},
			{
				"family": "Singh",
				"given": "Dhananjay"
			},
			{
				"family": "Chung",
				"given": "Wan-Young"
			}
		],
		"author": [
			{
				"family": "Cardenas",
				"given": "Irvin Steve"
			},
			{
				"family": "Lenhoff",
				"given": "Caitlyn"
			},
			{
				"family": "Park",
				"given": "Michelle"
			},
			{
				"family": "Xu",
				"given": "Tina Yuqiao"
			},
			{
				"family": "Lin",
				"given": "Xiangxu"
			},
			{
				"family": "Paladugula",
				"given": "Pradeep Kumar"
			},
			{
				"family": "Kim",
				"given": "Jong-Hoon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A3VIBPHJ",
		"type": "paper-conference",
		"container-title": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops",
		"page": "0–0",
		"source": "Google Scholar",
		"title": "Answering visual what-if questions: From actions to predicted scene descriptions",
		"title-short": "Answering visual what-if questions",
		"URL": "http://openaccess.thecvf.com/content_eccv_2018_workshops/w4/html/Wagner_Answering_Visual_em_What-If_Questions_From_Actions_to_Predicted_Scene_ECCVW_2018_paper.html",
		"author": [
			{
				"family": "Wagner",
				"given": "Misha"
			},
			{
				"family": "Basevi",
				"given": "Hector"
			},
			{
				"family": "Shetty",
				"given": "Rakshith"
			},
			{
				"family": "Li",
				"given": "Wenbin"
			},
			{
				"family": "Malinowski",
				"given": "Mateusz"
			},
			{
				"family": "Fritz",
				"given": "Mario"
			},
			{
				"family": "Leonardis",
				"given": "Ales"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HN7XI7T2",
		"type": "book",
		"publisher": "Stanford University",
		"source": "Google Scholar",
		"title": "Learning to generate and differentiate 3D objects using geometry & language",
		"URL": "https://search.proquest.com/openview/bb21a829c001751ae20585c735669c05/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Achlioptas",
				"given": "Panagiotis Panos"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/G3DVQUBH",
		"type": "paper-conference",
		"container-title": "Proceedings of the Symposium on Spatial User Interaction",
		"DOI": "10.1145/3267782.3267921",
		"event-place": "Berlin Germany",
		"event-title": "SUI '18: Symposium on Spatial User Interaction",
		"ISBN": "978-1-4503-5708-1",
		"language": "en",
		"page": "141-149",
		"publisher": "ACM",
		"publisher-place": "Berlin Germany",
		"source": "DOI.org (Crossref)",
		"title": "RobotIST: Interactive Situated Tangible Robot Programming",
		"title-short": "RobotIST",
		"URL": "https://dl.acm.org/doi/10.1145/3267782.3267921",
		"author": [
			{
				"family": "Sefidgar",
				"given": "Yasaman S."
			},
			{
				"family": "Weng",
				"given": "Thomas"
			},
			{
				"family": "Harvey",
				"given": "Heather"
			},
			{
				"family": "Elliott",
				"given": "Sarah"
			},
			{
				"family": "Cakmak",
				"given": "Maya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					10,
					13
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/D56M66UL",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Perception, Reasoning, Action: The New Frontier of Embodied AI",
		"title-short": "Perception, Reasoning, Action",
		"URL": "https://fdlandi.github.io/data/phd_thesis.pdf",
		"author": [
			{
				"family": "Landi",
				"given": "Federico"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XPLE2B8X",
		"type": "article",
		"abstract": "In recent years, developing AI for robotics has raised much attention. The interaction of vision and language of robots is particularly difficult. We consider that giving robots an understanding of visual semantics and language semantics will improve inference ability. In this paper, we propose a novel method-VSGM (Visual Semantic Graph Memory), which uses the semantic graph to obtain better visual image features, improve the robot's visual understanding ability. By providing prior knowledge of the robot and detecting the objects in the image, it predicts the correlation between the attributes of the object and the objects and converts them into a graph-based representation; and mapping the object in the image to be a top-down egocentric map. Finally, the important object features of the current task are extracted by Graph Neural Networks. The method proposed in this paper is verified in the ALFRED (Action Learning From Realistic Environments and Directives) dataset. In this dataset, the robot needs to perform daily indoor household tasks following the required language instructions. After the model is added to the VSGM, the task success rate can be improved by 6~10%.",
		"note": "arXiv:2105.08959 [cs]",
		"number": "arXiv:2105.08959",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "VSGM -- Enhance robot task understanding ability through visual semantic graph",
		"URL": "http://arxiv.org/abs/2105.08959",
		"author": [
			{
				"family": "Tsai",
				"given": "Cheng Yu"
			},
			{
				"family": "Su",
				"given": "Mu-Chun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					5,
					25
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5YQGQMD3",
		"type": "article",
		"abstract": "We present ScienceWorld, a benchmark to test agents' scientific reasoning abilities in a new interactive text environment at the level of a standard elementary school science curriculum. Despite the transformer-based progress seen in question-answering and scientific text processing, we find that current models cannot reason about or explain learned science concepts in novel contexts. For instance, models can easily answer what the conductivity of a known material is but struggle when asked how they would conduct an experiment in a grounded environment to find the conductivity of an unknown material. This begs the question of whether current models are simply retrieving answers by way of seeing a large number of similar examples or if they have learned to reason about concepts in a reusable manner. We hypothesize that agents need to be grounded in interactive environments to achieve such reasoning capabilities. Our experiments provide empirical evidence supporting this hypothesis -- showing that a 1.5 million parameter agent trained interactively for 100k steps outperforms a 11 billion parameter model statically trained for scientific question-answering and reasoning from millions of expert demonstrations.",
		"note": "arXiv:2203.07540 [cs]",
		"number": "arXiv:2203.07540",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ScienceWorld: Is your Agent Smarter than a 5th Grader?",
		"title-short": "ScienceWorld",
		"URL": "http://arxiv.org/abs/2203.07540",
		"author": [
			{
				"family": "Wang",
				"given": "Ruoyao"
			},
			{
				"family": "Jansen",
				"given": "Peter"
			},
			{
				"family": "Côté",
				"given": "Marc-Alexandre"
			},
			{
				"family": "Ammanabrolu",
				"given": "Prithviraj"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					14
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HQVIXIQE",
		"type": "article-journal",
		"container-title": "arXiv preprint arXiv:1902.03570",
		"source": "Google Scholar",
		"title": "Taranjeet Singh2 Akash Jain3 Shiv Baran Singh4 Stefan Lee1 Dhruv Batra1 1Georgia Institute of Technology 2Paytm 3 Zomato 4Cyware {deshraj, rishabhjain, harsh. agrawal, prithvijit3, steflee, dbatra}@ gatech. edu",
		"URL": "https://www.academia.edu/download/92165321/1902.03570v1.pdf",
		"author": [
			{
				"family": "Yadav",
				"given": "Deshraj"
			},
			{
				"family": "Jain",
				"given": "Rishabh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/PKD2L8BV",
		"type": "article-journal",
		"note": "publisher: Università degli studi di Modena e Reggio Emilia",
		"source": "Google Scholar",
		"title": "Percepire, Ragionare, Agire: la Nuova Frontiera dell’Embodied AI",
		"title-short": "Percepire, Ragionare, Agire",
		"URL": "https://iris.unimore.it/handle/11380/1271185",
		"author": [
			{
				"family": "Landi",
				"given": "Federico"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9QXEGDFT",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "University College Dublin",
		"source": "Google Scholar",
		"title": "Investigating Real-time Touchless Hand Interaction and Machine Learning Agents in Immersive Learning Environments",
		"URL": "https://www.researchgate.net/profile/Muhammad-Iqbal-248/publication/371378366_Investigating_Real-time_Touchless_Hand_Interaction_and_Machine_Learning_Agents_in_Immersive_Learning_Environments/links/6485bead79a722376526b76c/Investigating-Real-time-Touchless-Hand-Interaction-and-Machine-Learning-Agents-in-Immersive-Learning-Environments.pdf",
		"author": [
			{
				"family": "Iqbal",
				"given": "Muhammad Zahid"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SNQNNTGK",
		"type": "article",
		"abstract": "The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems.",
		"note": "arXiv:2210.01790 [cs]",
		"number": "arXiv:2210.01790",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals",
		"title-short": "Goal Misgeneralization",
		"URL": "http://arxiv.org/abs/2210.01790",
		"author": [
			{
				"family": "Shah",
				"given": "Rohin"
			},
			{
				"family": "Varma",
				"given": "Vikrant"
			},
			{
				"family": "Kumar",
				"given": "Ramana"
			},
			{
				"family": "Phuong",
				"given": "Mary"
			},
			{
				"family": "Krakovna",
				"given": "Victoria"
			},
			{
				"family": "Uesato",
				"given": "Jonathan"
			},
			{
				"family": "Kenton",
				"given": "Zac"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LFATN7L8",
		"type": "paper-conference",
		"container-title": "7th Annual Conference on Robot Learning",
		"source": "Google Scholar",
		"title": "A Data-Efficient Visual-Audio Representation with Intuitive Fine-tuning for Voice-Controlled Robots",
		"URL": "https://openreview.net/forum?id=dxOaNO8bge",
		"author": [
			{
				"family": "Chang",
				"given": "Peixin"
			},
			{
				"family": "Liu",
				"given": "Shuijing"
			},
			{
				"family": "Ji",
				"given": "Tianchen"
			},
			{
				"family": "Chakraborty",
				"given": "Neeloy"
			},
			{
				"family": "Hong",
				"given": "Kaiwen"
			},
			{
				"family": "Driggs-Campbell",
				"given": "Katherine Rose"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4NEWVTIW",
		"type": "article",
		"abstract": "We present SGoLAM, short for simultaneous goal localization and mapping, which is a simple and efficient algorithm for Multi-Object Goal navigation. Given an agent equipped with an RGB-D camera and a GPS/Compass sensor, our objective is to have the agent navigate to a sequence of target objects in realistic 3D environments. Our pipeline fully leverages the strength of classical approaches for visual navigation, by decomposing the problem into two key components: mapping and goal localization. The mapping module converts the depth observations into an occupancy map, and the goal localization module marks the locations of goal objects. The agent's policy is determined using the information provided by the two modules: if a current goal is found, plan towards the goal and otherwise, perform exploration. As our approach does not require any training of neural networks, it could be used in an off-the-shelf manner, and amenable for fast generalization in new, unseen environments. Nonetheless, our approach performs on par with the state-of-the-art learning-based approaches. SGoLAM is ranked 2nd in the CVPR 2021 MultiON (Multi-Object Goal Navigation) challenge. We have made our code publicly available at \\emph{https://github.com/eunsunlee/SGoLAM}.",
		"note": "arXiv:2110.07171 [cs]",
		"number": "arXiv:2110.07171",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SGoLAM: Simultaneous Goal Localization and Mapping for Multi-Object Goal Navigation",
		"title-short": "SGoLAM",
		"URL": "http://arxiv.org/abs/2110.07171",
		"author": [
			{
				"family": "Kim",
				"given": "Junho"
			},
			{
				"family": "Lee",
				"given": "Eun Sun"
			},
			{
				"family": "Lee",
				"given": "Mingi"
			},
			{
				"family": "Zhang",
				"given": "Donsu"
			},
			{
				"family": "Kim",
				"given": "Young Min"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					10,
					14
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/J68N6CNC",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Enriched Story Experiences with a New Video Interaction Model",
		"URL": "https://prism.ucalgary.ca/bitstreams/3d8d4666-8cc7-49f0-bd54-f2db98fda571/download",
		"author": [
			{
				"family": "Ma",
				"given": "Lynshao Celina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5CFKF6RC",
		"type": "article",
		"abstract": "In this work, we introduce SMART-LLM, an innovative framework designed for embodied multi-robot task planning. SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models (LLMs), harnesses the power of LLMs to convert high-level task instructions provided as input into a multi-robot task plan. It accomplishes this by executing a series of stages, including task decomposition, coalition formation, and task allocation, all guided by programmatic LLM prompts within the few-shot prompting paradigm. We create a benchmark dataset designed for validating the multi-robot task planning problem, encompassing four distinct categories of high-level instructions that vary in task complexity. Our evaluation experiments span both simulation and real-world scenarios, demonstrating that the proposed model can achieve promising results for generating multi-robot task plans. The experimental videos, code, and datasets from the work can be found at https://sites.google.com/view/smart-llm/.",
		"note": "arXiv:2309.10062 [cs]",
		"number": "arXiv:2309.10062",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SMART-LLM: Smart Multi-Agent Robot Task Planning using Large Language Models",
		"title-short": "SMART-LLM",
		"URL": "http://arxiv.org/abs/2309.10062",
		"author": [
			{
				"family": "Kannan",
				"given": "Shyam Sundar"
			},
			{
				"family": "Venkatesh",
				"given": "Vishnunandan L. N."
			},
			{
				"family": "Min",
				"given": "Byung-Cheol"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					18
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/ABYE4PM4",
		"type": "article-journal",
		"container-title": "Computers and Education: Artificial Intelligence",
		"note": "publisher: Elsevier",
		"page": "100087",
		"source": "Google Scholar",
		"title": "Educational applications of artificial intelligence in simulation-based learning: A systematic mapping review",
		"title-short": "Educational applications of artificial intelligence in simulation-based learning",
		"URL": "https://www.sciencedirect.com/science/article/pii/S2666920X2200042X",
		"author": [
			{
				"family": "Dai",
				"given": "Chih-Pu"
			},
			{
				"family": "Ke",
				"given": "Fengfeng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/26VGHAJW",
		"type": "article",
		"abstract": "The task of Visual Object Navigation (VON) involves an agent's ability to locate a particular object within a given scene. In order to successfully accomplish the VON task, two essential conditions must be fulfilled:1) the user must know the name of the desired object; and 2) the user-specified object must actually be present within the scene. To meet these conditions, a simulator can incorporate pre-defined object names and positions into the metadata of the scene. However, in real-world scenarios, it is often challenging to ensure that these conditions are always met. Human in an unfamiliar environment may not know which objects are present in the scene, or they may mistakenly specify an object that is not actually present. Nevertheless, despite these challenges, human may still have a demand for an object, which could potentially be fulfilled by other objects present within the scene in an equivalent manner. Hence, we propose Demand-driven Navigation (DDN), which leverages the user's demand as the task instruction and prompts the agent to find the object matches the specified demand. DDN aims to relax the stringent conditions of VON by focusing on fulfilling the user's demand rather than relying solely on predefined object categories or names. We propose a method first acquire textual attribute features of objects by extracting common knowledge from a large language model. These textual attribute features are subsequently aligned with visual attribute features using Contrastive Language-Image Pre-training (CLIP). By incorporating the visual attribute features as prior knowledge, we enhance the navigation process. Experiments on AI2Thor with the ProcThor dataset demonstrate the visual attribute features improve the agent's navigation performance and outperform the baseline methods commonly used in VON.",
		"note": "arXiv:2309.08138 [cs]",
		"number": "arXiv:2309.08138",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation",
		"title-short": "Find What You Want",
		"URL": "http://arxiv.org/abs/2309.08138",
		"author": [
			{
				"family": "Wang",
				"given": "Hongcheng"
			},
			{
				"family": "Chen",
				"given": "Andy Guan Hong"
			},
			{
				"family": "Li",
				"given": "Xiaoqi"
			},
			{
				"family": "Wu",
				"given": "Mingdong"
			},
			{
				"family": "Dong",
				"given": "Hao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11,
					6
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WSZSG94W",
		"type": "article-journal",
		"container-title": "Robotics and Autonomous Systems",
		"note": "publisher: Elsevier",
		"page": "104464",
		"source": "Google Scholar",
		"title": "Scalable modular synthetic data generation for advancing aerial autonomy",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0921889023001033",
		"volume": "166",
		"author": [
			{
				"family": "Sabet",
				"given": "Mehrnaz"
			},
			{
				"family": "Palanisamy",
				"given": "Praveen"
			},
			{
				"family": "Mishra",
				"given": "Sakshi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QY99AQBD",
		"type": "article",
		"abstract": "Vision-and-Language Navigation (VLN) is a task to guide an embodied agent moving to a target position using language instructions. Despite the significant performance improvement, the wide use of fine-grained instructions fails to characterize more practical linguistic variations in reality. To fill in this gap, we introduce a new setting, namely Underspecified vision-and-Language Navigation (ULN), and associated evaluation datasets. ULN evaluates agents using multi-level underspecified instructions instead of purely fine-grained or coarse-grained, which is a more realistic and general setting. As a primary step toward ULN, we propose a VLN framework that consists of a classification module, a navigation agent, and an Exploitation-to-Exploration (E2E) module. Specifically, we propose to learn Granularity Specific Sub-networks (GSS) for the agent to ground multi-level instructions with minimal additional parameters. Then, our E2E module estimates grounding uncertainty and conducts multi-step lookahead exploration to improve the success rate further. Experimental results show that existing VLN models are still brittle to multi-level language underspecification. Our framework is more robust and outperforms the baselines on ULN by ~10% relative success rate across all levels.",
		"note": "arXiv:2210.10020 [cs]",
		"number": "arXiv:2210.10020",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ULN: Towards Underspecified Vision-and-Language Navigation",
		"title-short": "ULN",
		"URL": "http://arxiv.org/abs/2210.10020",
		"author": [
			{
				"family": "Feng",
				"given": "Weixi"
			},
			{
				"family": "Fu",
				"given": "Tsu-Jui"
			},
			{
				"family": "Lu",
				"given": "Yujie"
			},
			{
				"family": "Wang",
				"given": "William Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					18
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RXQR2W9K",
		"type": "thesis",
		"genre": "B.S. thesis",
		"publisher": "Universitat Politècnica de Catalunya",
		"source": "Google Scholar",
		"title": "Unsupervised skill learning from pixels",
		"URL": "https://upcommons.upc.edu/handle/2117/353772",
		"author": [
			{
				"family": "Creus Castanyer",
				"given": "Roger"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JNTD2FQZ",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "EMBODIED QUESTION ANSWERING IN ROBOTIC ENVIRONMENT Automatic generation of a synthetic question-answer data-set",
		"URL": "https://gupea.ub.gu.se/handle/2077/70001",
		"author": [
			{
				"family": "Aruqi",
				"given": "Ali"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VKUMUHHV",
		"type": "article",
		"abstract": "Embodied agents face significant challenges when tasked with performing actions in diverse environments, particularly in generalizing across object types and executing suitable actions to accomplish tasks. Furthermore, agents should exhibit robustness, minimizing the execution of illegal actions. In this work, we present Egocentric Planning, an innovative approach that combines symbolic planning and Object-oriented POMDPs to solve tasks in complex environments, harnessing existing models for visual perception and natural language processing. We evaluated our approach in ALFRED, a simulated environment designed for domestic tasks, and demonstrated its high scalability, achieving an impressive 36.07% unseen success rate in the ALFRED benchmark and winning the ALFRED challenge at CVPR Embodied AI workshop. Our method requires reliable perception and the specification or learning of a symbolic description of the preconditions and effects of the agent's actions, as well as what object types reveal information about others. It is capable of naturally scaling to solve new tasks beyond ALFRED, as long as they can be solved using the available skills. This work offers a solid baseline for studying end-to-end and hybrid methods that aim to generalize to new tasks, including recent approaches relying on LLMs, but often struggle to scale to long sequences of actions or produce robust plans for novel tasks.",
		"note": "arXiv:2306.01295 [cs]",
		"number": "arXiv:2306.01295",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Egocentric Planning for Scalable Embodied Task Achievement",
		"URL": "http://arxiv.org/abs/2306.01295",
		"author": [
			{
				"family": "Liu",
				"given": "Xiaotian"
			},
			{
				"family": "Palacios",
				"given": "Hector"
			},
			{
				"family": "Muise",
				"given": "Christian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JNRE99SG",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Automatic Routing of Submarine Electrical Cables using Machine Learning",
		"URL": "https://apps.dtic.mil/sti/trecms/pdf/AD1213204.pdf",
		"author": [
			{
				"family": "Damaso",
				"given": "Katelyn"
			},
			{
				"family": "Herman",
				"given": "Jessica L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DR6GDJPP",
		"type": "article-journal",
		"container-title": "Advanced Robotics",
		"DOI": "10.1080/01691864.2022.2145235",
		"ISSN": "0169-1864, 1568-5535",
		"issue": "8",
		"journalAbbreviation": "Advanced Robotics",
		"language": "en",
		"page": "510-517",
		"source": "DOI.org (Crossref)",
		"title": "Evaluation of an online human-robot interaction competition platform based on virtual reality – case study in RCAP2021",
		"URL": "https://www.tandfonline.com/doi/full/10.1080/01691864.2022.2145235",
		"volume": "37",
		"author": [
			{
				"family": "Mizuchi",
				"given": "Yoshiaki"
			},
			{
				"family": "Yamada",
				"given": "Hiroki"
			},
			{
				"family": "Inamura",
				"given": "Tetsunari"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4,
					18
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SNWAK3W8",
		"type": "article-journal",
		"container-title": "Neurocomputing",
		"note": "publisher: Elsevier",
		"page": "368–377",
		"source": "Google Scholar",
		"title": "Multi goals and multi scenes visual mapless navigation in indoor using meta-learning and scene priors",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0925231221004707",
		"volume": "449",
		"author": [
			{
				"family": "Li",
				"given": "Fei"
			},
			{
				"family": "Guo",
				"given": "Chi"
			},
			{
				"family": "Luo",
				"given": "Binhan"
			},
			{
				"family": "Zhang",
				"given": "Huyin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/V36PS5SX",
		"type": "chapter",
		"container-title": "Intelligent Robotics and Applications",
		"event-place": "Singapore",
		"ISBN": "978-981-9964-94-9",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-981-99-6495-6_33",
		"page": "389-400",
		"publisher": "Springer Nature Singapore",
		"publisher-place": "Singapore",
		"source": "DOI.org (Crossref)",
		"title": "Experience Adapter: Adapting Pre-trained Language Models for Continual Task Planning",
		"title-short": "Experience Adapter",
		"URL": "https://link.springer.com/10.1007/978-981-99-6495-6_33",
		"volume": "14271",
		"editor": [
			{
				"family": "Yang",
				"given": "Huayong"
			},
			{
				"family": "Liu",
				"given": "Honghai"
			},
			{
				"family": "Zou",
				"given": "Jun"
			},
			{
				"family": "Yin",
				"given": "Zhouping"
			},
			{
				"family": "Liu",
				"given": "Lianqing"
			},
			{
				"family": "Yang",
				"given": "Geng"
			},
			{
				"family": "Ouyang",
				"given": "Xiaoping"
			},
			{
				"family": "Wang",
				"given": "Zhiyong"
			}
		],
		"author": [
			{
				"family": "Zhang",
				"given": "Jiatao"
			},
			{
				"family": "Liao",
				"given": "Jianfeng"
			},
			{
				"family": "Hu",
				"given": "Tuocheng"
			},
			{
				"family": "Zhou",
				"given": "Tian"
			},
			{
				"family": "Qian",
				"given": "Haofu"
			},
			{
				"family": "Zhang",
				"given": "Haoyang"
			},
			{
				"family": "Li",
				"given": "Han"
			},
			{
				"family": "Tang",
				"given": "LanLing"
			},
			{
				"family": "Meng",
				"given": "Qiwei"
			},
			{
				"family": "Song",
				"given": "Wei"
			},
			{
				"family": "Zhu",
				"given": "Shiqiang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YT2K22ZS",
		"type": "paper-conference",
		"container-title": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
		"page": "322–331",
		"source": "Google Scholar",
		"title": "Explicit object relation alignment for vision and language navigation",
		"URL": "https://aclanthology.org/2022.acl-srw.24/",
		"author": [
			{
				"family": "Zhang",
				"given": "Yue"
			},
			{
				"family": "Kordjamshidi",
				"given": "Parisa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SZS3EVZS",
		"type": "article",
		"abstract": "Highly autonomous generative agents powered by large language models promise to simulate intricate social behaviors in virtual societies. However, achieving real-time interactions with humans at a low computational cost remains challenging. Here, we introduce Lyfe Agents. They combine low-cost with real-time responsiveness, all while remaining intelligent and goal-oriented. Key innovations include: (1) an option-action framework, reducing the cost of high-level decisions; (2) asynchronous self-monitoring for better self-consistency; and (3) a Summarize-and-Forget memory mechanism, prioritizing critical memory items at a low cost. We evaluate Lyfe Agents' self-motivation and sociability across several multi-agent scenarios in our custom LyfeGame 3D virtual environment platform. When equipped with our brain-inspired techniques, Lyfe Agents can exhibit human-like self-motivated social reasoning. For example, the agents can solve a crime (a murder mystery) through autonomous collaboration and information exchange. Meanwhile, our techniques enabled Lyfe Agents to operate at a computational cost 10-100 times lower than existing alternatives. Our findings underscore the transformative potential of autonomous generative agents to enrich human social experiences in virtual worlds.",
		"note": "arXiv:2310.02172 [cs]",
		"number": "arXiv:2310.02172",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Lyfe Agents: Generative agents for low-cost real-time social interactions",
		"title-short": "Lyfe Agents",
		"URL": "http://arxiv.org/abs/2310.02172",
		"author": [
			{
				"family": "Kaiya",
				"given": "Zhao"
			},
			{
				"family": "Naim",
				"given": "Michelangelo"
			},
			{
				"family": "Kondic",
				"given": "Jovana"
			},
			{
				"family": "Cortes",
				"given": "Manuel"
			},
			{
				"family": "Ge",
				"given": "Jiaxin"
			},
			{
				"family": "Luo",
				"given": "Shuying"
			},
			{
				"family": "Yang",
				"given": "Guangyu Robert"
			},
			{
				"family": "Ahn",
				"given": "Andrew"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/PACRWVXF",
		"type": "article-journal",
		"container-title": "IEEE Access",
		"note": "publisher: IEEE",
		"source": "Google Scholar",
		"title": "A review on machine learning styles in computer vision-techniques and future directions",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9903420/",
		"author": [
			{
				"family": "Mahadevkar",
				"given": "Supriya V."
			},
			{
				"family": "Khemani",
				"given": "Bharti"
			},
			{
				"family": "Patil",
				"given": "Shruti"
			},
			{
				"family": "Kotecha",
				"given": "Ketan"
			},
			{
				"family": "Vora",
				"given": "Deepali"
			},
			{
				"family": "Abraham",
				"given": "Ajith"
			},
			{
				"family": "Gabralla",
				"given": "Lubina Abdelkareim"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/56F4FYH6",
		"type": "article-journal",
		"container-title": "Engineering Mathematics and Artificial Intelligence: Foundations, Methods, and Applications",
		"note": "publisher: CRC Press",
		"page": "387",
		"source": "Google Scholar",
		"title": "16 AI in Ecommerce: From",
		"title-short": "16 AI in Ecommerce",
		"URL": "https://books.google.com/books?hl=en&lr=&id=0wnHEAAAQBAJ&oi=fnd&pg=PA387&dq=(%22Reinforcement+Learning%22+OR+%22Deep+Reinforcement+Learning%22)+AND+((Instruction+OR+Prompt)+AND+(follow+OR+following))+AND+%22Natural+Language%22+AND+%223D+Environment%22&ots=wI8hRqydoj&sig=BUI6e-uQz42JH9onU7pKKFtByf4",
		"author": [
			{
				"family": "Riccoboni",
				"given": "Adam"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AMQJF8VA",
		"type": "book",
		"publisher": "University of California, Los Angeles",
		"source": "Google Scholar",
		"title": "Bidirectional Mental State Alignment for Human-Machine Collaboration",
		"URL": "https://search.proquest.com/openview/fb2a1d2a7d330ada05087598be7f9485/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Gao",
				"given": "Xiaofeng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/PV6RBJZU",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "The University of New Mexico",
		"source": "Google Scholar",
		"title": "Integrating Deep Learning and Augmented Reality to Enhance Situational Awareness in Firefighting Environments",
		"URL": "https://search.proquest.com/openview/553e4deb3558499841b696289954c84e/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Bhattarai",
				"given": "Manish"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VL2EGVJL",
		"type": "article",
		"abstract": "We introduce a novel interface for large scale collection of human memory and assistance. Using the 3D Matterport simulator we create a realistic indoor environments in which we have people perform specific embodied memory tasks that mimic household daily activities. This interface was then deployed on Amazon Mechanical Turk allowing us to test and record human memory, navigation and needs for assistance at a large scale that was previously impossible. Using the interface we collect the `The Visually Grounded Memory Assistant Dataset' which is aimed at developing our understanding of (1) the information people encode during navigation of 3D environments and (2) conditions under which people ask for memory assistance. Additionally we experiment with with predicting when people will ask for assistance using models trained on hand-selected visual and semantic features. This provides an opportunity to build stronger ties between the machine-learning and cognitive-science communities through learned models of human perception, memory, and cognition.",
		"note": "arXiv:2210.03787 [cs]",
		"number": "arXiv:2210.03787",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning a Visually Grounded Memory Assistant",
		"URL": "http://arxiv.org/abs/2210.03787",
		"author": [
			{
				"family": "Hahn",
				"given": "Meera"
			},
			{
				"family": "Carlberg",
				"given": "Kevin"
			},
			{
				"family": "Desai",
				"given": "Ruta"
			},
			{
				"family": "Hillis",
				"given": "James"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10,
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/PB3PJ5VY",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
		"note": "publisher: IEEE",
		"source": "Google Scholar",
		"title": "Development of Few-Shot Learning Capabilities in Artificial Neural Networks When Learning Through Self-Supervised Interaction",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10274870/",
		"author": [
			{
				"family": "Clay",
				"given": "Viviane"
			},
			{
				"family": "Pipa",
				"given": "Gordon"
			},
			{
				"family": "Kühnberger",
				"given": "Kai-Uwe"
			},
			{
				"family": "König",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IFSFUWIZ",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Neural Networks and Learning Systems",
		"note": "publisher: IEEE",
		"source": "Google Scholar",
		"title": "Robust-EQA: robust learning for embodied question answering with noisy labels",
		"title-short": "Robust-EQA",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10068177/",
		"author": [
			{
				"family": "Luo",
				"given": "Haonan"
			},
			{
				"family": "Lin",
				"given": "Guosheng"
			},
			{
				"family": "Shen",
				"given": "Fumin"
			},
			{
				"family": "Huang",
				"given": "Xingguo"
			},
			{
				"family": "Yao",
				"given": "Yazhou"
			},
			{
				"family": "Shen",
				"given": "Hengtao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/36EMZQZ8",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Deep Temporal Sets with Evidential Reinforced Attentions for Unique Behavioral Pattern Discovery",
		"URL": "https://openreview.net/forum?id=aaI18lTgjr",
		"author": [
			{
				"family": "Wang",
				"given": "Dingrong"
			},
			{
				"family": "Pandey",
				"given": "Deep Shankar"
			},
			{
				"family": "Neupane",
				"given": "Krishna Prasad"
			},
			{
				"family": "Yu",
				"given": "Zhiwei"
			},
			{
				"family": "Zheng",
				"given": "Ervine"
			},
			{
				"family": "Zheng",
				"given": "Zhi"
			},
			{
				"family": "Yu",
				"given": "Qi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LYCNAMK2",
		"type": "book",
		"publisher": "University of California, Berkeley",
		"source": "Google Scholar",
		"title": "On Building Generalizable Learning Agents",
		"URL": "https://search.proquest.com/openview/c4b7f5ed77400def0ab74a6125d9dcdf/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Wu",
				"given": "Yi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JS38G73H",
		"type": "article",
		"abstract": "In our pursuit of advancing multi-modal AI assistants capable of guiding users to achieve complex multi-step goals, we propose the task of \"Visual Planning for Assistance (VPA)\". Given a succinct natural language goal, e.g., \"make a shelf\", and a video of the user's progress so far, the aim of VPA is to devise a plan, i.e., a sequence of actions such as \"sand shelf\", \"paint shelf\", etc. to realize the specified goal. This requires assessing the user's progress from the (untrimmed) video, and relating it to the requirements of natural language goal, i.e., which actions to select and in what order? Consequently, this requires handling long video history and arbitrarily complex action dependencies. To address these challenges, we decompose VPA into video action segmentation and forecasting. Importantly, we experiment by formulating the forecasting step as a multi-modal sequence modeling problem, allowing us to leverage the strength of pre-trained LMs (as the sequence model). This novel approach, which we call Visual Language Model based Planner (VLaMP), outperforms baselines across a suite of metrics that gauge the quality of the generated plans. Furthermore, through comprehensive ablations, we also isolate the value of each component--language pre-training, visual observations, and goal information. We have open-sourced all the data, model checkpoints, and training code.",
		"note": "arXiv:2304.09179 [cs]",
		"number": "arXiv:2304.09179",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Pretrained Language Models as Visual Planners for Human Assistance",
		"URL": "http://arxiv.org/abs/2304.09179",
		"author": [
			{
				"family": "Patel",
				"given": "Dhruvesh"
			},
			{
				"family": "Eghbalzadeh",
				"given": "Hamid"
			},
			{
				"family": "Kamra",
				"given": "Nitin"
			},
			{
				"family": "Iuzzolino",
				"given": "Michael Louis"
			},
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Desai",
				"given": "Ruta"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					26
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6XFVGWAK",
		"type": "article-journal",
		"container-title": "Expert Systems with Applications",
		"note": "publisher: Elsevier",
		"page": "118002",
		"source": "Google Scholar",
		"title": "Augmented Reality and Artificial Intelligence in industry: Trends, tools, and future challenges",
		"title-short": "Augmented Reality and Artificial Intelligence in industry",
		"URL": "https://www.sciencedirect.com/science/article/pii/S0957417422012246",
		"author": [
			{
				"family": "Devagiri",
				"given": "Jeevan S."
			},
			{
				"family": "Paheding",
				"given": "Sidike"
			},
			{
				"family": "Niyaz",
				"given": "Quamar"
			},
			{
				"family": "Yang",
				"given": "Xiaoli"
			},
			{
				"family": "Smith",
				"given": "Samantha"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/94L888JY",
		"type": "article",
		"abstract": "Dialogue-based human-AI collaboration can revolutionize collaborative problem-solving, creative exploration, and social support. To realize this goal, the development of automated agents proficient in skills such as negotiating, following instructions, establishing common ground, and progressing shared tasks is essential. This survey begins by reviewing the evolution of dialogue management paradigms in collaborative dialogue systems, from traditional handcrafted and information-state based methods to AI planning-inspired approaches. It then shifts focus to contemporary data-driven dialogue management techniques, which seek to transfer deep learning successes from form-filling and open-domain settings to collaborative contexts. The paper proceeds to analyze a selected set of recent works that apply neural approaches to collaborative dialogue management, spotlighting prevailing trends in the field. This survey hopes to provide foundational background for future advancements in collaborative dialogue management, particularly as the dialogue systems community continues to embrace the potential of large language models.",
		"note": "arXiv:2307.09021 [cs]",
		"number": "arXiv:2307.09021",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards a Neural Era in Dialogue Management for Collaboration: A Literature Survey",
		"title-short": "Towards a Neural Era in Dialogue Management for Collaboration",
		"URL": "http://arxiv.org/abs/2307.09021",
		"author": [
			{
				"family": "Mannekote",
				"given": "Amogh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					18
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/23UYDC5P",
		"type": "article-journal",
		"container-title": "The Journal of Supercomputing",
		"DOI": "10.1007/s11227-021-04283-5",
		"ISSN": "0920-8542, 1573-0484",
		"issue": "7",
		"journalAbbreviation": "J Supercomput",
		"language": "en",
		"page": "9907-9933",
		"source": "DOI.org (Crossref)",
		"title": "Relational attention-based Markov logic network for visual navigation",
		"URL": "https://link.springer.com/10.1007/s11227-021-04283-5",
		"volume": "78",
		"author": [
			{
				"family": "Zhou",
				"given": "Kang"
			},
			{
				"family": "Guo",
				"given": "Chi"
			},
			{
				"family": "Zhang",
				"given": "Huyin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DRUNDI95",
		"type": "article-journal",
		"container-title": "Digital Communications and Networks",
		"note": "publisher: Elsevier",
		"source": "Google Scholar",
		"title": "A new technology perspective of the Metaverse: Its essence, framework and challenges",
		"title-short": "A new technology perspective of the Metaverse",
		"URL": "https://www.sciencedirect.com/science/article/pii/S2352864823000524",
		"author": [
			{
				"family": "Shi",
				"given": "Feifei"
			},
			{
				"family": "Ning",
				"given": "Huansheng"
			},
			{
				"family": "Zhang",
				"given": "Xiaohong"
			},
			{
				"family": "Li",
				"given": "Rongyang"
			},
			{
				"family": "Tian",
				"given": "Qiaohui"
			},
			{
				"family": "Zhang",
				"given": "Shiming"
			},
			{
				"family": "Zheng",
				"given": "Yuanyuan"
			},
			{
				"family": "Guo",
				"given": "Yudong"
			},
			{
				"family": "Daneshmand",
				"given": "Mahmoud"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/U2N85JA7",
		"type": "article-journal",
		"container-title": "IEEE Transactions on Visualization and Computer Graphics",
		"note": "publisher: IEEE",
		"source": "Google Scholar",
		"title": "ARGUS: Visualization of AI-Assisted Task Guidance in AR",
		"title-short": "ARGUS",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10305427/",
		"author": [
			{
				"family": "Castelo",
				"given": "Sonia"
			},
			{
				"family": "Rulff",
				"given": "Joao"
			},
			{
				"family": "McGowan",
				"given": "Erin"
			},
			{
				"family": "Steers",
				"given": "Bea"
			},
			{
				"family": "Wu",
				"given": "Guande"
			},
			{
				"family": "Chen",
				"given": "Shaoyu"
			},
			{
				"family": "Roman",
				"given": "Iran"
			},
			{
				"family": "Lopez",
				"given": "Roque"
			},
			{
				"family": "Brewer",
				"given": "Ethan"
			},
			{
				"family": "Zhao",
				"given": "Chen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WJHJGYAA",
		"type": "article",
		"abstract": "Most artificial neural networks used for object detection and recognition are trained in a fully supervised setup. This is not only very resource consuming as it requires large data sets of labeled examples but also very different from how humans learn. We introduce a setup in which an artificial agent first learns in a simulated world through self-supervised exploration. Following this, the representations learned through interaction with the world can be used to associate semantic concepts such as different types of doors. To do this, we use a method we call fast concept mapping which uses correlated firing patterns of neurons to define and detect semantic concepts. This association works instantaneous with very few labeled examples, similar to what we observe in humans in a phenomenon called fast mapping. Strikingly, this method already identifies objects with as little as one labeled example which highlights the quality of the encoding learned self-supervised through embodiment using curiosity-driven exploration. It therefor presents a feasible strategy for learning concepts without much supervision and shows that through pure interaction with the world meaningful representations of an environment can be learned.",
		"note": "arXiv:2102.02153 [cs]",
		"number": "arXiv:2102.02153",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Fast Concept Mapping: The Emergence of Human Abilities in Artificial Neural Networks when Learning Embodied and Self-Supervised",
		"title-short": "Fast Concept Mapping",
		"URL": "http://arxiv.org/abs/2102.02153",
		"author": [
			{
				"family": "Clay",
				"given": "Viviane"
			},
			{
				"family": "König",
				"given": "Peter"
			},
			{
				"family": "Pipa",
				"given": "Gordon"
			},
			{
				"family": "Kühnberger",
				"given": "Kai-Uwe"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					2,
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9B7JCSLY",
		"type": "article-journal",
		"note": "publisher: Simon Fraser University",
		"source": "Google Scholar",
		"title": "Neural state machine for 2D and 3D visual question answering",
		"URL": "https://summit.sfu.ca/item/34849",
		"author": [
			{
				"family": "Kochiev",
				"given": "Leon"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5JKN6C6W",
		"type": "chapter",
		"container-title": "Computer Vision – ECCV 2020",
		"event-place": "Cham",
		"ISBN": "978-3-030-58557-0",
		"language": "en",
		"note": "collection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-030-58558-7_28",
		"page": "471-490",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "A Cordial Sync: Going Beyond Marginal Policies for Multi-agent Embodied Tasks",
		"title-short": "A Cordial Sync",
		"URL": "https://link.springer.com/10.1007/978-3-030-58558-7_28",
		"volume": "12350",
		"editor": [
			{
				"family": "Vedaldi",
				"given": "Andrea"
			},
			{
				"family": "Bischof",
				"given": "Horst"
			},
			{
				"family": "Brox",
				"given": "Thomas"
			},
			{
				"family": "Frahm",
				"given": "Jan-Michael"
			}
		],
		"author": [
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Weihs",
				"given": "Luca"
			},
			{
				"family": "Kolve",
				"given": "Eric"
			},
			{
				"family": "Farhadi",
				"given": "Ali"
			},
			{
				"family": "Lazebnik",
				"given": "Svetlana"
			},
			{
				"family": "Kembhavi",
				"given": "Aniruddha"
			},
			{
				"family": "Schwing",
				"given": "Alexander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/B5C2TAWN",
		"type": "chapter",
		"container-title": "Representation Learning for Natural Language Processing",
		"page": "211–240",
		"publisher": "Springer Nature Singapore Singapore",
		"source": "Google Scholar",
		"title": "Cross-Modal Representation Learning",
		"URL": "https://library.oapen.org/bitstream/handle/20.500.12657/76271/978-981-99-1600-9.pdf?sequence=1#page=227",
		"author": [
			{
				"family": "Yao",
				"given": "Yuan"
			},
			{
				"family": "Liu",
				"given": "Zhiyuan"
			},
			{
				"family": "Lin",
				"given": "Yankai"
			},
			{
				"family": "Sun",
				"given": "Maosong"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EK49IRV3",
		"type": "article-journal",
		"container-title": "Applied Artificial Intelligence",
		"DOI": "10.1080/08839514.2023.2179167",
		"ISSN": "0883-9514, 1087-6545",
		"issue": "1",
		"journalAbbreviation": "Applied Artificial Intelligence",
		"language": "en",
		"page": "2179167",
		"source": "DOI.org (Crossref)",
		"title": "Learning Bidirectional Action-Language Translation with Limited Supervision and Testing with Incongruent Input",
		"URL": "https://www.tandfonline.com/doi/full/10.1080/08839514.2023.2179167",
		"volume": "37",
		"author": [
			{
				"family": "Özdemir",
				"given": "Ozan"
			},
			{
				"family": "Kerzel",
				"given": "Matthias"
			},
			{
				"family": "Weber",
				"given": "Cornelius"
			},
			{
				"family": "Lee",
				"given": "Jae Hee"
			},
			{
				"family": "Hafez",
				"given": "Muhammad Burhan"
			},
			{
				"family": "Bruns",
				"given": "Patrick"
			},
			{
				"family": "Wermter",
				"given": "Stefan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					12,
					31
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/E9MBUJXL",
		"type": "article",
		"abstract": "In this study, we explore the potential of Multimodal Large Language Models (MLLMs) in improving embodied decision-making processes for agents. While Large Language Models (LLMs) have been widely used due to their advanced reasoning skills and vast world knowledge, MLLMs like GPT4-Vision offer enhanced visual understanding and reasoning capabilities. We investigate whether state-of-the-art MLLMs can handle embodied decision-making in an end-to-end manner and whether collaborations between LLMs and MLLMs can enhance decision-making. To address these questions, we introduce a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action. Additionally, we propose HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision-making. We compare end-to-end embodied decision-making and HOLMES on our benchmark and find that the GPT4-Vision model demonstrates strong end-to-end embodied decision-making abilities, outperforming GPT4-HOLMES in terms of average decision accuracy (+3%). However, this performance is exclusive to the latest GPT4-Vision model, surpassing the open-source state-of-the-art MLLM by 26%. Our results indicate that powerful MLLMs like GPT4-Vision hold promise for decision-making in embodied agents, offering new avenues for MLLM research. Code and data are open at https://github.com/pkunlp-icler/PCA-EVAL/.",
		"note": "arXiv:2310.02071 [cs]",
		"number": "arXiv:2310.02071",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond",
		"title-short": "Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model",
		"URL": "http://arxiv.org/abs/2310.02071",
		"author": [
			{
				"family": "Chen",
				"given": "Liang"
			},
			{
				"family": "Zhang",
				"given": "Yichi"
			},
			{
				"family": "Ren",
				"given": "Shuhuai"
			},
			{
				"family": "Zhao",
				"given": "Haozhe"
			},
			{
				"family": "Cai",
				"given": "Zefan"
			},
			{
				"family": "Wang",
				"given": "Yuchi"
			},
			{
				"family": "Wang",
				"given": "Peiyi"
			},
			{
				"family": "Liu",
				"given": "Tianyu"
			},
			{
				"family": "Chang",
				"given": "Baobao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					16
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/N8L3AJY2",
		"type": "article",
		"abstract": "HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.",
		"note": "arXiv:2306.11565 [cs]",
		"number": "arXiv:2306.11565",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "HomeRobot: Open-Vocabulary Mobile Manipulation",
		"title-short": "HomeRobot",
		"URL": "http://arxiv.org/abs/2306.11565",
		"author": [
			{
				"family": "Yenamandra",
				"given": "Sriram"
			},
			{
				"family": "Ramachandran",
				"given": "Arun"
			},
			{
				"family": "Yadav",
				"given": "Karmesh"
			},
			{
				"family": "Wang",
				"given": "Austin"
			},
			{
				"family": "Khanna",
				"given": "Mukul"
			},
			{
				"family": "Gervet",
				"given": "Theophile"
			},
			{
				"family": "Yang",
				"given": "Tsung-Yen"
			},
			{
				"family": "Jain",
				"given": "Vidhi"
			},
			{
				"family": "Clegg",
				"given": "Alexander William"
			},
			{
				"family": "Turner",
				"given": "John"
			},
			{
				"family": "Kira",
				"given": "Zsolt"
			},
			{
				"family": "Savva",
				"given": "Manolis"
			},
			{
				"family": "Chang",
				"given": "Angel"
			},
			{
				"family": "Chaplot",
				"given": "Devendra Singh"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Paxton",
				"given": "Chris"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AHJPVYKZ",
		"type": "article",
		"abstract": "General-purpose mobile robots need to complete tasks without exact human instructions. Large language models (LLMs) is a promising direction for realizing commonsense world knowledge and reasoning-based planning. Vision-language models (VLMs) transform environment percepts into vision-language semantics interpretable by LLMs. However, completing complex tasks often requires reasoning about information beyond what is currently perceived. We propose latent compositional semantic embeddings z* as a principled learning-based knowledge representation for queryable spatio-semantic memories. We mathematically prove that z* can always be found, and the optimal z* is the centroid for any set Z. We derive a probabilistic bound for estimating separability of related and unrelated semantics. We prove that z* is discoverable by iterative optimization by gradient descent from visual appearance and singular descriptions. We experimentally verify our findings on four embedding spaces incl. CLIP and SBERT. Our results show that z* can represent up to 10 semantics encoded by SBERT, and up to 100 semantics for ideal uniformly distributed high-dimensional embeddings. We demonstrate that a simple dense VLM trained on the COCO-Stuff dataset can learn z* for 181 overlapping semantics by 42.23 mIoU, while improving conventional non-overlapping open-vocabulary segmentation performance by +3.48 mIoU compared with a popular SOTA model.",
		"note": "arXiv:2310.04981 [cs]",
		"number": "arXiv:2310.04981",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Compositional Semantics for Open Vocabulary Spatio-semantic Representations",
		"URL": "http://arxiv.org/abs/2310.04981",
		"author": [
			{
				"family": "Karlsson",
				"given": "Robin"
			},
			{
				"family": "Lepe-Salazar",
				"given": "Francisco"
			},
			{
				"family": "Takeda",
				"given": "Kazuya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					7
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/USWAF3X6",
		"type": "article-journal",
		"container-title": "International Journal of Human–Computer Interaction",
		"DOI": "10.1080/10447318.2022.2118189",
		"ISSN": "1044-7318, 1532-7590",
		"journalAbbreviation": "International Journal of Human–Computer Interaction",
		"language": "en",
		"page": "1-19",
		"source": "DOI.org (Crossref)",
		"title": "Digital Twins in Human-Computer Interaction: A Systematic Review",
		"title-short": "Digital Twins in Human-Computer Interaction",
		"URL": "https://www.tandfonline.com/doi/full/10.1080/10447318.2022.2118189",
		"author": [
			{
				"family": "Barricelli",
				"given": "Barbara Rita"
			},
			{
				"family": "Fogli",
				"given": "Daniela"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					9,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/X5HNFE8F",
		"type": "paper-conference",
		"container-title": "Thirty-seventh Conference on Neural Information Processing Systems",
		"source": "Google Scholar",
		"title": "SPRING: Studying Papers and Reasoning to play Games",
		"title-short": "SPRING",
		"URL": "https://openreview.net/forum?id=jU9qiRMDtR",
		"author": [
			{
				"family": "Wu",
				"given": "Yue"
			},
			{
				"family": "Min",
				"given": "So Yeon"
			},
			{
				"family": "Prabhumoye",
				"given": "Shrimai"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Salakhutdinov",
				"given": "Ruslan"
			},
			{
				"family": "Azaria",
				"given": "Amos"
			},
			{
				"family": "Mitchell",
				"given": "Tom"
			},
			{
				"family": "Li",
				"given": "Yuanzhi"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/W7ZQPCCV",
		"type": "thesis",
		"genre": "PhD Thesis",
		"source": "Google Scholar",
		"title": "Theory of Mind in Situated Communication for Collaborative Tasks",
		"URL": "https://deepblue.lib.umich.edu/handle/2027.42/176435",
		"author": [
			{
				"family": "Bara",
				"given": "Cristian-Paul"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AFLHEPML",
		"type": "paper-conference",
		"container-title": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition",
		"page": "826–842",
		"source": "Google Scholar",
		"title": "Vision+ Language Applications: A Survey",
		"title-short": "Vision+ Language Applications",
		"URL": "https://openaccess.thecvf.com/content/CVPR2023W/GCV/html/Zhou_Vision__Language_Applications_A_Survey_CVPRW_2023_paper.html",
		"author": [
			{
				"family": "Zhou",
				"given": "Yutong"
			},
			{
				"family": "Shimada",
				"given": "Nobutaka"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/MV6CHXHL",
		"type": "article-journal",
		"abstract": "Abstract\n            The next step of digital development is the metaverse, which has the potential to drastically alter how people use technology and expand the range of services available beyond conventional systems that can be accessed online. As the efficiency, performance, and quality of service access reach their peak levels, the focus has shifted to the user experience. Due to this, there is an increasing demand for more involved and thorough customer service, and service providers are willing to increase their present standards. Consumers are genuinely asking for tactile and immersive elements in their digital interfaces, but these features can only be made possible by the metaverse's potentially futuristic subfields of virtual reality (VR), augmented reality (AR), mixed reality (MR), and extended reality (XR). However, the metaverse may not be widely used due to significant security and privacy issues either from underlying technology or produced by the new digital environment. A variety of fundamental problems, such as scalability and interoperability, can arise in terms of ensuring security for the metaverse because of the metaverse's inherent properties, such as immersive realism, sustainability, and heterogeneity. In this survey, we propose a hypothetical meta‐stack framework to understand the various components in the realm of metaverse and then provide wide‐ranging insights on the most recent development in metaverse realm in the context of cutting‐edge technologies, security vulnerabilities and preventive measures specific to the metaverse and the research challenges pertaining to metaverse.",
		"container-title": "Computer Animation and Virtual Worlds",
		"DOI": "10.1002/cav.2150",
		"ISSN": "1546-4261, 1546-427X",
		"issue": "5",
		"journalAbbreviation": "Computer Animation & Virtual",
		"language": "en",
		"page": "e2150",
		"source": "DOI.org (Crossref)",
		"title": "The realm of metaverse: A survey",
		"title-short": "The realm of metaverse",
		"URL": "https://onlinelibrary.wiley.com/doi/10.1002/cav.2150",
		"volume": "34",
		"author": [
			{
				"family": "Venugopal",
				"given": "Jothi Prakash"
			},
			{
				"family": "Subramanian",
				"given": "Arul Antran Vijay"
			},
			{
				"family": "Peatchimuthu",
				"given": "Jegathesh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7H6LV7WI",
		"type": "article-journal",
		"container-title": "Artnodes",
		"issue": "31",
		"note": "publisher: Universitat Oberta de Catalunya",
		"page": "1–10",
		"source": "Google Scholar",
		"title": "AI. R Taletorium: Artificial Intelligence 1001 Cyber Nights",
		"title-short": "AI. R Taletorium",
		"URL": "https://www.raco.cat/index.php/Artnodes/article/view/n31-nikolic",
		"author": [
			{
				"family": "Nikolić",
				"given": "Predrag"
			},
			{
				"family": "Bertin",
				"given": "Giacomo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EA6UUB9A",
		"type": "article",
		"abstract": "This paper describes REALab, a platform for embedded agency research in reinforcement learning (RL). REALab is designed to model the structure of tampering problems that may arise in real-world deployments of RL. Standard Markov Decision Process (MDP) formulations of RL and simulated environments mirroring the MDP structure assume secure access to feedback (e.g., rewards). This may be unrealistic in settings where agents are embedded and can corrupt the processes producing feedback (e.g., human supervisors, or an implemented reward function). We describe an alternative Corrupt Feedback MDP formulation and the REALab environment platform, which both avoid the secure feedback assumption. We hope the design of REALab provides a useful perspective on tampering problems, and that the platform may serve as a unit test for the presence of tampering incentives in RL agent designs.",
		"note": "arXiv:2011.08820 [cs]",
		"number": "arXiv:2011.08820",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "REALab: An Embedded Perspective on Tampering",
		"title-short": "REALab",
		"URL": "http://arxiv.org/abs/2011.08820",
		"author": [
			{
				"family": "Kumar",
				"given": "Ramana"
			},
			{
				"family": "Uesato",
				"given": "Jonathan"
			},
			{
				"family": "Ngo",
				"given": "Richard"
			},
			{
				"family": "Everitt",
				"given": "Tom"
			},
			{
				"family": "Krakovna",
				"given": "Victoria"
			},
			{
				"family": "Legg",
				"given": "Shane"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					11,
					17
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QF5E6XU4",
		"type": "paper-conference",
		"container-title": "2017 IEEE 26th International Symposium on Industrial Electronics (ISIE)",
		"page": "1311–1318",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "An adaptive self-organizing fuzzy logic controller in a serious game for motor impairment rehabilitation",
		"URL": "https://ieeexplore.ieee.org/abstract/document/8001435/",
		"author": [
			{
				"family": "Esfahlani",
				"given": "Shabnam Sadeghi"
			},
			{
				"family": "Cirstea",
				"given": "Silvia"
			},
			{
				"family": "Sanaei",
				"given": "Alireza"
			},
			{
				"family": "Wilson",
				"given": "George"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KLPVXTH9",
		"type": "article",
		"abstract": "In this document we describe a rationale for a research program aimed at building an open \"assistant\" in the game Minecraft, in order to make progress on the problems of natural language understanding and learning from dialogue.",
		"note": "arXiv:1907.09273 [cs]",
		"number": "arXiv:1907.09273",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Why Build an Assistant in Minecraft?",
		"URL": "http://arxiv.org/abs/1907.09273",
		"author": [
			{
				"family": "Szlam",
				"given": "Arthur"
			},
			{
				"family": "Gray",
				"given": "Jonathan"
			},
			{
				"family": "Srinet",
				"given": "Kavya"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Joulin",
				"given": "Armand"
			},
			{
				"family": "Synnaeve",
				"given": "Gabriel"
			},
			{
				"family": "Kiela",
				"given": "Douwe"
			},
			{
				"family": "Yu",
				"given": "Haonan"
			},
			{
				"family": "Chen",
				"given": "Zhuoyuan"
			},
			{
				"family": "Goyal",
				"given": "Siddharth"
			},
			{
				"family": "Guo",
				"given": "Demi"
			},
			{
				"family": "Rothermel",
				"given": "Danielle"
			},
			{
				"family": "Zitnick",
				"given": "C. Lawrence"
			},
			{
				"family": "Weston",
				"given": "Jason"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					25
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4MKK3MNK",
		"type": "article-journal",
		"abstract": "A wide range of human–robot collaborative applications in diverse domains, such as manufacturing, health care, the entertainment industry, and social interactions, require an autonomous robot to follow its human companion. Different working environments and applications pose diverse challenges by adding constraints on the choice of sensors, degree of autonomy, and dynamics of a person-following robot. Researchers have addressed these challenges in many ways and contributed to the development of a large body of literature. This paper provides a comprehensive overview of the literature by categorizing different aspects of person-following by autonomous robots. Also, the corresponding operational challenges are identified based on various design choices for ground, underwater, and aerial scenarios. In addition, state-of-the-art methods for perception, planning, control, and interaction are elaborately discussed and their applicability in varied operational scenarios is presented. Then some of the prominent methods are qualitatively compared, corresponding practicalities are illustrated, and their feasibility is analyzed for various use cases. Furthermore, several prospective application areas are identified, and open problems are highlighted for future research.",
		"container-title": "The International Journal of Robotics Research",
		"DOI": "10.1177/0278364919881683",
		"ISSN": "0278-3649, 1741-3176",
		"issue": "14",
		"journalAbbreviation": "The International Journal of Robotics Research",
		"language": "en",
		"page": "1581-1618",
		"source": "DOI.org (Crossref)",
		"title": "Person-following by autonomous robots: A categorical overview",
		"title-short": "Person-following by autonomous robots",
		"URL": "http://journals.sagepub.com/doi/10.1177/0278364919881683",
		"volume": "38",
		"author": [
			{
				"family": "Islam",
				"given": "Md Jahidul"
			},
			{
				"family": "Hong",
				"given": "Jungseok"
			},
			{
				"family": "Sattar",
				"given": "Junaed"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/D3BWUU6Q",
		"type": "article",
		"abstract": "Denoising diffusion models have shown great promise in human motion synthesis conditioned on natural language descriptions. However, integrating spatial constraints, such as pre-defined motion trajectories and obstacles, remains a challenge despite being essential for bridging the gap between isolated human motion and its surrounding environment. To address this issue, we propose Guided Motion Diffusion (GMD), a method that incorporates spatial constraints into the motion generation process. Specifically, we propose an effective feature projection scheme that manipulates motion representation to enhance the coherency between spatial information and local poses. Together with a new imputation formulation, the generated motion can reliably conform to spatial constraints such as global motion trajectories. Furthermore, given sparse spatial constraints (e.g. sparse keyframes), we introduce a new dense guidance approach to turn a sparse signal, which is susceptible to being ignored during the reverse steps, into denser signals to guide the generated motion to the given constraints. Our extensive experiments justify the development of GMD, which achieves a significant improvement over state-of-the-art methods in text-based motion generation while allowing control of the synthesized motions with spatial constraints.",
		"note": "arXiv:2305.12577 [cs]",
		"number": "arXiv:2305.12577",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Guided Motion Diffusion for Controllable Human Motion Synthesis",
		"URL": "http://arxiv.org/abs/2305.12577",
		"author": [
			{
				"family": "Karunratanakul",
				"given": "Korrawe"
			},
			{
				"family": "Preechakul",
				"given": "Konpat"
			},
			{
				"family": "Suwajanakorn",
				"given": "Supasorn"
			},
			{
				"family": "Tang",
				"given": "Siyu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					29
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/P2LWRIRL",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Nova Southeastern University",
		"source": "Google Scholar",
		"title": "Modified Structured Domain Randomization in a Synthetic Environment for Learning Algorithms",
		"URL": "https://search.proquest.com/openview/b34c1307df8c9d17598c6264cfa3bae3/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Croft",
				"given": "Bryan L."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8GLFCV32",
		"type": "article-journal",
		"abstract": "Robotics is powered by software. Software tools control the rate of innovation in robotics research, drive the growth of the robotics industry, and power the education of future innovators and developers. Nearly 900,000 open-source repositories on GitHub are tagged with the keyword robotics—a potentially vast resource, but only a fraction of those are truly accessible in terms of quality, licensability, understandability, and total cost of ownership. The challenge is to match this resource to the needs of students, researchers, and companies to power cutting-edge research and real-world industrial solutions. This article reviews software tools for robotics, including both those created by the community at large and those created by the authors, as well as their impact on education, research, and industry.\n            Expected final online publication date for the Annual Review of Control, Robotics, and Autonomous Systems, Volume 7 is May 2024. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",
		"container-title": "Annual Review of Control, Robotics, and Autonomous Systems",
		"DOI": "10.1146/annurev-control-061323-095841",
		"ISSN": "2573-5144, 2573-5144",
		"issue": "1",
		"journalAbbreviation": "Annu. Rev. Control Robot. Auton. Syst.",
		"language": "en",
		"page": "annurev-control-061323-095841",
		"source": "DOI.org (Crossref)",
		"title": "Robotics Software: Past, Present, and Future",
		"title-short": "Robotics Software",
		"URL": "https://www.annualreviews.org/doi/10.1146/annurev-control-061323-095841",
		"volume": "7",
		"author": [
			{
				"family": "Haviland",
				"given": "Jesse"
			},
			{
				"family": "Corke",
				"given": "Peter"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024",
					5,
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VCHGRJJ8",
		"type": "article-journal",
		"container-title": "CAAI Artificial Intelligence Research",
		"DOI": "10.26599/AIR.2022.9150008",
		"ISSN": "2097-194X",
		"issue": "2",
		"language": "en",
		"page": "111-136",
		"source": "DOI.org (Crossref)",
		"title": "A Survey of Vision and Language Related Multi-Modal Task",
		"URL": "https://www.sciopen.com/article/10.26599/AIR.2022.9150008",
		"volume": "1",
		"author": [
			{
				"family": "Wang",
				"given": "Lanxiao"
			},
			{
				"family": "Hu",
				"given": "Wenzhe"
			},
			{
				"family": "Qiu",
				"given": "Heqian"
			},
			{
				"family": "Shang",
				"given": "Chao"
			},
			{
				"family": "Zhao",
				"given": "Taijin"
			},
			{
				"family": "Qiu",
				"given": "Benliu"
			},
			{
				"family": "Ngan",
				"given": "King Ngi"
			},
			{
				"family": "Li",
				"given": "Hongliang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3M36WPVF",
		"type": "document",
		"source": "Google Scholar",
		"title": "Robotics for People: Perspectives on Interaction, Learning and Safety",
		"title-short": "Robotics for People",
		"URL": "https://hal.science/hal-03296937/document",
		"author": [
			{
				"family": "Aly",
				"given": "Amir"
			},
			{
				"family": "Bajcsy",
				"given": "Andrea"
			},
			{
				"family": "Schoellig",
				"given": "Angela"
			},
			{
				"family": "Fridovich-Keil",
				"given": "David"
			},
			{
				"family": "Correia",
				"given": "Filipa"
			},
			{
				"family": "Baraka",
				"given": "Kim"
			},
			{
				"family": "Gombolay",
				"given": "Matthew"
			},
			{
				"family": "Gopalan",
				"given": "Nakul"
			},
			{
				"family": "Senanayake",
				"given": "Ransalu"
			},
			{
				"family": "Kousik",
				"given": "Shreyas"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/8Y3LSZCI",
		"type": "article",
		"abstract": "Sight and hearing are two senses that play a vital role in human communication and scene understanding. To mimic human perception ability, audio-visual learning, aimed at developing computational approaches to learn from both audio and visual modalities, has been a flourishing field in recent years. A comprehensive survey that can systematically organize and analyze studies of the audio-visual field is expected. Starting from the analysis of audio-visual cognition foundations, we introduce several key findings that have inspired our computational studies. Then, we systematically review the recent audio-visual learning studies and divide them into three categories: audio-visual boosting, cross-modal perception and audio-visual collaboration. Through our analysis, we discover that, the consistency of audio-visual data across semantic, spatial and temporal support the above studies. To revisit the current development of the audio-visual learning field from a more macro view, we further propose a new perspective on audio-visual scene understanding, then discuss and analyze the feasible future direction of the audio-visual learning area. Overall, this survey reviews and outlooks the current audio-visual learning field from different aspects. We hope it can provide researchers with a better understanding of this area. A website including constantly-updated survey is released: \\url{https://gewu-lab.github.io/audio-visual-learning/}.",
		"note": "arXiv:2208.09579 [cs]",
		"number": "arXiv:2208.09579",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Learning in Audio-visual Context: A Review, Analysis, and New Perspective",
		"title-short": "Learning in Audio-visual Context",
		"URL": "http://arxiv.org/abs/2208.09579",
		"author": [
			{
				"family": "Wei",
				"given": "Yake"
			},
			{
				"family": "Hu",
				"given": "Di"
			},
			{
				"family": "Tian",
				"given": "Yapeng"
			},
			{
				"family": "Li",
				"given": "Xuelong"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					8,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/93PY56IY",
		"type": "article-journal",
		"container-title": "IEEE Access",
		"note": "publisher: IEEE",
		"source": "Google Scholar",
		"title": "Intelligent Metaverse Scene Content Construction",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10189902/",
		"author": [
			{
				"family": "Wang",
				"given": "Junxiang"
			},
			{
				"family": "Chen",
				"given": "Siru"
			},
			{
				"family": "Liu",
				"given": "Yuxuan"
			},
			{
				"family": "Lau",
				"given": "Richen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HN52H45Q",
		"type": "book",
		"publisher": "Packt Publishing Ltd",
		"source": "Google Scholar",
		"title": "Generative AI with Python and TensorFlow 2: Create images, text, and music with VAEs, GANs, LSTMs, Transformer models",
		"title-short": "Generative AI with Python and TensorFlow 2",
		"URL": "https://books.google.com/books?hl=en&lr=&id=4HIsEAAAQBAJ&oi=fnd&pg=PP1&dq=(%22Reinforcement+Learning%22+OR+%22Deep+Reinforcement+Learning%22)+AND+((Instruction+OR+Prompt)+AND+(follow+OR+following))+AND+%22Natural+Language%22+AND+%223D+Environment%22&ots=AaXiq5DDae&sig=bbZvjgCHXcl-Jw1thl4EneX-A1A",
		"author": [
			{
				"family": "Babcock",
				"given": "Joseph"
			},
			{
				"family": "Bali",
				"given": "Raghav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/K3RPWKSI",
		"type": "article",
		"abstract": "We present a retrospective on the state of Embodied AI research. Our analysis focuses on 13 challenges presented at the Embodied AI Workshop at CVPR. These challenges are grouped into three themes: (1) visual navigation, (2) rearrangement, and (3) embodied vision-and-language. We discuss the dominant datasets within each theme, evaluation metrics for the challenges, and the performance of state-of-the-art models. We highlight commonalities between top approaches to the challenges and identify potential future directions for Embodied AI research.",
		"note": "arXiv:2210.06849 [cs]",
		"number": "arXiv:2210.06849",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Retrospectives on the Embodied AI Workshop",
		"URL": "http://arxiv.org/abs/2210.06849",
		"author": [
			{
				"family": "Deitke",
				"given": "Matt"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			},
			{
				"family": "Bisk",
				"given": "Yonatan"
			},
			{
				"family": "Campari",
				"given": "Tommaso"
			},
			{
				"family": "Chang",
				"given": "Angel X."
			},
			{
				"family": "Chaplot",
				"given": "Devendra Singh"
			},
			{
				"family": "Chen",
				"given": "Changan"
			},
			{
				"family": "D'Arpino",
				"given": "Claudia Pérez"
			},
			{
				"family": "Ehsani",
				"given": "Kiana"
			},
			{
				"family": "Farhadi",
				"given": "Ali"
			},
			{
				"family": "Fei-Fei",
				"given": "Li"
			},
			{
				"family": "Francis",
				"given": "Anthony"
			},
			{
				"family": "Gan",
				"given": "Chuang"
			},
			{
				"family": "Grauman",
				"given": "Kristen"
			},
			{
				"family": "Hall",
				"given": "David"
			},
			{
				"family": "Han",
				"given": "Winson"
			},
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Kembhavi",
				"given": "Aniruddha"
			},
			{
				"family": "Krantz",
				"given": "Jacob"
			},
			{
				"family": "Lee",
				"given": "Stefan"
			},
			{
				"family": "Li",
				"given": "Chengshu"
			},
			{
				"family": "Majumder",
				"given": "Sagnik"
			},
			{
				"family": "Maksymets",
				"given": "Oleksandr"
			},
			{
				"family": "Martín-Martín",
				"given": "Roberto"
			},
			{
				"family": "Mottaghi",
				"given": "Roozbeh"
			},
			{
				"family": "Raychaudhuri",
				"given": "Sonia"
			},
			{
				"family": "Roberts",
				"given": "Mike"
			},
			{
				"family": "Savarese",
				"given": "Silvio"
			},
			{
				"family": "Savva",
				"given": "Manolis"
			},
			{
				"family": "Shridhar",
				"given": "Mohit"
			},
			{
				"family": "Sünderhauf",
				"given": "Niko"
			},
			{
				"family": "Szot",
				"given": "Andrew"
			},
			{
				"family": "Talbot",
				"given": "Ben"
			},
			{
				"family": "Tenenbaum",
				"given": "Joshua B."
			},
			{
				"family": "Thomason",
				"given": "Jesse"
			},
			{
				"family": "Toshev",
				"given": "Alexander"
			},
			{
				"family": "Truong",
				"given": "Joanne"
			},
			{
				"family": "Weihs",
				"given": "Luca"
			},
			{
				"family": "Wu",
				"given": "Jiajun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					12,
					4
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HXFEXSAN",
		"type": "article",
		"abstract": "We introduce SwiftSage, a novel agent framework inspired by the dual-process theory of human cognition, designed to excel in action planning for complex interactive reasoning tasks. SwiftSage integrates the strengths of behavior cloning and prompting large language models (LLMs) to enhance task completion performance. The framework comprises two primary modules: the Swift module, representing fast and intuitive thinking, and the Sage module, emulating deliberate thought processes. The Swift module is a small encoder-decoder LM fine-tuned on the oracle agent's action trajectories, while the Sage module employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a heuristic method to harmoniously integrate the two modules, resulting in a more efficient and robust problem-solving process. In 30 tasks from the ScienceWorld benchmark, SwiftSage significantly outperforms other methods such as SayCan, ReAct, and Reflexion, demonstrating its effectiveness in solving complex real-world tasks.",
		"note": "arXiv:2305.17390 [cs]",
		"number": "arXiv:2305.17390",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks",
		"title-short": "SwiftSage",
		"URL": "http://arxiv.org/abs/2305.17390",
		"author": [
			{
				"family": "Lin",
				"given": "Bill Yuchen"
			},
			{
				"family": "Fu",
				"given": "Yicheng"
			},
			{
				"family": "Yang",
				"given": "Karina"
			},
			{
				"family": "Ammanabrolu",
				"given": "Prithviraj"
			},
			{
				"family": "Brahman",
				"given": "Faeze"
			},
			{
				"family": "Huang",
				"given": "Shiyu"
			},
			{
				"family": "Bhagavatula",
				"given": "Chandra"
			},
			{
				"family": "Choi",
				"given": "Yejin"
			},
			{
				"family": "Ren",
				"given": "Xiang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					5,
					27
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KWAG3MT5",
		"type": "article-journal",
		"container-title": "Geosciences",
		"issue": "4",
		"note": "publisher: MDPI",
		"page": "96",
		"source": "Google Scholar",
		"title": "3D point clouds in archaeology: Advances in acquisition, processing and knowledge integration applied to quasi-planar objects",
		"title-short": "3D point clouds in archaeology",
		"URL": "https://www.mdpi.com/2076-3263/7/4/96",
		"volume": "7",
		"author": [
			{
				"family": "Poux",
				"given": "Florent"
			},
			{
				"family": "Neuville",
				"given": "Romain"
			},
			{
				"family": "Van Wersch",
				"given": "Line"
			},
			{
				"family": "Nys",
				"given": "Gilles-Antoine"
			},
			{
				"family": "Billen",
				"given": "Roland"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AWAYH3TM",
		"type": "article-journal",
		"abstract": "Abstract\n            \n              Visual mapless navigation (VMN), modeling a direct mapping between sensory inputs and agent actions, aims to navigate from a stochastic origin location to a prescribed goal in an unseen scene. A fundamental yet challenging issue in visual mapless navigation is generalizing to a new scene. Furthermore, it is of pivotal concern to design a method to make effective policy learning. To address these issues, we introduce a novel visual mapless navigation model, which integrates hierarchical semantic information represented by context vector with meta-learning to improve the generalization performance gap between known and unknown environments. Extensive experimental results on AI2-THOR benchmark dataset demonstrate that our model significantly outperforms the state-of-the-art model by\n              \n                \n                  $$15.79\\%$$\n                  \n                    \n                      15.79\n                      %\n                    \n                  \n                \n              \n              for the SPL and by\n              \n                \n                  $$23.83\\%$$\n                  \n                    \n                      23.83\n                      %\n                    \n                  \n                \n              \n              for the success rate. In addition, the exploration rate experiment shows that our model can effectively improve the invalid exploration behavior of the agent and accelerate the convergence speed of the model. Our implementation code and data can be viewed on\n              https://github.com/zhiyu-tech/WHU-CVVMN\n              .",
		"container-title": "Complex & Intelligent Systems",
		"DOI": "10.1007/s40747-022-00902-7",
		"ISSN": "2199-4536, 2198-6053",
		"issue": "2",
		"journalAbbreviation": "Complex Intell. Syst.",
		"language": "en",
		"page": "2031-2041",
		"source": "DOI.org (Crossref)",
		"title": "Context vector-based visual mapless navigation in indoor using hierarchical semantic information and meta-learning",
		"URL": "https://link.springer.com/10.1007/s40747-022-00902-7",
		"volume": "9",
		"author": [
			{
				"family": "Li",
				"given": "Fei"
			},
			{
				"family": "Guo",
				"given": "Chi"
			},
			{
				"family": "Zhang",
				"given": "Huyin"
			},
			{
				"family": "Luo",
				"given": "Binhan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					4
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LJ7SRUNH",
		"type": "book",
		"publisher": "Oxford University Press",
		"source": "Google Scholar",
		"title": "Natural General Intelligence: How understanding the brain can help us build AI",
		"title-short": "Natural General Intelligence",
		"URL": "https://books.google.com/books?hl=en&lr=&id=I7efEAAAQBAJ&oi=fnd&pg=PP1&dq=(%22Reinforcement+Learning%22+OR+%22Deep+Reinforcement+Learning%22)+AND+((Instruction+OR+Prompt)+AND+(follow+OR+following))+AND+%22Natural+Language%22+AND+%223D+Environment%22&ots=en_ua8GSIK&sig=PBymDrjEkXXVNIj0XTR42mtVY3k",
		"author": [
			{
				"family": "Summerfield",
				"given": "Christopher"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/S8LQAIMA",
		"type": "article",
		"abstract": "3D visual grounding is the ability to localize objects in 3D scenes conditioned by utterances. Most existing methods devote the referring head to localize the referred object directly, causing failure in complex scenarios. In addition, it does not illustrate how and why the network reaches the final decision. In this paper, we address this question Can we design an interpretable 3D visual grounding framework that has the potential to mimic the human perception system?. To this end, we formulate the 3D visual grounding problem as a sequence-to-sequence task by first predicting a chain of anchors and then the final target. Interpretability not only improves the overall performance but also helps us identify failure cases. Following the chain of thoughts approach enables us to decompose the referring task into interpretable intermediate steps, boosting the performance and making our framework extremely data-efficient. Moreover, our proposed framework can be easily integrated into any existing architecture. We validate our approach through comprehensive experiments on the Nr3D, Sr3D, and Scanrefer benchmarks and show consistent performance gains compared to existing methods without requiring manually annotated data. Furthermore, our proposed framework, dubbed CoT3DRef, is significantly data-efficient, whereas on the Sr3D dataset, when trained only on 10% of the data, we match the SOTA performance that trained on the entire data.",
		"note": "arXiv:2310.06214 [cs]",
		"number": "arXiv:2310.06214",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CoT3DRef: Chain-of-Thoughts Data-Efficient 3D Visual Grounding",
		"title-short": "CoT3DRef",
		"URL": "http://arxiv.org/abs/2310.06214",
		"author": [
			{
				"family": "Bakr",
				"given": "Eslam Mohamed"
			},
			{
				"family": "Ayman",
				"given": "Mohamed"
			},
			{
				"family": "Ahmed",
				"given": "Mahmoud"
			},
			{
				"family": "Slim",
				"given": "Habib"
			},
			{
				"family": "Elhoseiny",
				"given": "Mohamed"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					9
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3TKQUT5M",
		"type": "article",
		"abstract": "Recent studies have presented compelling evidence that large language models (LLMs) can equip embodied agents with the self-driven capability to interact with the world, which marks an initial step toward versatile robotics. However, these efforts tend to overlook the visual richness of open worlds, rendering the entire interactive process akin to \"a blindfolded text-based game.\" Consequently, LLM-based agents frequently encounter challenges in intuitively comprehending their surroundings and producing responses that are easy to understand. In this paper, we propose Steve-Eye, an end-to-end trained large multimodal model designed to address this limitation. Steve-Eye integrates the LLM with a visual encoder which enables it to process visual-text inputs and generate multimodal feedback. In addition, we use a semi-automatic strategy to collect an extensive dataset comprising 850K open-world instruction pairs, empowering our model to encompass three essential functions for an agent: multimodal perception, foundational knowledge base, and skill prediction and planning. Lastly, we develop three open-world evaluation benchmarks, then carry out extensive experiments from a wide range of perspectives to validate our model's capability to strategically act and plan. Codes and datasets will be released.",
		"note": "arXiv:2310.13255 [cs]",
		"number": "arXiv:2310.13255",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds",
		"title-short": "Steve-Eye",
		"URL": "http://arxiv.org/abs/2310.13255",
		"author": [
			{
				"family": "Zheng",
				"given": "Sipeng"
			},
			{
				"family": "Liu",
				"given": "Jiazheng"
			},
			{
				"family": "Feng",
				"given": "Yicheng"
			},
			{
				"family": "Lu",
				"given": "Zongqing"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					10,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WEUTBHQX",
		"type": "book",
		"publisher": "University of California, Berkeley",
		"source": "Google Scholar",
		"title": "Compositionality and Modularity for Robot Learning",
		"URL": "https://search.proquest.com/openview/1196cc4cb2cc4fbcae1f15b507743591/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Devin",
				"given": "Coline"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/Q4XHGPND",
		"type": "article",
		"abstract": "Visual Indoor Navigation (VIN) task has drawn increasing attention from the data-driven machine learning communities especially with the recently reported success from learning-based methods. Due to the innate complexity of this task, researchers have tried approaching the problem from a variety of different angles, the full scope of which has not yet been captured within an overarching report. This survey first summarizes the representative work of learning-based approaches for the VIN task and then identifies and discusses lingering issues impeding the VIN performance, as well as motivates future research in these key areas worth exploring for the community.",
		"note": "arXiv:2002.11310 [cs]",
		"number": "arXiv:2002.11310",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "From Seeing to Moving: A Survey on Learning for Visual Indoor Navigation (VIN)",
		"title-short": "From Seeing to Moving",
		"URL": "http://arxiv.org/abs/2002.11310",
		"author": [
			{
				"family": "Ye",
				"given": "Xin"
			},
			{
				"family": "Yang",
				"given": "Yezhou"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					7,
					2
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FTB8YATT",
		"type": "article",
		"abstract": "Since 2021, the term \"Metaverse\" has been the most popular one, garnering a lot of interest. Because of its contained environment and built-in computing and networking capabilities, a modern car makes an intriguing location to host its own little metaverse. Additionally, the travellers don't have much to do to pass the time while traveling, making them ideal customers for immersive services. Vetaverse (Vehicular-Metaverse), which we define as the future continuum between vehicular industries and Metaverse, is envisioned as a blended immersive realm that scales up to cities and countries, as digital twins of the intelligent Transportation Systems, referred to as \"TS-Metaverse\", as well as customized XR services inside each Individual Vehicle, referred to as \"IV-Metaverse\". The two subcategories serve fundamentally different purposes, namely long-term interconnection, maintenance, monitoring, and management on scale for large transportation systems (TS), and personalized, private, and immersive infotainment services (IV). By outlining the framework of Vetaverse and examining important enabler technologies, we reveal this impending trend. Additionally, we examine unresolved issues and potential routes for future study while highlighting some intriguing Vetaverse services.",
		"note": "arXiv:2210.15109 [cs]",
		"number": "arXiv:2210.15109",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Vetaverse: A Survey on the Intersection of Metaverse, Vehicles, and Transportation Systems",
		"title-short": "Vetaverse",
		"URL": "http://arxiv.org/abs/2210.15109",
		"author": [
			{
				"family": "Zhou",
				"given": "Pengyuan"
			},
			{
				"family": "Zhu",
				"given": "Jinjing"
			},
			{
				"family": "Wang",
				"given": "Yiting"
			},
			{
				"family": "Lu",
				"given": "Yunfan"
			},
			{
				"family": "Wei",
				"given": "Zixiang"
			},
			{
				"family": "Shi",
				"given": "Haolin"
			},
			{
				"family": "Ding",
				"given": "Yuchen"
			},
			{
				"family": "Gao",
				"given": "Yu"
			},
			{
				"family": "Huang",
				"given": "Qinglong"
			},
			{
				"family": "Shi",
				"given": "Yan"
			},
			{
				"family": "Alhilal",
				"given": "Ahmad"
			},
			{
				"family": "Lee",
				"given": "Lik-Hang"
			},
			{
				"family": "Braud",
				"given": "Tristan"
			},
			{
				"family": "Hui",
				"given": "Pan"
			},
			{
				"family": "Wang",
				"given": "Lin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					16
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SVDA5BUI",
		"type": "book",
		"publisher": "Packt Publishing Ltd",
		"source": "Google Scholar",
		"title": "The Machine Learning Solutions Architect Handbook: Create machine learning platforms to run solutions in an enterprise setting",
		"title-short": "The Machine Learning Solutions Architect Handbook",
		"URL": "https://books.google.com/books?hl=en&lr=&id=dllVEAAAQBAJ&oi=fnd&pg=PP1&dq=(%22Reinforcement+Learning%22+OR+%22Deep+Reinforcement+Learning%22)+AND+((Instruction+OR+Prompt)+AND+(follow+OR+following))+AND+%22Natural+Language%22+AND+%223D+Environment%22&ots=vCbbpK6GDn&sig=ZbQkm4XkvHBJTbk8Mrn1wTKDgvo",
		"author": [
			{
				"family": "Ping",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HKDG62PJ",
		"type": "article-journal",
		"container-title": "Advances in Neural Information Processing Systems",
		"page": "9700–9712",
		"source": "Google Scholar",
		"title": "Multion: Benchmarking semantic map memory using multi-object navigation",
		"title-short": "Multion",
		"URL": "https://proceedings.neurips.cc/paper/2020/hash/6e01383fd96a17ae51cc3e15447e7533-Abstract.html",
		"volume": "33",
		"author": [
			{
				"family": "Wani",
				"given": "Saim"
			},
			{
				"family": "Patel",
				"given": "Shivansh"
			},
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Chang",
				"given": "Angel"
			},
			{
				"family": "Savva",
				"given": "Manolis"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/UYTCNW3U",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Answering visual what-if questions",
		"URL": "http://pure-oai.bham.ac.uk/ws/files/57251224/Answering_Visual_What_If_Questions.pdf",
		"author": [
			{
				"family": "Wagner",
				"given": "Misha"
			},
			{
				"family": "Basevi",
				"given": "Hector"
			},
			{
				"family": "Shetty",
				"given": "Rakshith"
			},
			{
				"family": "Li",
				"given": "Wenbin"
			},
			{
				"family": "Malinowski",
				"given": "Mateusz"
			},
			{
				"family": "Fritz",
				"given": "Mario"
			},
			{
				"family": "Leonardis",
				"given": "Ales"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JX3V4EMP",
		"type": "article",
		"abstract": "With the advances in robotic technology, research in human-robot collaboration (HRC) has gained in importance. For robots to interact with humans autonomously they need active decision making that takes human partners into account. However, state-of-the-art research in HRC does often assume a leader-follower division, in which one agent leads the interaction. We believe that this is caused by the lack of a reliable representation of the human and the environment to allow autonomous decision making. This problem can be overcome by an embodied approach to HRC which is inspired by psychological studies of human-human interaction (HHI). In this survey, we review neuroscientific and psychological findings of the sensorimotor patterns that govern HHI and view them in a robotics context. Additionally, we study the advances made by the robotic community into the direction of embodied HRC. We focus on the mechanisms that are required for active, physical human-robot collaboration. Finally, we discuss the similarities and differences in the two fields of study which pinpoint directions of future research.",
		"note": "arXiv:1705.10146 [cs]",
		"number": "arXiv:1705.10146",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Human-Robot Collaboration: From Psychology to Social Robotics",
		"title-short": "Human-Robot Collaboration",
		"URL": "http://arxiv.org/abs/1705.10146",
		"author": [
			{
				"family": "Bütepage",
				"given": "Judith"
			},
			{
				"family": "Kragic",
				"given": "Danica"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					5,
					29
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YXXFMMUI",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Princess Sumaya University for Technology (Jordan)",
		"source": "Google Scholar",
		"title": "Artificial Intelligence Impact on Soldiers in Virtual Reality Training Simulators",
		"URL": "https://search.proquest.com/openview/367763e7a9c2601e11c4e75be87a59e4/1?pq-origsite=gscholar&cbl=2026366&diss=y",
		"author": [
			{
				"family": "AlSaeed",
				"given": "Dania Adnan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5J8UL8FL",
		"type": "article-journal",
		"note": "publisher: EΛ. ME. ΠA., Σχoλ\\acute\\eta Mηχανıotaκ\\acuteømegaν (ΣMHX), ΠMΣ Πληρoφoρıotaκ\\acute\\eta καıota Πoλυμ\\acute\\varepsilonσα",
		"source": "Google Scholar",
		"title": "Extracting learning analytics from game based learning sessions generated by multimodal data sources.",
		"URL": "https://apothesis.lib.hmu.gr/handle/20.500.12688/9947",
		"author": [
			{
				"family": "Trampas",
				"given": "Apostolos-Marios"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/TXG5A3WN",
		"type": "article",
		"abstract": "We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual 'foundation models' for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data scale and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 5.6M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Finally, we show that task or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. These models required over 10,000 GPU-hours to train and can be found on our website for the benefit of the research community.",
		"note": "arXiv:2303.18240 [cs]",
		"number": "arXiv:2303.18240",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?",
		"URL": "http://arxiv.org/abs/2303.18240",
		"author": [
			{
				"family": "Majumdar",
				"given": "Arjun"
			},
			{
				"family": "Yadav",
				"given": "Karmesh"
			},
			{
				"family": "Arnaud",
				"given": "Sergio"
			},
			{
				"family": "Ma",
				"given": "Yecheng Jason"
			},
			{
				"family": "Chen",
				"given": "Claire"
			},
			{
				"family": "Silwal",
				"given": "Sneha"
			},
			{
				"family": "Jain",
				"given": "Aryan"
			},
			{
				"family": "Berges",
				"given": "Vincent-Pierre"
			},
			{
				"family": "Abbeel",
				"given": "Pieter"
			},
			{
				"family": "Malik",
				"given": "Jitendra"
			},
			{
				"family": "Batra",
				"given": "Dhruv"
			},
			{
				"family": "Lin",
				"given": "Yixin"
			},
			{
				"family": "Maksymets",
				"given": "Oleksandr"
			},
			{
				"family": "Rajeswaran",
				"given": "Aravind"
			},
			{
				"family": "Meier",
				"given": "Franziska"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					3,
					31
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JKJFTUW8",
		"type": "article-journal",
		"container-title": "Daedalus",
		"issue": "2",
		"note": "publisher: MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info …",
		"page": "85–99",
		"source": "Google Scholar",
		"title": "Searching for computer vision north stars",
		"URL": "https://direct.mit.edu/daed/article-abstract/151/2/85/110602",
		"volume": "151",
		"author": [
			{
				"family": "Fei-Fei",
				"given": "Li"
			},
			{
				"family": "Krishna",
				"given": "Ranjay"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YWR5J2FS",
		"type": "article",
		"abstract": "We present a model which predicts how individual users of a dialog system understand and produce utterances based on user groups. In contrast to previous work, these user groups are not specified beforehand, but learned in training. We evaluate on two referring expression (RE) generation tasks; our experiments show that our model can identify user groups and learn how to most effectively talk to them, and can dynamically assign unseen users to the correct groups as they interact with the system.",
		"note": "arXiv:1806.05947 [cs]",
		"number": "arXiv:1806.05947",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Discovering User Groups for Natural Language Generation",
		"URL": "http://arxiv.org/abs/1806.05947",
		"author": [
			{
				"family": "Engonopoulos",
				"given": "Nikos"
			},
			{
				"family": "Teichmann",
				"given": "Christoph"
			},
			{
				"family": "Koller",
				"given": "Alexander"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					15
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3HN9VGL5",
		"type": "chapter",
		"container-title": "Stepping into Virtual Reality",
		"event-place": "Cham",
		"ISBN": "978-3-031-36486-0",
		"language": "en",
		"note": "DOI: 10.1007/978-3-031-36487-7_4",
		"page": "81-124",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"source": "DOI.org (Crossref)",
		"title": "Virtual Characters",
		"URL": "https://link.springer.com/10.1007/978-3-031-36487-7_4",
		"container-author": [
			{
				"family": "Gutiérrez A.",
				"given": "Mario A."
			},
			{
				"family": "Vexo",
				"given": "Frédéric"
			},
			{
				"family": "Thalmann",
				"given": "Daniel"
			}
		],
		"author": [
			{
				"family": "Gutiérrez A.",
				"given": "Mario A."
			},
			{
				"family": "Vexo",
				"given": "Frédéric"
			},
			{
				"family": "Thalmann",
				"given": "Daniel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6WGKBHQU",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Northeastern University",
		"source": "Google Scholar",
		"title": "Towards socially interactive agents: Learning generative models of social interactions via crowdsourcing",
		"title-short": "Towards socially interactive agents",
		"URL": "https://search.proquest.com/openview/23afcbc247e46905ca8a696c02d4b437/1?pq-origsite=gscholar&cbl=44156",
		"author": [
			{
				"family": "Feng",
				"given": "Dan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A2L9H73H",
		"type": "book",
		"publisher": "Murat Durmus",
		"source": "Google Scholar",
		"title": "A Primer to the 42 Most commonly used Machine Learning Algorithms (With Code Samples)",
		"URL": "https://books.google.com/books?hl=en&lr=&id=VemrEAAAQBAJ&oi=fnd&pg=PT9&dq=(%22Reinforcement+Learning%22+OR+%22Deep+Reinforcement+Learning%22)+AND+((Instruction+OR+Prompt)+AND+(follow+OR+following))+AND+%22Natural+Language%22+AND+%223D+Environment%22&ots=o1OV1F_os3&sig=N5CfDmAcvjvW170qYZQzjzgsWtk",
		"author": [
			{
				"family": "Durmus",
				"given": "Murat"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5A8EXWMJ",
		"type": "article-journal",
		"container-title": "Journal of Sensor and Actuator Networks",
		"issue": "3",
		"note": "publisher: MDPI",
		"page": "53",
		"source": "Google Scholar",
		"title": "A study on sensor system latency in vr motion sickness",
		"URL": "https://www.mdpi.com/2224-2708/10/3/53",
		"volume": "10",
		"author": [
			{
				"family": "Kundu",
				"given": "Ripan Kumar"
			},
			{
				"family": "Rahman",
				"given": "Akhlaqur"
			},
			{
				"family": "Paul",
				"given": "Shuva"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/U3KIP9VJ",
		"type": "paper-conference",
		"container-title": "Position Papers for the ASCR Workshop on Visualization for Scientific Discovery, Decision-Making, and Communication",
		"source": "Google Scholar",
		"title": "Visualization of scientific data lossy reduction error",
		"URL": "https://www.osti.gov/servlets/purl/1843572#page=67",
		"author": [
			{
				"family": "Cappello",
				"given": "Franck"
			},
			{
				"family": "Sheng Di ANL",
				"given": "Pascal Grosset"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EG4K3HQ8",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "University of Maryland, College Park",
		"source": "Google Scholar",
		"title": "A Neurocomputational Model of Causal Reasoning and Compositional Working Memory for Imitation Learning",
		"URL": "https://search.proquest.com/openview/b71cc6a7e555e23345ffa533fdec0338/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Davis",
				"given": "Gregory Patrick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HGWEJH6U",
		"type": "article",
		"abstract": "We propose a simple but effective modular approach MOPA (Modular ObjectNav with PointGoal agents) to systematically investigate the inherent modularity of the object navigation task in Embodied AI. MOPA consists of four modules: (a) an object detection module trained to identify objects from RGB images, (b) a map building module to build a semantic map of the observed objects, (c) an exploration module enabling the agent to explore the environment, and (d) a navigation module to move to identified target objects. We show that we can effectively reuse a pretrained PointGoal agent as the navigation model instead of learning to navigate from scratch, thus saving time and compute. We also compare various exploration strategies for MOPA and find that a simple uniform strategy significantly outperforms more advanced exploration methods.",
		"note": "arXiv:2304.03696 [cs]",
		"number": "arXiv:2304.03696",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "MOPA: Modular Object Navigation with PointGoal Agents",
		"title-short": "MOPA",
		"URL": "http://arxiv.org/abs/2304.03696",
		"author": [
			{
				"family": "Raychaudhuri",
				"given": "Sonia"
			},
			{
				"family": "Campari",
				"given": "Tommaso"
			},
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Savva",
				"given": "Manolis"
			},
			{
				"family": "Chang",
				"given": "Angel X."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					9,
					14
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DK3Y8IVX",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Purdue University",
		"source": "Google Scholar",
		"title": "Interactive and Intelligent Camera View Composing",
		"URL": "https://search.proquest.com/openview/5cf218d9cab7b8ff15353e8b52a1fef3/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Kang",
				"given": "Hao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6W22LKUM",
		"type": "paper-conference",
		"container-title": "2023 IEEE International Conference on Mechatronics and Automation (ICMA)",
		"page": "2171–2176",
		"publisher": "IEEE",
		"source": "Google Scholar",
		"title": "A Survey of Object Goal Navigation: Datasets, Metrics and Methods",
		"title-short": "A Survey of Object Goal Navigation",
		"URL": "https://ieeexplore.ieee.org/abstract/document/10215817/",
		"author": [
			{
				"family": "Wang",
				"given": "Dewei"
			},
			{
				"family": "Chen",
				"given": "Jiaming"
			},
			{
				"family": "Cheng",
				"given": "Jiyu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FCDC4LYJ",
		"type": "book",
		"publisher": "University of California, Berkeley",
		"source": "Google Scholar",
		"title": "Computational sensorimotor learning",
		"URL": "https://search.proquest.com/openview/3e268ecf6213c7dde3856604158ce41c/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Agrawal",
				"given": "Pulkit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FJ5BIGQ8",
		"type": "article",
		"abstract": "This paper describes an implementation of a bot assistant in Minecraft, and the tools and platform allowing players to interact with the bot and to record those interactions. The purpose of building such an assistant is to facilitate the study of agents that can complete tasks specified by dialogue, and eventually, to learn from dialogue interactions.",
		"note": "arXiv:1907.08584 [cs]",
		"number": "arXiv:1907.08584",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "CraftAssist: A Framework for Dialogue-enabled Interactive Agents",
		"title-short": "CraftAssist",
		"URL": "http://arxiv.org/abs/1907.08584",
		"author": [
			{
				"family": "Gray",
				"given": "Jonathan"
			},
			{
				"family": "Srinet",
				"given": "Kavya"
			},
			{
				"family": "Jernite",
				"given": "Yacine"
			},
			{
				"family": "Yu",
				"given": "Haonan"
			},
			{
				"family": "Chen",
				"given": "Zhuoyuan"
			},
			{
				"family": "Guo",
				"given": "Demi"
			},
			{
				"family": "Goyal",
				"given": "Siddharth"
			},
			{
				"family": "Zitnick",
				"given": "C. Lawrence"
			},
			{
				"family": "Szlam",
				"given": "Arthur"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/MDLNT9ZK",
		"type": "document",
		"source": "Google Scholar",
		"title": "Explaining rifle shooting factors through multi-sensor body tracking: Using transformers and attention to mine actionable patterns from skeleton graphs",
		"title-short": "Explaining rifle shooting factors through multi-sensor body tracking",
		"URL": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:1568577",
		"author": [
			{
				"family": "Andersson",
				"given": "Filip"
			},
			{
				"family": "Flyckt",
				"given": "Jonatan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/636LZW9I",
		"type": "book",
		"publisher": "Lamar University-Beaumont",
		"source": "Google Scholar",
		"title": "Brain tumor segmentation using deep learning technique",
		"URL": "https://search.proquest.com/openview/9dc3064fb24f708cdaefe7e2de41cfbf/1?pq-origsite=gscholar&cbl=18750",
		"author": [
			{
				"family": "Singh",
				"given": "Oyesh Mann"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XWPZVEMZ",
		"type": "article",
		"abstract": "Embodied AI is an inevitable trend that emphasizes the interaction between intelligent entities and the real world, with broad applications in Robotics, especially target-driven navigation. This task requires the robot to find an object of a certain category efficiently in an unknown domestic environment. Recent works focus on exploiting layout relationships by graph neural networks (GNNs). However, most of them obtain robot actions directly from observations in an end-to-end manner via an incomplete relation graph, which is not interpretable and reliable. We decouple this task and propose ReVoLT, a hierarchical framework: (a) an object detection visual front-end, (b) a high-level reasoner (infers semantic sub-goals), (c) an intermediate-level planner (computes geometrical positions), and (d) a low-level controller (executes actions). ReVoLT operates with a multi-layer semantic-spatial topological graph. The reasoner uses multiform structured relations as priors, which are obtained from combinatorial relation extraction networks composed of unsupervised GraphSAGE, GCN, and GraphRNN-based Region Rollout. The reasoner performs with Upper Confidence Bound for Tree (UCT) to infer semantic sub-goals, accounting for trade-offs between exploitation (depth-first searching) and exploration (regretting). The lightweight intermediate-level planner generates instantaneous spatial sub-goal locations via an online constructed Voronoi local graph. The simulation experiments demonstrate that our framework achieves better performance in the target-driven navigation tasks and generalizes well, which has an 80% improvement compared to the existing state-of-the-art method. The code and result video will be released at https://ventusff.github.io/ReVoLT-website/.",
		"note": "arXiv:2301.02382 [cs]",
		"number": "arXiv:2301.02382",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "ReVoLT: Relational Reasoning and Voronoi Local Graph Planning for Target-driven Navigation",
		"title-short": "ReVoLT",
		"URL": "http://arxiv.org/abs/2301.02382",
		"author": [
			{
				"family": "Liu",
				"given": "Junjia"
			},
			{
				"family": "Guo",
				"given": "Jianfei"
			},
			{
				"family": "Meng",
				"given": "Zehui"
			},
			{
				"family": "Xue",
				"given": "Jingtao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					1,
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XAX6XGB6",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Metaverse for Healthcare: A Survey on Potential Applications, Challenges and Future Directions",
		"title-short": "Metaverse for Healthcare",
		"URL": "https://metaversemedicorum.org/wp-content/uploads/2022/12/2209.04160.pdf",
		"author": [
			{
				"family": "YENDURI",
				"given": "GOKUL"
			},
			{
				"family": "JHAVERI",
				"given": "RUTVIJ H."
			},
			{
				"family": "ALAZAB",
				"given": "MAMOUN"
			},
			{
				"family": "MADDIKUNTA",
				"given": "REDDY"
			},
			{
				"family": "GADEKALLU",
				"given": "THIPPA REDDY"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DVIILDJW",
		"type": "book",
		"publisher": "Taylor & Francis",
		"source": "Google Scholar",
		"title": "Artificial intelligence and playable media",
		"URL": "https://books.google.com/books?hl=en&lr=&id=7HR5EAAAQBAJ&oi=fnd&pg=PT6&dq=(%22Reinforcement+Learning%22+OR+%22Deep+Reinforcement+Learning%22)+AND+((Instruction+OR+Prompt)+AND+(follow+OR+following))+AND+%22Natural+Language%22+AND+%223D+Environment%22&ots=2dPPh5fo7k&sig=RKBqCbzqa3WJLAXlcpDfaU_iRd4",
		"author": [
			{
				"family": "Freedman",
				"given": "Eric"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CJHUJZ26",
		"type": "article-journal",
		"container-title": "IEEE access",
		"note": "publisher: IEEE",
		"page": "133995–134030",
		"source": "Google Scholar",
		"title": "6G and beyond: The future of wireless communications systems",
		"title-short": "6G and beyond",
		"URL": "https://ieeexplore.ieee.org/abstract/document/9145564.",
		"volume": "8",
		"author": [
			{
				"family": "Akyildiz",
				"given": "Ian F."
			},
			{
				"family": "Kak",
				"given": "Ahan"
			},
			{
				"family": "Nie",
				"given": "Shuai"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IFUX84HN",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Strategic Technical Insights",
		"URL": "https://cradpdf.drdc-rddc.gc.ca/PDFS/unc269/p805244_A1b.pdf",
		"author": [
			{
				"family": "Auger",
				"given": "Alain"
			},
			{
				"family": "Wiseman",
				"given": "Erica"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/H4BWM6RS",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Vision-based Perception for Dexterous Hand-arm Teleoperation",
		"URL": "https://d-nb.info/125635080X/34",
		"author": [
			{
				"family": "Teleoperation",
				"given": "Dexterous Hand-arm"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A2TMUL89",
		"type": "article",
		"abstract": "Multi-agent embodied tasks have recently been studied in complex indoor visual environments. Collaboration among multiple agents can improve work efficiency and has significant practical value. However, most of the existing research focuses on homogeneous multi-agent tasks. Compared with homogeneous agents, heterogeneous agents can leverage their different capabilities to allocate corresponding sub-tasks and cooperate to complete complex tasks. Heterogeneous multi-agent tasks are common in real-world scenarios, and the collaboration strategy among heterogeneous agents is a challenging and important problem to be solved. To study collaboration among heterogeneous agents, we propose the heterogeneous multi-agent tidying-up task, in which multiple heterogeneous agents with different capabilities collaborate with each other to detect misplaced objects and place them in reasonable locations. This is a demanding task since it requires agents to make the best use of their different capabilities to conduct reasonable task planning and complete the whole task. To solve this task, we build a heterogeneous multi-agent tidying-up benchmark dataset in a large number of houses with multiple rooms based on ProcTHOR-10K. We propose the hierarchical decision model based on misplaced object detection, reasonable receptacle prediction, as well as the handshake-based group communication mechanism. Extensive experiments are conducted to demonstrate the effectiveness of the proposed model. The project's website and videos of experiments can be found at https://hetercol.github.io/.",
		"note": "arXiv:2307.13957 [cs]",
		"number": "arXiv:2307.13957",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Heterogeneous Embodied Multi-Agent Collaboration",
		"URL": "http://arxiv.org/abs/2307.13957",
		"author": [
			{
				"family": "Liu",
				"given": "Xinzhu"
			},
			{
				"family": "Guo",
				"given": "Di"
			},
			{
				"family": "Liu",
				"given": "Huaping"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					26
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YAXDESSY",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "University of Saskatchewan",
		"source": "Google Scholar",
		"title": "How simulation can illuminate pedagogical and system design issues in dynamic open ended learning environments",
		"URL": "https://harvest.usask.ca/handle/10388/7763",
		"author": [
			{
				"family": "Frost",
				"given": "Stephanie Lee"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/BX9Z7MZA",
		"type": "thesis",
		"genre": "Master's Thesis",
		"publisher": "Nord universitet",
		"source": "Google Scholar",
		"title": "The impact of machine learning on the Norwegian legal industry",
		"URL": "https://nordopen.nord.no/nord-xmlui/bitstream/handle/11250/2679364/Caffrey.pdf?sequence=1",
		"author": [
			{
				"family": "Caffrey",
				"given": "Anthony Christopher"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3Z8ZLYE2",
		"type": "book",
		"publisher": "University of California, Davis",
		"source": "Google Scholar",
		"title": "Evaluating and Enhancing Embodied, Avatar-Mediated Interaction",
		"URL": "https://search.proquest.com/openview/f5e1b08214b415d9041a7908c1f1dac9/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Smith",
				"given": "Harrison Jesse Averett"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3VQ8U4ZL",
		"type": "article",
		"abstract": "With the advent of new technologies and endeavors for automation in almost all day-to-day activities, the recent discussions on the metaverse life have a greater expectation. Furthermore, we are in the era of the fifth industrial revolution, where machines and humans collaborate to maximize productivity with the effective utilization of human intelligence and other resources. Hence, Industry 5.0 in the metaverse may have tremendous technological integration for a more immersive experience and enhanced communication.These technological amalgamations are suitable for the present environment and entirely different from the previous perception of virtual technologies. This work presents a comprehensive review of the applications of the metaverse in Industry 5.0 (so-called industrial metaverse). In particular, we first provide a preliminary to the metaverse and industry 5.0 and discuss key enabling technologies of the industrial metaverse, including virtual and augmented reality, 3D modeling, artificial intelligence, edge computing, digital twin, blockchain, and 6G communication networks. This work then explores diverse metaverse applications in Industry 5.0 vertical domains like Society 5.0, agriculture, supply chain management, healthcare, education, and transportation. A number of research projects are presented to showcase the conceptualization and implementation of the industrial metaverse. Furthermore, various challenges in realizing the industrial metaverse, feasible solutions, and future directions for further research have been presented.",
		"note": "arXiv:2308.02677 [cs]",
		"number": "arXiv:2308.02677",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Metaverse for Industry 5.0 in NextG Communications: Potential Applications and Future Challenges",
		"title-short": "Metaverse for Industry 5.0 in NextG Communications",
		"URL": "http://arxiv.org/abs/2308.02677",
		"author": [
			{
				"family": "Prabadevi",
				"given": "B."
			},
			{
				"family": "Deepa",
				"given": "N."
			},
			{
				"family": "Victor",
				"given": "Nancy"
			},
			{
				"family": "Gadekallu",
				"given": "Thippa Reddy"
			},
			{
				"family": "Maddikunta",
				"given": "Praveen Kumar Reddy"
			},
			{
				"family": "Yenduri",
				"given": "Gokul"
			},
			{
				"family": "Wang",
				"given": "Wei"
			},
			{
				"family": "Pham",
				"given": "Quoc Viet"
			},
			{
				"family": "Huynh-The",
				"given": "Thien"
			},
			{
				"family": "Liyanage",
				"given": "Madhusanka"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					7,
					31
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FMRJACCU",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Iowa State University",
		"source": "Google Scholar",
		"title": "Applying FastMDP to complex aerospace-related problems",
		"URL": "https://search.proquest.com/openview/1641daae3c7c25fcd917339fe45f05cd/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Bertram",
				"given": "Joshua"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DIJ6QV78",
		"type": "article-journal",
		"note": "publisher: University of the Arts London",
		"source": "Google Scholar",
		"title": "Investigating attitudes of professional writers to GPT text generation AI based creative support tools",
		"URL": "https://ualresearchonline.arts.ac.uk/id/eprint/18621/1/Investigating%20attitudes%20of%20writers%20GPT%20text%20generation%20based%20creative%20support%20tools%20G%20Davis%20M%20Grierson%202021-22%20%281%29.pdf",
		"author": [
			{
				"family": "Davis",
				"given": "Geoff"
			},
			{
				"family": "Grierson",
				"given": "Mick"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EQLFRX68",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Enhanced vision-language navigation by using scene recognition auxiliary task",
		"URL": "https://repositorio.uc.cl/xmlui/handle/11534/51219",
		"author": [
			{
				"family": "Manterola Valenzuela",
				"given": "Raimundo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/UT6AN5GW",
		"type": "book",
		"publisher": "CRC Press",
		"source": "Google Scholar",
		"title": "The Metaweb: The Next Level of the Internet",
		"title-short": "The Metaweb",
		"URL": "https://books.google.com/books?hl=en&lr=&id=MKPREAAAQBAJ&oi=fnd&pg=PT10&dq=(%22Reinforcement+Learning%22+OR+%22Deep+Reinforcement+Learning%22)+AND+((Instruction+OR+Prompt)+AND+(follow+OR+following))+AND+%22Natural+Language%22+AND+%223D+Environment%22&ots=juIimBEb8K&sig=_m8QlmaPgl2tzRoe4ViQuIFuj7I",
		"author": [
			{
				"family": "Bridgit",
				"given": "D. A. O."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/LQI3UF6M",
		"type": "book",
		"publisher": "Springer",
		"source": "Google Scholar",
		"title": "Universal Access in Human-Computer Interaction. Multimodality and Assistive Environments: 13th International Conference, UAHCI 2019, Held as Part of the 21st HCI International Conference, HCII 2019, Orlando, FL, USA, July 26–31, 2019, Proceedings, Part II",
		"title-short": "Universal Access in Human-Computer Interaction. Multimodality and Assistive Environments",
		"URL": "https://books.google.com/books?hl=en&lr=&id=5KKhDwAAQBAJ&oi=fnd&pg=PP6&dq=(%22Reinforcement+Learning%22+OR+%22Deep+Reinforcement+Learning%22)+AND+((Instruction+OR+Prompt)+AND+(follow+OR+following))+AND+%22Natural+Language%22+AND+%223D+Environment%22&ots=9QsErfI6Mo&sig=qektiUXsIb-MgPiIwVKC9Vkif9k",
		"volume": "11573",
		"author": [
			{
				"family": "Antona",
				"given": "Margherita"
			},
			{
				"family": "Stephanidis",
				"given": "Constantine"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/V7X5I2NV",
		"type": "book",
		"publisher": "RAND Corporation",
		"source": "Google Scholar",
		"title": "Evaluating the effectiveness of artificial intelligence systems in intelligence analysis",
		"URL": "https://apps.dtic.mil/sti/trecms/pdf/AD1146077.pdf",
		"author": [
			{
				"family": "Ish",
				"given": "Daniel"
			},
			{
				"family": "Ettinger",
				"given": "Jared"
			},
			{
				"family": "Ferris",
				"given": "Christopher"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/52G57NIV",
		"type": "document",
		"source": "Google Scholar",
		"title": "Sustainable automated transportation systems directing towards smart cities: A feasibility study of droid delivery in Stockholm",
		"title-short": "Sustainable automated transportation systems directing towards smart cities",
		"URL": "https://www.diva-portal.org/smash/record.jsf?pid=diva2:1591183",
		"author": [
			{
				"family": "Movaheddin",
				"given": "Armin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CXYA3XLH",
		"type": "chapter",
		"container-title": "AI and Its Convergence With Communication Technologies",
		"page": "181–211",
		"publisher": "IGI Global",
		"source": "Google Scholar",
		"title": "Impact of Artificial Intelligence and Communication Tools in Veterinary and Medical Sciences: AI in Health Sciences",
		"title-short": "Impact of Artificial Intelligence and Communication Tools in Veterinary and Medical Sciences",
		"URL": "https://www.igi-global.com/chapter/impact-of-artificial-intelligence-and-communication-tools-in-veterinary-and-medical-sciences/329272",
		"author": [
			{
				"family": "Saeed",
				"given": "Muhammad Rizwan"
			},
			{
				"family": "Abdullah",
				"given": "Muhammad"
			},
			{
				"family": "Zoraiz",
				"given": "Muhammad"
			},
			{
				"family": "Ahmad",
				"given": "Waqas"
			},
			{
				"family": "Naeem",
				"given": "Muhammad Ahsan"
			},
			{
				"family": "Akram",
				"given": "Qaiser"
			},
			{
				"family": "Younus",
				"given": "Muhammad"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/TQ53H9ZJ",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "The University of North Carolina at Chapel Hill",
		"source": "Google Scholar",
		"title": "Multimodal and Embodied Learning with Language as the Anchor",
		"URL": "https://search.proquest.com/openview/3e03db05577e757c60452285a1a0a0b2/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Kim",
				"given": "Hyounghun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CPGCXTFW",
		"type": "article-journal",
		"container-title": "Frontiers in Future Transportation",
		"note": "publisher: Frontiers Media SA",
		"page": "688482",
		"source": "Google Scholar",
		"title": "Automotive intelligence embedded in electric connected autonomous and shared vehicles technology for sustainable green mobility",
		"URL": "https://www.frontiersin.org/articles/10.3389/ffutr.2021.688482/full",
		"volume": "2",
		"author": [
			{
				"family": "Vermesan",
				"given": "Ovidiu"
			},
			{
				"family": "John",
				"given": "Reiner"
			},
			{
				"family": "Pype",
				"given": "Patrick"
			},
			{
				"family": "Daalderop",
				"given": "Gerardo"
			},
			{
				"family": "Kriegel",
				"given": "Kai"
			},
			{
				"family": "Mitic",
				"given": "Gerhard"
			},
			{
				"family": "Lorentz",
				"given": "Vincent"
			},
			{
				"family": "Bahr",
				"given": "Roy"
			},
			{
				"family": "Sand",
				"given": "Hans Erik"
			},
			{
				"family": "Bockrath",
				"given": "Steffen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SJSRL833",
		"type": "article-journal",
		"container-title": "Applied Sciences",
		"issue": "19",
		"note": "publisher: MDPI",
		"page": "10667",
		"source": "Google Scholar",
		"title": "Ten Years of Active Learning Techniques and Object Detection: A Systematic Review",
		"title-short": "Ten Years of Active Learning Techniques and Object Detection",
		"URL": "https://www.mdpi.com/2076-3417/13/19/10667",
		"volume": "13",
		"author": [
			{
				"family": "Garcia",
				"given": "Dibet"
			},
			{
				"family": "Carias",
				"given": "João"
			},
			{
				"family": "Adão",
				"given": "Telmo"
			},
			{
				"family": "Jesus",
				"given": "Rui"
			},
			{
				"family": "Cunha",
				"given": "Antonio"
			},
			{
				"family": "Magalhães",
				"given": "Luis G."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/36HV2VDE",
		"type": "document",
		"publisher": "Springer Nature",
		"source": "Google Scholar",
		"title": "Computer Aided Verification: 35th International Conference, CAV 2023, Paris, France, July 17–22, 2023, Proceedings, Part I",
		"title-short": "Computer Aided Verification",
		"URL": "https://library.oapen.org/handle/20.500.12657/75412",
		"author": [
			{
				"family": "Enea",
				"given": "Constantin"
			},
			{
				"family": "Lal",
				"given": "Akash"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WJR4BFJ7",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "The University of North Carolina at Chapel Hill",
		"source": "Google Scholar",
		"title": "Real-Time Robot Motion Planning Algorithms and Applications Under Uncertainty",
		"URL": "https://search.proquest.com/openview/12b82ae3e83ee79376fd9f8233cb4979/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Park",
				"given": "Jae Sung"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9Q34JWQH",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "University of Maryland, College Park",
		"source": "Google Scholar",
		"title": "Enriching Communication Between Humans and AI Agents",
		"URL": "https://search.proquest.com/openview/109513bcc89ce8bb3600ff68c61e3f3c/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Nguyen",
				"given": "Khanh"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JGRV5P7B",
		"type": "chapter",
		"container-title": "Handbook of Evolutionary Machine Learning",
		"event-place": "Singapore",
		"ISBN": "978-981-9938-13-1",
		"language": "en",
		"note": "collection-title: Genetic and Evolutionary Computation\nDOI: 10.1007/978-981-99-3814-8_25",
		"page": "715-737",
		"publisher": "Springer Nature Singapore",
		"publisher-place": "Singapore",
		"source": "DOI.org (Crossref)",
		"title": "Evolutionary Machine Learning and Games",
		"URL": "https://link.springer.com/10.1007/978-981-99-3814-8_25",
		"editor": [
			{
				"family": "Banzhaf",
				"given": "Wolfgang"
			},
			{
				"family": "Machado",
				"given": "Penousal"
			},
			{
				"family": "Zhang",
				"given": "Mengjie"
			}
		],
		"author": [
			{
				"family": "Togelius",
				"given": "Julian"
			},
			{
				"family": "Khalifa",
				"given": "Ahmed"
			},
			{
				"family": "Earle",
				"given": "Sam"
			},
			{
				"family": "Green",
				"given": "Michael Cerny"
			},
			{
				"family": "Soros",
				"given": "Lisa"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2024"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/MURM3MWR",
		"type": "book",
		"publisher": "Royal College of Art (United Kingdom)",
		"source": "Google Scholar",
		"title": "Cinematic assemblage: Sinofuturist worldbuilding and the smart city",
		"title-short": "Cinematic assemblage",
		"URL": "https://search.proquest.com/openview/31005a204f44e5d4d121c6755df92742/1?pq-origsite=gscholar&cbl=2026366&diss=y",
		"author": [
			{
				"family": "Lek",
				"given": "Lawrence"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IRQSHJ35",
		"type": "article",
		"abstract": "Space exploration has been on the rise since the 1960s. Along with the other planets such as Mercury, Venus, Saturn, and Jupiter, Mars certainly plays a significant role in the history of space exploration and has the potential to be the first extraterrestrial planet to host human life. In this context, tremendous effort has been put into developing new technologies to photograph, measure, and analyze the red planet. As the amount of data collected from science instruments around and on Mars increased, the need for fast and reliable communication between Earth and space probes has emerged. However, communicating over deep space has always been a big challenge due to the propagation characteristics of radio waves. Nowadays, the collaboration of private companies like SpaceX with space agencies to make Mars colonization a reality, introduces even more challenges, such as providing high data rate, low latency, energy-efficient, reliable, and mobility-resistant communication infrastructures in the Martian environment. Propagation medium and wireless channel characteristics of Mars should be extensively studied to achieve these goals. This survey article presents a comprehensive overview of the Mars missions and channel modeling studies of the near-Earth, interstellar, and near-planet links. Studies featuring three-dimensional (3D) channel modeling simulations on the Martian surface are also reviewed. We have also presented our own computer simulations considering various scenarios based on realistic 3D Martian terrains using the Wireless Insite software. Path loss exponent, power delay profile, and root-mean-square delay spread for these scenarios are calculated and tabularized in this study. Furthermore, future insights on emerging communication technologies for Mars are given.",
		"note": "arXiv:2211.14245 [astro-ph]",
		"number": "arXiv:2211.14245",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Communications for the Planet Mars: Past, Present, and Future",
		"title-short": "Communications for the Planet Mars",
		"URL": "http://arxiv.org/abs/2211.14245",
		"author": [
			{
				"family": "Koktas",
				"given": "Enes"
			},
			{
				"family": "Basar",
				"given": "Ertugrul"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					25
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AKIL7YM9",
		"type": "article",
		"abstract": "Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive \"indicator properties\" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.",
		"note": "arXiv:2308.08708 [cs, q-bio]",
		"number": "arXiv:2308.08708",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "Consciousness in Artificial Intelligence: Insights from the Science of Consciousness",
		"title-short": "Consciousness in Artificial Intelligence",
		"URL": "http://arxiv.org/abs/2308.08708",
		"author": [
			{
				"family": "Butlin",
				"given": "Patrick"
			},
			{
				"family": "Long",
				"given": "Robert"
			},
			{
				"family": "Elmoznino",
				"given": "Eric"
			},
			{
				"family": "Bengio",
				"given": "Yoshua"
			},
			{
				"family": "Birch",
				"given": "Jonathan"
			},
			{
				"family": "Constant",
				"given": "Axel"
			},
			{
				"family": "Deane",
				"given": "George"
			},
			{
				"family": "Fleming",
				"given": "Stephen M."
			},
			{
				"family": "Frith",
				"given": "Chris"
			},
			{
				"family": "Ji",
				"given": "Xu"
			},
			{
				"family": "Kanai",
				"given": "Ryota"
			},
			{
				"family": "Klein",
				"given": "Colin"
			},
			{
				"family": "Lindsay",
				"given": "Grace"
			},
			{
				"family": "Michel",
				"given": "Matthias"
			},
			{
				"family": "Mudrik",
				"given": "Liad"
			},
			{
				"family": "Peters",
				"given": "Megan A. K."
			},
			{
				"family": "Schwitzgebel",
				"given": "Eric"
			},
			{
				"family": "Simon",
				"given": "Jonathan"
			},
			{
				"family": "VanRullen",
				"given": "Rufin"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					8,
					22
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/4SJ7X8PX",
		"type": "article",
		"abstract": "This survey includes systematic generalization and a history of how machine learning addresses it. We aim to summarize and organize the related information of both conventional and recent improvements. We first look at the definition of systematic generalization, then introduce Classicist and Connectionist. We then discuss different types of Connectionists and how they approach the generalization. Two crucial problems of variable binding and causality are discussed. We look into systematic generalization in language, vision, and VQA fields. Recent improvements from different aspects are discussed. Systematic generalization has a long history in artificial intelligence. We could cover only a small portion of many contributions. We hope this paper provides a background and is beneficial for discoveries in future work.",
		"note": "arXiv:2211.11956 [cs]",
		"number": "arXiv:2211.11956",
		"publisher": "arXiv",
		"source": "arXiv.org",
		"title": "A Short Survey of Systematic Generalization",
		"URL": "http://arxiv.org/abs/2211.11956",
		"author": [
			{
				"family": "Li",
				"given": "Yuanpeng"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					11,
					21
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EI3J6FA6",
		"type": "book",
		"publisher": "Stanford University",
		"source": "Google Scholar",
		"title": "Compositional Reasoning in Robot Learning",
		"URL": "https://search.proquest.com/openview/5ee8fe3656f0d9079d48bf1d8ebb7a09/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Xu",
				"given": "Danfei"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XEWS5F2T",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "University of Pennsylvania",
		"source": "Google Scholar",
		"title": "COTTAGE: Coherent Text Adventure Games Generation",
		"title-short": "COTTAGE",
		"URL": "https://callison-burch.github.io/publications/masters-theses/River-Yijiang-Dong-masters-thesis-2023.pdf",
		"author": [
			{
				"family": "Dong",
				"given": "Yijiang River"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HX4AWPEC",
		"type": "chapter",
		"container-title": "The Development of Deep Learning Technologies",
		"event-place": "Singapore",
		"ISBN": "9789811545832",
		"language": "en",
		"note": "DOI: 10.1007/978-981-15-4584-9_2",
		"page": "13-45",
		"publisher": "Springer Singapore",
		"publisher-place": "Singapore",
		"source": "DOI.org (Crossref)",
		"title": "Deep Learning Development Status in China",
		"URL": "http://link.springer.com/10.1007/978-981-15-4584-9_2",
		"container-author": [
			{
				"literal": "Chinese Academy of Engineering"
			}
		],
		"author": [
			{
				"literal": "Center for Electronics and Information Studies, Chinese Academy of Engineering"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YP7AETCZ",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Schedule Highlights",
		"URL": "https://edwardjchou.github.io/files/2018-earthquake-nips.pdf",
		"author": [
			{
				"family": "Garcia",
				"given": "Kloumann"
			},
			{
				"family": "Lattimore",
				"given": "Mullainathan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2A5PGFBK",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "The University of Iowa",
		"source": "Google Scholar",
		"title": "Knowledge generation and communication in intelligent and immersive systems: a case study on flooding",
		"title-short": "Knowledge generation and communication in intelligent and immersive systems",
		"URL": "https://search.proquest.com/openview/35ae00933b7565867df9664bf5e0f042/1?pq-origsite=gscholar&cbl=51922&diss=y",
		"author": [
			{
				"family": "Sermet",
				"given": "Yusuf"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QELB9Z3N",
		"type": "article-journal",
		"container-title": "Frontiers in Artificial Intelligence",
		"note": "publisher: Frontiers",
		"page": "240",
		"source": "Google Scholar",
		"title": "Cutting-edge communication and learning assistive technologies for disabled children: An artificial intelligence perspective",
		"title-short": "Cutting-edge communication and learning assistive technologies for disabled children",
		"URL": "https://www.frontiersin.org/articles/10.3389/frai.2022.970430/full?&utm_source=Email_to_authors_&utm_medium=Email&utm_content=T1_11.5e1_author&utm_campaign=Email_publication&field=&journalName=Frontiers_in_Artificial_Intelligence&id=970430",
		"author": [
			{
				"family": "Zdravkova",
				"given": "Katerina"
			},
			{
				"family": "Krasniqi",
				"given": "Venera"
			},
			{
				"family": "Dalipi",
				"given": "Fisnik"
			},
			{
				"family": "Ferati",
				"given": "Mexhid"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/R36BEMP3",
		"type": "book",
		"publisher": "University of Washington",
		"source": "Google Scholar",
		"title": "Intelligence Through the Lens of Interaction",
		"URL": "https://search.proquest.com/openview/5bffeed8c8fd14a00c73fa82d0c314fa/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Ehsani",
				"given": "Kiana"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/SD9PCZNK",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Next Generation Virtual Worlds",
		"URL": "https://www.vdc-fellbach.de/fileadmin/user_upload/2023-06_-_Next_Generation_Virtual_Worlds_-_EU-Kommission.pdf",
		"author": [
			{
				"family": "Torres",
				"given": "I. Hupont"
			},
			{
				"family": "Charisi",
				"given": "V."
			},
			{
				"family": "De Prato",
				"given": "G."
			},
			{
				"family": "Pogorzelska",
				"given": "K."
			},
			{
				"family": "Schade",
				"given": "S."
			},
			{
				"family": "Kotsev",
				"given": "A."
			},
			{
				"family": "Sobolewski",
				"given": "M."
			},
			{
				"family": "Brown",
				"given": "N. Duch"
			},
			{
				"family": "Calza",
				"given": "E."
			},
			{
				"family": "Dunker",
				"given": "C."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WBZCAQW4",
		"type": "book",
		"publisher": "Liverpool John Moores University (United Kingdom)",
		"source": "Google Scholar",
		"title": "Investigation into Game-Based Crisis Scenario Modelling and Simulation System",
		"URL": "https://search.proquest.com/openview/69fd49f6f40883e2fb6c6f56dc47df4b/1?pq-origsite=gscholar&cbl=2026366&diss=y",
		"author": [
			{
				"family": "Praiwattana",
				"given": "Pisit"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/IHH82AXE",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "The University of Arizona",
		"source": "Google Scholar",
		"title": "Achieiving Holistic Interoperability with Model-Based Systems Engineering",
		"URL": "https://search.proquest.com/openview/3600a579a8d4fa5964fc2c6df4d4f6a0/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Kirshner",
				"given": "Mitchell"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YUDZDLGN",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Staats-und Universitätsbibliothek Hamburg Carl von Ossietzky",
		"source": "Google Scholar",
		"title": "Vision-based Perception for Dexterous Hand-arm Teleoperation",
		"URL": "https://ediss.sub.uni-hamburg.de/bitstream/ediss/9564/1/main.pdf",
		"author": [
			{
				"family": "Li",
				"given": "Shuang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/UKIKI3FM",
		"type": "book",
		"publisher": "University of California, Davis",
		"source": "Google Scholar",
		"title": "Generating, Evaluating and Recognizing Embodied Communication",
		"URL": "https://search.proquest.com/openview/03ff656251592555084296ff52b32820/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Abdullah",
				"given": "Ahsan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KXH4RPPE",
		"type": "book",
		"publisher": "University of California, Santa Cruz",
		"source": "Google Scholar",
		"title": "Design Out Helplessness: AI Interventions for Game Inclusivity",
		"title-short": "Design Out Helplessness",
		"URL": "https://search.proquest.com/openview/8ac75b37e707086bb0e3d9bde9d73517/1?pq-origsite=gscholar&cbl=18750&diss=y",
		"author": [
			{
				"family": "Aytemiz",
				"given": "Batu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/56Y7RYMT",
		"type": "article-journal",
		"container-title": "人工知能学会第二種研究会資料",
		"issue": "AGI-016",
		"note": "publisher: 一般社団法人 人工知能学会",
		"page": "03",
		"source": "Google Scholar",
		"title": "Instruction Following におけるサブタスクへの分割と抽象化された行動の予測による長い行動系列への頑健性の向上",
		"URL": "https://www.jstage.jst.go.jp/article/jsaisigtwo/2020/AGI-016/2020_03/_article/-char/ja/",
		"volume": "2020",
		"author": [
			{
				"literal": "篠田一聡"
			},
			{
				"literal": "竹澤祐貴"
			},
			{
				"literal": "鈴木雅大"
			},
			{
				"literal": "岩澤有祐"
			},
			{
				"literal": "松尾豊"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CQEYV5X5",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Universität Linz",
		"source": "Google Scholar",
		"title": "BabyRUDDER-Learn grounded language through sample efficient Reinforcement Learning/submitted by Maximilian Grosser",
		"URL": "https://epub.jku.at/obvulihs/content/titleinfo/5816813",
		"author": [
			{
				"family": "Grosser",
				"given": "Maximilian"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/VGKHU8IC",
		"type": "article-journal",
		"source": "Google Scholar",
		"title": "Language Models for Task Execution via Graphical User Interfaces (グラフィカルユーザーインターフェースを介したタスク実行のための言語モデル)",
		"URL": "https://ir.soken.ac.jp/record/6589/files/%E7%94%B22364.pdf",
		"author": [
			{
				"literal": "壹岐太一"
			},
			{
				"literal": "イキタイチ"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2JIWTIJ4",
		"type": "thesis",
		"genre": "PhD Thesis",
		"publisher": "Université Grenoble Alpes",
		"source": "Google Scholar",
		"title": "Apprentissage d'agents multi-tâches sous une supervision minimale",
		"URL": "https://www.theses.fr/2023GRALM032",
		"author": [
			{
				"family": "Mezghani",
				"given": "Lina"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/45BIEWJB",
		"type": "thesis",
		"genre": "Master's Thesis",
		"publisher": "Πανεπıotaστ\\acute\\etaμıotao Πεıotaραıota\\acuteømegaς",
		"source": "Google Scholar",
		"title": "Συμπερıotaφoρ\\acute\\alpha ευφυ\\acuteømegaν πρακτóρømegaν χρησıotaμoπoıota\\acuteømegaντας ενıotaσχυτıotaκ\\acute\\eta μ\\acute\\alphaþetaηση μ\\acute\\varepsilonσømega τoυ εργαλε\\acuteıotaoυ ML-Agents της Unity",
		"URL": "https://dione.lib.unipi.gr/xmlui/handle/unipi/15031",
		"author": [
			{
				"family": "Δαβ\\acuteıotaλλας",
				"given": "Xρ\\acute\\etaστoς"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/F8YPEM75",
		"type": "article-journal",
		"container-title": "Application Research of Computers/Jisuanji Yingyong Yanjiu",
		"issue": "11",
		"source": "Google Scholar",
		"title": "视觉—语言—行为: 视觉语言融合研究综述.",
		"title-short": "视觉—语言—行为",
		"URL": "https://lironui.github.io/Files/%E8%A7%86%E8%A7%89%E8%AF%AD%E8%A8%80%E8%A1%8C%E4%B8%BA.pdf",
		"volume": "37",
		"author": [
			{
				"literal": "李睿"
			},
			{
				"literal": "郑顺义"
			},
			{
				"literal": "王西旗"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					19
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A557W2QV",
		"type": "article-journal",
		"abstract": "In this work, we address the problem of visual navigation by following instructions. In this task, the robot must interpret a natural language instruction in order to follow a predefined path in a possibly unknown environment. Despite different approaches have been proposed in the last years, they are all based on the assumption that the environment contains objects or other elements that can be used to formulate instructions, such as houses or offices. On the contrary, we focus on situations where the environment objects cannot be used to specify a navigation path. In particular, we consider 3D maze-like environments as our test bench because they can be very large and offer very intricate structures. We show that without reference points, visual navigation and instruction following can be rather challenging, and that standard approaches can not be applied successfully. For this reason, we propose a new architecture that explicitly learns both visual navigation and instruction understanding. We demonstrate with simulated experiments that our method can effectively follow instructions and navigate in previously unseen mazes of various sizes.",
		"container-title": "IEEE Robotics and Automation Letters",
		"DOI": "10.1109/LRA.2020.2965857",
		"ISSN": "2377-3766, 2377-3774",
		"issue": "2",
		"journalAbbreviation": "IEEE Robot. Autom. Lett.",
		"page": "1175-1182",
		"source": "Semantic Scholar",
		"title": "Deep Reinforcement Learning for Instruction Following Visual Navigation in 3D Maze-Like Environments",
		"URL": "https://ieeexplore.ieee.org/document/8957297/",
		"volume": "5",
		"author": [
			{
				"family": "Devo",
				"given": "Alessandro"
			},
			{
				"family": "Costante",
				"given": "Gabriele"
			},
			{
				"family": "Valigi",
				"given": "Paolo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					4
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RR2UADKK",
		"type": "article-journal",
		"abstract": "Language-guided Embodied AI benchmarks requiring an agent to navigate an environment and manipulate objects typically allow one-way communication: the human user gives a natural language command to the agent, and the agent can only follow the command passively. We present DialFRED, a dialogue-enabled embodied instruction following benchmark based on the ALFRED benchmark. DialFRED allows an agent to actively ask questions to the human user; the additional information in the user's response is used by the agent to better complete its task. We release a human-annotated dataset with 53 K task-relevant questions and answers and an oracle to answer questions. To tackle DialFRED, we propose a questioner-performer framework wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning. Experimental results show that asking the right questions leads to significantly improved task performance. We make DialFRED publicly available and encourage researchers to propose and evaluate their solutions to building dialog-enabled embodied agents: https://github.com/xfgao/DialFRED.",
		"container-title": "IEEE Robotics and Automation Letters",
		"DOI": "10.1109/LRA.2022.3193254",
		"ISSN": "2377-3766, 2377-3774",
		"issue": "4",
		"journalAbbreviation": "IEEE Robot. Autom. Lett.",
		"page": "10049-10056",
		"source": "Semantic Scholar",
		"title": "DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following",
		"title-short": "DialFRED",
		"URL": "https://ieeexplore.ieee.org/document/9837390/",
		"volume": "7",
		"author": [
			{
				"family": "Gao",
				"given": "Xiaofeng"
			},
			{
				"family": "Gao",
				"given": "Qiaozi"
			},
			{
				"family": "Gong",
				"given": "Ran"
			},
			{
				"family": "Lin",
				"given": "Kaixiang"
			},
			{
				"family": "Thattai",
				"given": "Govind"
			},
			{
				"family": "Sukhatme",
				"given": "Gaurav S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AZS7PT3B",
		"type": "article-journal",
		"abstract": "Recent years have seen embodied visual navigation advance in two distinct directions: (i) in equipping the AI agent to follow natural language instructions, and (ii) in making the navigable world multimodal, e.g., audio-visual navigation. However, the real world is not only multimodal, but also often complex, and thus in spite of these advances, agents still need to understand the uncertainty in their actions and seek instructions to navigate. To this end, we present AVLEN~ -- an interactive agent for Audio-Visual-Language Embodied Navigation. Similar to audio-visual navigation tasks, the goal of our embodied agent is to localize an audio event via navigating the 3D visual world; however, the agent may also seek help from a human (oracle), where the assistance is provided in free-form natural language. To realize these abilities, AVLEN uses a multimodal hierarchical reinforcement learning backbone that learns: (a) high-level policies to choose either audio-cues for navigation or to query the oracle, and (b) lower-level policies to select navigation actions based on its audio-visual and language inputs. The policies are trained via rewarding for the success on the navigation task while minimizing the number of queries to the oracle. To empirically evaluate AVLEN, we present experiments on the SoundSpaces framework for semantic audio-visual navigation tasks. Our results show that equipping the agent to ask for help leads to a clear improvement in performance, especially in challenging cases, e.g., when the sound is unheard during training or in the presence of distractor sounds.",
		"DOI": "10.48550/ARXIV.2210.07940",
		"license": "arXiv.org perpetual, non-exclusive license",
		"note": "publisher: arXiv\nversion: 1",
		"source": "Semantic Scholar",
		"title": "AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments",
		"title-short": "AVLEN",
		"URL": "https://arxiv.org/abs/2210.07940",
		"author": [
			{
				"family": "Paul",
				"given": "Sudipta"
			},
			{
				"family": "Roy-Chowdhury",
				"given": "Amit K."
			},
			{
				"family": "Cherian",
				"given": "Anoop"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	}
]