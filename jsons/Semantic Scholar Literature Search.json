[
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A557W2QV",
		"type": "article-journal",
		"abstract": "In this work, we address the problem of visual navigation by following instructions. In this task, the robot must interpret a natural language instruction in order to follow a predefined path in a possibly unknown environment. Despite different approaches have been proposed in the last years, they are all based on the assumption that the environment contains objects or other elements that can be used to formulate instructions, such as houses or offices. On the contrary, we focus on situations where the environment objects cannot be used to specify a navigation path. In particular, we consider 3D maze-like environments as our test bench because they can be very large and offer very intricate structures. We show that without reference points, visual navigation and instruction following can be rather challenging, and that standard approaches can not be applied successfully. For this reason, we propose a new architecture that explicitly learns both visual navigation and instruction understanding. We demonstrate with simulated experiments that our method can effectively follow instructions and navigate in previously unseen mazes of various sizes.",
		"container-title": "IEEE Robotics and Automation Letters",
		"DOI": "10.1109/LRA.2020.2965857",
		"ISSN": "2377-3766, 2377-3774",
		"issue": "2",
		"journalAbbreviation": "IEEE Robot. Autom. Lett.",
		"page": "1175-1182",
		"source": "Semantic Scholar",
		"title": "Deep Reinforcement Learning for Instruction Following Visual Navigation in 3D Maze-Like Environments",
		"URL": "https://ieeexplore.ieee.org/document/8957297/",
		"volume": "5",
		"author": [
			{
				"family": "Devo",
				"given": "Alessandro"
			},
			{
				"family": "Costante",
				"given": "Gabriele"
			},
			{
				"family": "Valigi",
				"given": "Paolo"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					4
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HYMK5IFZ",
		"type": "article-journal",
		"abstract": "Recent work has described neural-network-based agents that are trained with reinforcement learning (RL) to execute language-like commands in simulated worlds, as a step towards an intelligent agent or robot that can be instructed by human users. However, the optimisation of multi-goal motor policies via deep RL from scratch requires many episodes of experience. Consequently, instruction-following with deep RL typically involves language generated from templates (by an environment simulator), which does not reflect the varied or ambiguous expressions of real users. Here, we propose a conceptually simple method for training instruction-following agents with deep RL that are robust to natural human instructions. By applying our method with a state-of-the-art pre-trained text-based language model (BERT), on tasks requiring agents to identify and position everyday objects relative to other objects in a naturalistic 3D simulated room, we demonstrate substantially-above-chance zero-shot transfer from synthetic template commands to natural instructions given by humans. Our approach is a general recipe for training any deep RL-based system to interface with human users, and bridges the gap between two research directions of notable recent success: agent-centric motor behavior and text-based representation learning.",
		"container-title": "ArXiv",
		"source": "Semantic Scholar",
		"title": "Human Instruction-Following with Deep Reinforcement Learning via Transfer-Learning from Text",
		"URL": "https://www.semanticscholar.org/paper/Human-Instruction-Following-with-Deep-Reinforcement-Hill-Mokra/1ccd73d95b5e23279929866bc15ec2983f9fa700",
		"author": [
			{
				"family": "Hill",
				"given": "Felix"
			},
			{
				"family": "Mokra",
				"given": "Sona"
			},
			{
				"family": "Wong",
				"given": "Nathaniel"
			},
			{
				"family": "Harley",
				"given": "Tim"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					5,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EF437ZV5",
		"type": "article-journal",
		"abstract": "Navigating and understanding the real world remains a key challenge in machine learning and inspires a great variety of research in areas such as language grounding, planning, navigation and computer vision. We propose an instruction-following task that requires all of the above, and which combines the practicality of simulated environments with the challenges of ambiguous, noisy real world data. StreetNav is built on top of Google Street View and provides visually accurate environments representing real places. Agents are given driving instructions which they must learn to interpret in order to successfully navigate in this environment. Since humans equipped with driving instructions can readily navigate in previously unseen cities, we set a high bar and test our trained agents for similar cognitive capabilities. Although deep reinforcement learning (RL) methods are frequently evaluated only on data that closely follow the training distribution, our dataset extends to multiple cities and has a clean train/test separation. This allows for thorough testing of generalisation ability. This paper presents the StreetNav environment and tasks, models that establish strong baselines, and extensive analysis of the task and the trained agents.",
		"container-title": "Proceedings of the AAAI Conference on Artificial Intelligence",
		"DOI": "10.1609/aaai.v34i07.6849",
		"ISSN": "2374-3468, 2159-5399",
		"issue": "07",
		"journalAbbreviation": "AAAI",
		"page": "11773-11781",
		"source": "Semantic Scholar",
		"title": "Learning to Follow Directions in Street View",
		"URL": "https://ojs.aaai.org/index.php/AAAI/article/view/6849",
		"volume": "34",
		"author": [
			{
				"family": "Hermann",
				"given": "Karl Moritz"
			},
			{
				"family": "Malinowski",
				"given": "Mateusz"
			},
			{
				"family": "Mirowski",
				"given": "Piotr"
			},
			{
				"family": "Banki-Horvath",
				"given": "Andras"
			},
			{
				"family": "Anderson",
				"given": "Keith"
			},
			{
				"family": "Hadsell",
				"given": "Raia"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					4,
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RIU3H7YW",
		"type": "article-journal",
		"abstract": "Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, for many real-world natural language commands that involve a degree of underspecification or ambiguity, such as tidy the room, it would be challenging or impossible to program an appropriate reward function. To overcome this, we present a method for learning to follow commands from a training set of instructions and corresponding example goal-states, rather than an explicit reward function. Importantly, the example goal-states are not seen at test time. The approach effectively separates the representation of what instructions require from how they can be executed. In a simple grid world, the method enables an agent to learn a range of commands requiring interaction with blocks and understanding of spatial relations and underspecified abstract arrangements. We further show the method allows our agent to adapt to changes in the environment without requiring new training examples.",
		"container-title": "ArXiv",
		"source": "Semantic Scholar",
		"title": "Learning to Follow Language Instructions with Adversarial Reward Induction",
		"URL": "https://www.semanticscholar.org/paper/Learning-to-Follow-Language-Instructions-with-Bahdanau-Hill/71b152f65fd9967ec39f1e1f359ad0d99be1bab2",
		"author": [
			{
				"family": "Bahdanau",
				"given": "Dzmitry"
			},
			{
				"family": "Hill",
				"given": "Felix"
			},
			{
				"family": "Leike",
				"given": "J."
			},
			{
				"family": "Hughes",
				"given": "Edward"
			},
			{
				"family": "Kohli",
				"given": "Pushmeet"
			},
			{
				"family": "Grefenstette",
				"given": "Edward"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					6,
					5
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/K3KT8MM9",
		"type": "paper-conference",
		"abstract": "context data the inability the untapped utility of data-driven control and simulation tools for urban mobility optimization. Expected scientific contributions include advanced algorithms to mine context-sensitive spatiotemporal traffic patterns; and deep reinforcement learning methods optimize traffic in accordance with context-sensitive predictions. contributions will be made available within an auditable decision support system, attemptively deployed the National Infrastructure for Distributed Computing (INCD), to be promptly used by CML. addition to the immediate utility of iLU for the mobility in Lisbon and its managing, the contributions will be designed to ensure its interoperability and scalability to other cities in and beyond. The occurrence of failures in public transport vehicles during its regular operation is a source of numerous negative impacts that affect not only the operator company but, especially, the clients. In this context, the early detection of such failures can avoid the cancellation of trips. This project focuses on the analysis of the failures of the Compressed-Air Production Unit (APU) in the Metro do Porto (MP) fleet, one of the equipment that most contributes to the cancellation of trips. The goal of the project is to develop a system based on real-time data analysis to notify the maintenance team of the existence of a failure in development, which is undetectable according to traditional maintenance criteria, avoiding its occurrence during the train operation. The use of automated data analysis systems for predicting failures is a cutting-edge technique in high-end industries such as space or aeronautics and is either inexistent or very incipient in less technological sectors. The central APU system installed on the roof of vehicles from MP feeds different units, which perform functions of different degrees of criticality. The maintenance regime currently applied to the APU system is based on pre-scheduled and corrective maintenance activities to respond to events where the failure has already manifested itself in its ultimate consequences. Our goal is to install a set of sensors in the APU system of a vehicle so that data is collected regularly. This data is pre-processed to form a data set based on which we can develop an early failure detection model, following an unsupervised or a semi-supervised approach. Ideally, this model should trigger an alarm to allow the intervention in a very embryonic phase of the failure, in which its symptoms and consequences are still imperceptible, thus avoiding the cancellation of trips. Once the failure detection model is implemented, we intend to study and infer what are the most probable causes of the failure. For this purpose, we will explore probabilistic and/or contextual approaches methods. An effective fault diagnosis can reduce the costs of the repairing plan. Moreover, it can be used to optimize the sensor equipment required, and thus, improve the conditions of application of this early failure detection system to the entire fleet of vehicles of the MP. As a long-term perspective, we believe that this study can be extended to other subsystems of the fleet such as traction converter or automatic those brain scans, using databases of diagnosed patient brain scans. We now have prototypes able to detect disease signatures at early stages of disease; which will be able to provide clinicians, not only a patient-personalized descriptive report of several brain measures but, most innovatively, a score probability of the patient presenting a particular neuropsychiatric illness. This enables a more sensitive, accurate, objective and quantitatively-based diagnosis and thus earlier and more cost- and time-effective disease management. This project aims at the statistical validation and the technical and clinical-administrative trialling of biomarker models for diagnostic classification for the most common neurodegenerative illnesses, Alzheimer’s’ and Parkinson’s. Moreover, there is potential for easy scalability disease-wise: the successful access to retrospective data from hospitals will allow further neuropsychiatric illnesses data to explored with the same neuroimaging and AI-based statistical pipeline in our servers (for example, multiple sclerosis, migraines, epilepsy, schizophrenia and bipolar disorder). Our vision is that with tight collaboration with the Portuguese administration services, we will be able to use existing public health data which, via our neuroimaging AI-based approach, can then give back to the same national public health system a cost-effective disease management solution. Real-time, online-learning and intelligent decision support systems are of most importance to supply intensive care professionals with important information in useful time. Decision errors affect 10% of hospitalized patients. In the case of 30-day postoperative, 12.6% of deaths were associated to medical errors. An excellent decision-making is focused into 3 principal categories: (1) accurate data, (2) pertinent knowledge, and (3) appropriate problem-solving skills. Combining the results achieved in previous projects and making use of the data collected from the bedside monitors, ventilators, pharmacy, electronic health record, electronic nursing record, laboratory system, patient medical history, therapies and procedures, this project aims to extend the actual state of the art in order to support clinical decision-making by: 1 Translating clinical notes based in natural language/narratives into useful data for analytics; 2 - Automatic problem (clinical findings) detection; 3 - Discover cause-effect relations among problems, semiologic notes, assessment notes and clinical decisions (planning); 4 - Improve intelligent decision support on therapies, orders and procedures; 5 - Extend to other intensive medicine areas. The expected results with greatest societal impact and in the ICU decision and management are: 1 - Augment the accuracy on the clinical decisions - the physicians will be supported in their decisions about the most adequate therapies/procedures. They will be able to assess what-if scenarios and decide better; 2- Promote evidence-based clinical decisions – clinical decisions on therapies, procedures and orders (e.g, lab exams) will follow open models based in a clinical ontology for intensive medicine; 3 - Acting proactively – the decision models obtained will be used in association with prediction models to anticipate good decision with non-return situations; 4 - Augmenting the quality of life – a more accurate decision combining patient medical history with decision scenarios can improve the patient life quality; 5 - Costs reduction without compromising the efficiency – cheaper decisions will be proposed without compromising patient outcome; 6 - Reduce the number of clinical errors – decreasing the number of bad decisions taken patients will recover faster and will have a better quality of life reducing costs; 7 - Improve quality of ICU– quality performance will increase. Work-related disorders (WRD) have a major impact on the well being of individuals and their quality of life, productivity and absenteeism, resulting in a great impact on the global economy. The absence of recent or relevant national statistics concerning WRD, leads to a lack of drive to promote studies that characterize the working population, in terms of work-related morbidity and mortality and scientifically supported policies and strategies for occupational health(OH) improvement. This is particularly poignant when addressing the issue in Public Administration (PA). Hence, those diseases are underrepresented on public debates and global population awareness. In spite of the reported work on clinical occupational history, and studies evaluating the occupational exposure to hazards, based on the analysis of biosignals of repetitive activities, there are no studies that combines the historical OH and data from daily working activity, with biosignals. The project “Prevention of occupational disorders on PA based on Artificial Intelligence” will answer to the aforementioned question, by contributing to: the identification and characterization of profiles related with potential WRD, of the Portuguese population and their relationship with professional categories and risk situations at work; forecast the progress of occupational disorders and the associated risk factors, to evaluate the socio-economic impact in morbidity and mortality, with emphasis to the reduced life expectancies caused by disorders and incapacity; the characterization of daily activities, while profiling a sector of workers of PA; the identification of clusters of working activities prone to measure the exposure to risk factors; the definition of national indicators to help monitor and give alerts for occupational risk at PA. Three computational models, based on artificial intelligence (AI) techniques, will be developed: (1) global characterization of occupational disorder profiles; (2) design of daily working activities profiles of an individual; and (3) integration of the information resulting from both previous models, enabling to infer the level of occupational risk of a worker based on his/her occupational history and the data from daily monitorization. This project aims to identify occupational risk, by combining the power of historical records of the global population and the precision data of personalized occupational exposure records. The combination will be achieved through advanced machine learning and AI techniques. The The IPSentinel is Portuguese infrastructure developed by DGT and IPMA for and providing images of Sentinel satellites, covering the Portuguese territory and its search and rescue area. This free EO data has been used to inform environmental models, business strategies and political decisions. The main goal of this project is to explore the applications and limitations of artifi",
		"source": "Semantic Scholar",
		"title": "Research in Data Science and Artificial Intelligence applied",
		"URL": "https://www.semanticscholar.org/paper/Research-in-Data-Science-and-Artificial-applied/68f59299b92eb81f45bde0bef503376be9acd987",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/PWBXYYLM",
		"type": "article-journal",
		"abstract": "Navigation is an important cognitive task that enables humans and animals to traverse, with or without maps, over long distances in the complex world. Such long-range navigation can simultaneously support self-localisation (\"I am here\") and a representation of the goal (\"I am going there\"). For this reason, studying navigation is fundamental to the study and development of artificial intelligence, and trying to replicate navigation in artificial agents can also help neuroscientists understand its biological underpinnings. This talk will cover our own journey to understand navigation by building deep reinforcement learning agents, starting from learning to control a simple agent that can explore and memorise large 3D mazes to designing agents with a read-write memory that can generalise to unseen mazes from one traversal. I will show how these artificial agents relate to navigation in the real world, both through the study of the emergence of grid cell representations in neural networks and by demonstrating that these agents can navigate in Street View-based real world photographic environments. I will finally present two approaches in our ongoing work on leveraging multimodal information for generalising navigation policies to unseen environments in Street View, one consisting in following language instructions and the second one in transferring navigation policies by training on aerial views.",
		"container-title": "1st International Workshop on Multimodal Understanding and Learning for Embodied Applications",
		"DOI": "10.1145/3347450.3357659",
		"language": "en",
		"note": "event-title: MM '19: The 27th ACM International Conference on Multimedia\nISBN: 9781450369183\npublisher-place: Nice France\npublisher: ACM",
		"page": "25-25",
		"source": "Semantic Scholar",
		"title": "Learning to Navigate",
		"URL": "https://dl.acm.org/doi/10.1145/3347450.3357659",
		"author": [
			{
				"family": "Mirowski",
				"given": "Piotr"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					10,
					15
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RR2UADKK",
		"type": "article-journal",
		"abstract": "Language-guided Embodied AI benchmarks requiring an agent to navigate an environment and manipulate objects typically allow one-way communication: the human user gives a natural language command to the agent, and the agent can only follow the command passively. We present DialFRED, a dialogue-enabled embodied instruction following benchmark based on the ALFRED benchmark. DialFRED allows an agent to actively ask questions to the human user; the additional information in the user's response is used by the agent to better complete its task. We release a human-annotated dataset with 53 K task-relevant questions and answers and an oracle to answer questions. To tackle DialFRED, we propose a questioner-performer framework wherein the questioner is pre-trained with the human-annotated data and fine-tuned with reinforcement learning. Experimental results show that asking the right questions leads to significantly improved task performance. We make DialFRED publicly available and encourage researchers to propose and evaluate their solutions to building dialog-enabled embodied agents: https://github.com/xfgao/DialFRED.",
		"container-title": "IEEE Robotics and Automation Letters",
		"DOI": "10.1109/LRA.2022.3193254",
		"ISSN": "2377-3766, 2377-3774",
		"issue": "4",
		"journalAbbreviation": "IEEE Robot. Autom. Lett.",
		"page": "10049-10056",
		"source": "Semantic Scholar",
		"title": "DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following",
		"title-short": "DialFRED",
		"URL": "https://ieeexplore.ieee.org/document/9837390/",
		"volume": "7",
		"author": [
			{
				"family": "Gao",
				"given": "Xiaofeng"
			},
			{
				"family": "Gao",
				"given": "Qiaozi"
			},
			{
				"family": "Gong",
				"given": "Ran"
			},
			{
				"family": "Lin",
				"given": "Kaixiang"
			},
			{
				"family": "Thattai",
				"given": "Govind"
			},
			{
				"family": "Sukhatme",
				"given": "Gaurav S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/GR6LBZWN",
		"type": "webpage",
		"title": "[PDF] From Language to Goals: Inverse Reinforcement Learning for Vision-Based Instruction Following | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/From-Language-to-Goals%3A-Inverse-Reinforcement-for-Fu-Balan/758311575a6385bb15d4f9af8c0e671cb98184b4",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/B94Y7MZR",
		"type": "article-journal",
		"abstract": "Text-based games present a unique class of sequential decision making problem in which agents interact with a partially observable, simulated environment via actions and observations conveyed through natural language. Such observations typically include instructions that, in a reinforcement learning (RL) setting, can directly or indirectly guide a player towards completing reward-worthy tasks. In this work, we study the ability of RL agents to follow such instructions. We conduct experiments that show that the performance of state-of-the-art text-based game agents is largely unaffected by the presence or absence of such instructions, and that these agents are typically unable to execute tasks to completion. To further study and address the task of instruction following, we equip RL agents with an internal structured representation of natural language instructions in the form of Linear Temporal Logic (LTL), a formal language that is increasingly used for temporally extended reward specification in RL. Our framework both supports and highlights the benefit of understanding the temporal semantics of instructions and in measuring progress towards achievement of such a temporally extended behaviour. Experiments with 500+ games in TextWorld demonstrate the superior performance of our approach.",
		"DOI": "10.48550/ARXIV.2211.04591",
		"license": "Creative Commons Attribution 4.0 International",
		"note": "publisher: arXiv\nversion: 1",
		"source": "Semantic Scholar",
		"title": "Learning to Follow Instructions in Text-Based Games",
		"URL": "https://arxiv.org/abs/2211.04591",
		"author": [
			{
				"family": "Tuli",
				"given": "Mathieu"
			},
			{
				"family": "Li",
				"given": "Andrew C."
			},
			{
				"family": "Vaezipoor",
				"given": "Pashootan"
			},
			{
				"family": "Klassen",
				"given": "Toryn Q."
			},
			{
				"family": "Sanner",
				"given": "Scott"
			},
			{
				"family": "McIlraith",
				"given": "Sheila A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KWIWRCPG",
		"type": "paper-conference",
		"abstract": "Introduction In order to achieve higher levels of autonomy, robots need th ability to interact naturally with humans in unstructured environments. One of the most intuitive and fle xibl interaction modalities is to allow a human teammate to instruct a robot with natural language commands . I order to follow natural language directions, a robot needs to convert symbolic natural language instruct ions to low level actions and observations. We would like a system that can take a command from a teammate suc h as “Follow me to the kitchen” and generate a sequence of actions that corresponds to the desir d motion through the environment.",
		"source": "Semantic Scholar",
		"title": "Inverse Reinforcement Learning for Following Instructions",
		"URL": "https://www.semanticscholar.org/paper/Inverse-Reinforcement-Learning-for-Following-Roy/664d1a2023924eefc4593a119a0e1715ddcb8b64",
		"author": [
			{
				"family": "Roy",
				"given": "N."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KMIU6MK2",
		"type": "paper-conference",
		"abstract": "Text-based games present a unique class of sequential decision making problem in which agents interact with a partially observable, simulated environment via actions and observations conveyed through natural language. Such observations typically include instructions that, in a reinforcement learning (RL) setting, can directly or indirectly guide a player towards completing reward-worthy tasks. In this work, we study the ability of RL agents to follow such instructions. We conduct experiments that show that the performance of state-of-the-art text-based game agents is largely unaffected by the presence or absence of such instructions, and that these agents are typically unable to execute tasks to completion. To further study and address the task of instruction following, we equip RL agents with an internal structured representation of natural language instructions in the form of Linear Temporal Logic (LTL), a formal language that is increasingly used for temporally extended reward specification in RL. Our framework both supports and highlights the benefit of understanding the temporal semantics of instructions and in measuring progress towards achievement of such a temporally extended behaviour. Experiments demonstrate the superior performance of our approach.",
		"source": "Semantic Scholar",
		"title": "Instruction Following in Text-Based Games",
		"URL": "https://www.semanticscholar.org/paper/Instruction-Following-in-Text-Based-Games-Tuli-Li/72c7d446423c0af808b63394c78c2167c96c649d",
		"author": [
			{
				"family": "Tuli",
				"given": "Mathieu"
			},
			{
				"family": "Li",
				"given": "Andrew C."
			},
			{
				"family": "Vaezipoor",
				"given": "Pashootan"
			},
			{
				"family": "Klassen",
				"given": "Toryn Q."
			},
			{
				"family": "Sanner",
				"given": "S."
			},
			{
				"family": "McIlraith",
				"given": "Sheila A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/Z2N3KBHE",
		"type": "article-journal",
		"abstract": "Abstract\n            Video description refers to understanding visual content and transforming that acquired understanding into automatic textual narration. It bridges the key AI fields of computer vision and natural language processing in conjunction with real-time and practical applications. Deep learning-based approaches employed for video description have demonstrated enhanced results compared to conventional approaches. The current literature lacks a thorough interpretation of the recently developed and employed sequence to sequence techniques for video description. This paper fills that gap by focusing mainly on deep learning-enabled approaches to automatic caption generation. Sequence to sequence models follow an Encoder–Decoder architecture employing a specific composition of CNN, RNN, or the variants LSTM or GRU as an encoder and decoder block. This standard-architecture can be fused with an attention mechanism to focus on a specific distinctiveness, achieving high quality results. Reinforcement learning employed within the Encoder–Decoder structure can progressively deliver state-of-the-art captions by following exploration and exploitation strategies. The transformer mechanism is a modern and efficient transductive architecture for robust output. Free from recurrence, and solely based on self-attention, it allows parallelization along with training on a massive amount of data. It can fully utilize the available GPUs for most NLP tasks. Recently, with the emergence of several versions of transformers, long term dependency handling is not an issue anymore for researchers engaged in video processing for summarization and description, or for autonomous-vehicle, surveillance, and instructional purposes. They can get auspicious directions from this research.",
		"container-title": "Artificial Intelligence Review",
		"DOI": "10.1007/s10462-023-10414-6",
		"ISSN": "0269-2821, 1573-7462",
		"issue": "11",
		"journalAbbreviation": "Artif Intell Rev",
		"language": "en",
		"page": "13293-13372",
		"source": "Semantic Scholar",
		"title": "Video description: A comprehensive survey of deep learning approaches",
		"title-short": "Video description",
		"URL": "https://link.springer.com/10.1007/s10462-023-10414-6",
		"volume": "56",
		"author": [
			{
				"family": "Rafiq",
				"given": "Ghazala"
			},
			{
				"family": "Rafiq",
				"given": "Muhammad"
			},
			{
				"family": "Choi",
				"given": "Gyu Sang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					11
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/I23T95TW",
		"type": "paper-conference",
		"abstract": ": Enabling human operators to interact with robotic agents using natural language would allow non-experts to intuitively instruct these agents. Towards this goal, we propose a novel Transformer-based model which enables a user to guide a robot arm through a 3D multi-step manipulation task with natural language commands. Our system maps images and commands to masks over grasp or place locations, grounding the language directly in perceptual space. In a suite of block rearrangement tasks, we show that these masks can be combined with an existing manipulation framework without re-training, greatly improving learning efﬁciency. Our masking model is several orders of magnitude more sample efﬁcient than typical Transformer models, operating with hundreds, not millions, of examples. Our modular design allows us to leverage supervised and reinforcement learning, providing an easy interface for experimentation with different architec-tures 2 . Our model completes block manipulation tasks with synthetic commands 530% more often than a UNet-based baseline, and learns to localize actions correctly while creating a mapping of symbols to perceptual input that supports compositional reasoning. We provide a valuable resource for 3D manipulation instruction following research by porting an existing 3D block dataset with crowdsourced language to a simulated environment. Our method’s 25 . 3% absolute improvement in identifying the correct block on the ported dataset demonstrates its ability to handle syntactic and lexical variation.",
		"event-title": "Conference on Robot Learning",
		"source": "Semantic Scholar",
		"title": "Guiding Multi-Step Rearrangement Tasks with Natural Language Instructions",
		"URL": "https://www.semanticscholar.org/paper/Guiding-Multi-Step-Rearrangement-Tasks-with-Natural-Stengel-Eskin-Hundt/dff59ec1f1d3c01c3c7046517aa7b0612655764c",
		"author": [
			{
				"family": "Stengel-Eskin",
				"given": "Elias"
			},
			{
				"family": "Hundt",
				"given": "Andrew T."
			},
			{
				"family": "He",
				"given": "Zhuohong"
			},
			{
				"family": "Murali",
				"given": "Aditya"
			},
			{
				"family": "Gopalan",
				"given": "N."
			},
			{
				"family": "Gombolay",
				"given": "M."
			},
			{
				"family": "Hager",
				"given": "Gregory"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/RHTUM6U8",
		"type": "article-journal",
		"abstract": "Learning in environments with large state and action spaces, and sparse rewards, can hinder a Reinforcement Learning (RL) agent's learning through trial-and-error. For instance, following natural language instructions on the Web (such as booking a flight ticket) leads to RL settings where input vocabulary and number of actionable elements on a page can grow very large. Even though recent approaches improve the success rate on relatively simple environments with the help of human demonstrations to guide the exploration, they still fail in environments where the set of possible instructions can reach millions. We approach the aforementioned problems from a different perspective and propose guided RL approaches that can generate unbounded amount of experience for an agent to learn from. Instead of learning from a complicated instruction with a large vocabulary, we decompose it into multiple sub-instructions and schedule a curriculum in which an agent is tasked with a gradually increasing subset of these relatively easier sub-instructions. In addition, when the expert demonstrations are not available, we propose a novel meta-learning framework that generates new instruction following tasks and trains the agent more effectively. We train DQN, deep reinforcement learning agent, with Q-value function approximated with a novel QWeb neural network architecture on these smaller, synthetic instructions. We evaluate the ability of our agent to generalize to new instructions on World of Bits benchmark, on forms with up to 100 elements, supporting 14 million possible instructions. The QWeb agent outperforms the baseline without using any human demonstration achieving 100% success rate on several difficult environments.",
		"container-title": "ArXiv",
		"source": "Semantic Scholar",
		"title": "Learning to Navigate the Web",
		"URL": "https://www.semanticscholar.org/paper/Learning-to-Navigate-the-Web-Gur-R%C3%BCckert/a7038473320c50df76fa950aca486015c5659503",
		"author": [
			{
				"family": "Gur",
				"given": "Izzeddin"
			},
			{
				"family": "Rückert",
				"given": "U."
			},
			{
				"family": "Faust",
				"given": "Aleksandra"
			},
			{
				"family": "Hakkani-Tür",
				"given": "Dilek Z."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					12,
					21
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/AZS7PT3B",
		"type": "article-journal",
		"abstract": "Recent years have seen embodied visual navigation advance in two distinct directions: (i) in equipping the AI agent to follow natural language instructions, and (ii) in making the navigable world multimodal, e.g., audio-visual navigation. However, the real world is not only multimodal, but also often complex, and thus in spite of these advances, agents still need to understand the uncertainty in their actions and seek instructions to navigate. To this end, we present AVLEN~ -- an interactive agent for Audio-Visual-Language Embodied Navigation. Similar to audio-visual navigation tasks, the goal of our embodied agent is to localize an audio event via navigating the 3D visual world; however, the agent may also seek help from a human (oracle), where the assistance is provided in free-form natural language. To realize these abilities, AVLEN uses a multimodal hierarchical reinforcement learning backbone that learns: (a) high-level policies to choose either audio-cues for navigation or to query the oracle, and (b) lower-level policies to select navigation actions based on its audio-visual and language inputs. The policies are trained via rewarding for the success on the navigation task while minimizing the number of queries to the oracle. To empirically evaluate AVLEN, we present experiments on the SoundSpaces framework for semantic audio-visual navigation tasks. Our results show that equipping the agent to ask for help leads to a clear improvement in performance, especially in challenging cases, e.g., when the sound is unheard during training or in the presence of distractor sounds.",
		"DOI": "10.48550/ARXIV.2210.07940",
		"license": "arXiv.org perpetual, non-exclusive license",
		"note": "publisher: arXiv\nversion: 1",
		"source": "Semantic Scholar",
		"title": "AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments",
		"title-short": "AVLEN",
		"URL": "https://arxiv.org/abs/2210.07940",
		"author": [
			{
				"family": "Paul",
				"given": "Sudipta"
			},
			{
				"family": "Roy-Chowdhury",
				"given": "Amit K."
			},
			{
				"family": "Cherian",
				"given": "Anoop"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/32C4N596",
		"type": "article-journal",
		"abstract": "Recent work has shown that large text-based neural language models, trained with conventional supervised learning objectives, acquire a surprising propensity for few- and one-shot learning. Here, we show that an embodied agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional reinforcement learning algorithms. After a single introduction to a novel object via continuous visual perception and a language prompt (\"This is a dax\"), the agent can re-identify the object and manipulate it as instructed (\"Put the dax on the bed\"). In doing so, it seamlessly integrates short-term, within-episode knowledge of the appropriate referent for the word \"dax\" with long-term lexical and motor knowledge acquired across episodes (i.e. \"bed\" and \"putting\"). We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful for later executing instructions. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for 'fast-mapping', a fundamental pillar of human cognitive development and a potentially transformative capacity for agents that interact with human users.",
		"container-title": "ArXiv",
		"source": "Semantic Scholar",
		"title": "Grounded Language Learning Fast and Slow",
		"URL": "https://www.semanticscholar.org/paper/Grounded-Language-Learning-Fast-and-Slow-Hill-Tieleman/1c39625ed65389cfb9d268f93f406455665f201b",
		"author": [
			{
				"family": "Hill",
				"given": "Felix"
			},
			{
				"family": "Tieleman",
				"given": "O."
			},
			{
				"family": "Glehn",
				"given": "Tamara",
				"dropping-particle": "von"
			},
			{
				"family": "Wong",
				"given": "Nathaniel"
			},
			{
				"family": "Merzic",
				"given": "Hamza"
			},
			{
				"family": "Clark",
				"given": "S."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					9,
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7VE5JPWF",
		"type": "webpage",
		"title": "Communicating Hierarchical Neural Controllers for Learning Zero-shot Task Generalization | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/Communicating-Hierarchical-Neural-Controllers-for-Oh-Singh/3693414d385401997c13a4faa39a8b6c6cd4a4dd",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/I3ZPKBRX",
		"type": "paper-conference",
		"abstract": "An interactive instruction following task has been proposed as a benchmark for learning to map natural language instructions and first-person vision into sequences of actions to interact with objects in 3D environments. We found that an existing end-to-end neural model for this task tends to fail to interact with objects of unseen attributes and follow various instructions. We assume that this problem is caused by the high sensitivity of neural feature extraction to small changes in vision and language inputs. To mitigate this problem, we propose a neuro-symbolic approach that utilizes high-level symbolic features, which are robust to small changes in raw inputs, as intermediate representations. We verify the effectiveness of our model with the subtask evaluation on the ALFRED benchmark. Our experiments show that our approach significantly outperforms the end-to-end neural model by 9, 46, and 74 points in the success rate on the ToggleObject, PickupObject, and SliceObject subtasks in unseen environments respectively.",
		"DOI": "10.1007/978-3-031-27818-1_52",
		"event-place": "Cham",
		"ISBN": "978-3-031-27817-4",
		"language": "en",
		"note": "Book Title: MultiMedia Modeling\ncollection-title: Lecture Notes in Computer Science\nDOI: 10.1007/978-3-031-27818-1_52",
		"page": "635-646",
		"publisher": "Springer Nature Switzerland",
		"publisher-place": "Cham",
		"source": "Semantic Scholar",
		"title": "Improving the Robustness to Variations of Objects and Instructions with a Neuro-Symbolic Approach for Interactive Instruction Following",
		"URL": "https://link.springer.com/10.1007/978-3-031-27818-1_52",
		"volume": "13834",
		"editor": [
			{
				"family": "Dang-Nguyen",
				"given": "Duc-Tien"
			},
			{
				"family": "Gurrin",
				"given": "Cathal"
			},
			{
				"family": "Larson",
				"given": "Martha"
			},
			{
				"family": "Smeaton",
				"given": "Alan F."
			},
			{
				"family": "Rudinac",
				"given": "Stevan"
			},
			{
				"family": "Dao",
				"given": "Minh-Son"
			},
			{
				"family": "Trattner",
				"given": "Christoph"
			},
			{
				"family": "Chen",
				"given": "Phoebe"
			}
		],
		"author": [
			{
				"family": "Shinoda",
				"given": "Kazutoshi"
			},
			{
				"family": "Takezawa",
				"given": "Yuki"
			},
			{
				"family": "Suzuki",
				"given": "Masahiro"
			},
			{
				"family": "Iwasawa",
				"given": "Yusuke"
			},
			{
				"family": "Matsuo",
				"given": "Yutaka"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EJU9KUA3",
		"type": "paper-conference",
		"abstract": "Vision-and-language navigation requires an agent to navigate through a real 3D environment following a given natural language instruction. Despite significant advances, few previous works are able to fully utilize the strong correspondence between the visual and textual sequences. Meanwhile, due to the lack of intermediate supervision, the agent's performance at following each part of the instruction remains untrackable during navigation. In this work, we focus on the granularity of the visual and language sequences as well as the trackability of agents through the completion of instruction. We provide agents with fine-grained annotations during training and find that they are able to follow the instruction better and have a higher chance of reaching the target at test time. We enrich the previous dataset with sub-instructions and their corresponding paths. To make use of this data, we propose an effective sub-instruction attention and shifting modules that attend and select a single sub-instruction at each time-step. We implement our sub-instruction modules in four state-of-the-art agents, compare with their baseline model, and show that our proposed method improves the performance of all four agents.",
		"container-title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
		"DOI": "10.18653/v1/2020.emnlp-main.271",
		"event-place": "Online",
		"event-title": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
		"language": "en",
		"page": "3360-3376",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Online",
		"source": "Semantic Scholar",
		"title": "Sub-Instruction Aware Vision-and-Language Navigation",
		"URL": "https://www.aclweb.org/anthology/2020.emnlp-main.271",
		"author": [
			{
				"family": "Hong",
				"given": "Yicong"
			},
			{
				"family": "Rodriguez",
				"given": "Cristian"
			},
			{
				"family": "Wu",
				"given": "Qi"
			},
			{
				"family": "Gould",
				"given": "Stephen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/5E9D8XEZ",
		"type": "article-journal",
		"abstract": "Embodied Concept Learner in an interactive 3D environment enables the robotic agent to learn semantics and depth unsupervisedly acting like babies, and is fully transparent and step-by-step interpretable in long-term planning. Humans, even at a very early age, can learn visual concepts and understand geometry and layout through active interaction with the environment, and generalize their compositions to complete tasks described by natural languages in novel scenes. To mimic such capability, we propose Embodied Concept Learner (ECL) in an interactive 3D environment. Specifically, a robot agent can ground visual concepts, build semantic maps and plan actions to complete tasks by learning purely from human demonstrations and language instructions, without access to ground-truth semantic and depth supervisions from simulations. ECL consists of: (i) an instruction parser that translates the natural languages into executable programs; (ii) an embodied concept learner that grounds visual concepts based on language descriptions; (iii) a map constructor that estimates depth and constructs semantic maps by leveraging the learned concepts; and (iv) a program executor with deterministic policies to execute each program. ECL has several appealing benefits thanks to its modularized design. Firstly, it enables the robotic agent to learn semantics and depth unsupervisedly acting like babies, e.g., ground concepts through active interaction and perceive depth by disparities when moving forward. Secondly, ECL is fully transparent and step-by-step interpretable in long-term planning. Thirdly, ECL could be beneficial for the embodied instruction following (EIF), outperforming previous works on the ALFRED benchmark when the semantic label is not provided. Also, the learned concept can be reused for other downstream tasks, such as reasoning of object states. Project page: http://ecl.csail.mit.edu/",
		"container-title": "Conference on Robot Learning",
		"language": "en",
		"source": "www.semanticscholar.org",
		"title": "Embodied Concept Learner: Self-supervised Learning of Concepts and Mapping through Instruction Following",
		"title-short": "Embodied Concept Learner",
		"URL": "https://www.semanticscholar.org/paper/Embodied-Concept-Learner%3A-Self-supervised-Learning-Ding-Xu/e3a664b7f7e68b85d0114dd44e1fbbe221e0bab1",
		"author": [
			{
				"family": "Ding",
				"given": "Mingyu"
			},
			{
				"family": "Xu",
				"given": "Yan"
			},
			{
				"family": "Chen",
				"given": "Zhenfang"
			},
			{
				"family": "Cox",
				"given": "David D."
			},
			{
				"family": "Luo",
				"given": "Ping"
			},
			{
				"family": "Tenenbaum",
				"given": "J."
			},
			{
				"family": "Gan",
				"given": "Chuang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/KJD8YN8R",
		"type": "article-journal",
		"abstract": "With the continuously thriving popularity around the world, fitness activity analytic has become an emerging research topic in computer vision. While a variety of new tasks and algorithms have been proposed recently, there are growing hunger for data resources involved in high-quality data, fine-grained labels, and diverse environments. In this paper, we present FLAG3D, a large-scale 3D fitness activity dataset with language instruction containing 180K sequences of 60 categories. FLAG3D features the following three aspects: 1) accurate and dense 3D human pose captured from advanced MoCap system to handle the complex activity and large movement, 2) detailed and professional language instruction to describe how to perform a specific activity, 3) versatile video resources from a high-tech MoCap system, rendering software, and cost-effective smartphones in natural environments. Extensive experiments and in-depth analysis show that FLAG3D contributes great research value for various challenges, such as cross-domain human action recognition, dynamic human mesh recovery, and language-guided human action generation. Our dataset and source code are publicly available at https://andytang15.github.io/FLAG3D.",
		"container-title": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
		"DOI": "10.1109/CVPR52729.2023.02117",
		"note": "event-title: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\nISBN: 9798350301298\npublisher-place: Vancouver, BC, Canada\npublisher: IEEE",
		"page": "22106-22117",
		"source": "Semantic Scholar",
		"title": "FLAG3D: A 3D Fitness Activity Dataset with Language Instruction",
		"title-short": "FLAG3D",
		"URL": "https://ieeexplore.ieee.org/document/10203838/",
		"author": [
			{
				"family": "Tang",
				"given": "Yansong"
			},
			{
				"family": "Liu",
				"given": "Jinpeng"
			},
			{
				"family": "Liu",
				"given": "Aoyang"
			},
			{
				"family": "Yang",
				"given": "Bin"
			},
			{
				"family": "Dai",
				"given": "Wenxun"
			},
			{
				"family": "Rao",
				"given": "Yongming"
			},
			{
				"family": "Lu",
				"given": "Jiwen"
			},
			{
				"family": "Zhou",
				"given": "Jie"
			},
			{
				"family": "Li",
				"given": "Xiu"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023",
					6
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/Z3FEVJTC",
		"type": "paper-conference",
		"abstract": "In the Vision-and-Language Navigation (VLN) task an embodied agent navigates a 3D environment, following natural language instructions. A challenge in this task is how to handle ‘off the path’ scenarios where an agent veers from a reference path. Prior work supervises the agent with actions based on the shortest path from the agent’s location to the goal, but such goal-oriented supervision is often not in alignment with the instruction. Furthermore, the evaluation metrics employed by prior work do not measure how much of a language instruction the agent is able to follow. In this work, we propose a simple and effective language-aligned supervision scheme, and a new metric that measures the number of sub-instructions the agent has completed during navigation.",
		"container-title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/2021.emnlp-main.328",
		"event-place": "Online and Punta Cana, Dominican Republic",
		"event-title": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
		"language": "en",
		"page": "4018-4028",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Online and Punta Cana, Dominican Republic",
		"source": "Semantic Scholar",
		"title": "Language-Aligned Waypoint (LAW) Supervision for Vision-and-Language Navigation in Continuous Environments",
		"URL": "https://aclanthology.org/2021.emnlp-main.328",
		"author": [
			{
				"family": "Raychaudhuri",
				"given": "Sonia"
			},
			{
				"family": "Wani",
				"given": "Saim"
			},
			{
				"family": "Patel",
				"given": "Shivansh"
			},
			{
				"family": "Jain",
				"given": "Unnat"
			},
			{
				"family": "Chang",
				"given": "Angel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/PBLYCJIA",
		"type": "article-journal",
		"abstract": "In vision-and-language navigation (VLN), an embodied agent is required to navigate in realistic 3D environments following natural language instructions. One major bottleneck for existing VLN approaches is the lack of sufficient training data, resulting in unsatisfactory generalization to unseen environments. While VLN data is typically collected manually, such an approach is expensive and prevents scalability. In this work, we address the data scarcity issue by proposing to automatically create a large-scale VLN dataset from 900 unlabeled 3D buildings from HM3D. We generate a navigation graph for each building and transfer object predictions from 2D to generate pseudo 3D object labels by cross-view consistency. We then fine-tune a pretrained language model using pseudo object labels as prompts to alleviate the cross-modal gap in instruction generation. Our resulting HM3D-AutoVLN dataset is an order of magnitude larger than existing VLN datasets in terms of navigation environments and instructions. We experimentally demonstrate that HM3D-AutoVLN significantly increases the generalization ability of resulting VLN models. On the SPL metric, our approach improves over state of the art by 7.1% and 8.1% on the unseen validation splits of REVERIE and SOON datasets respectively.",
		"DOI": "10.48550/ARXIV.2208.11781",
		"license": "arXiv.org perpetual, non-exclusive license",
		"note": "publisher: arXiv\nversion: 1",
		"source": "Semantic Scholar",
		"title": "Learning from Unlabeled 3D Environments for Vision-and-Language Navigation",
		"URL": "https://arxiv.org/abs/2208.11781",
		"author": [
			{
				"family": "Chen",
				"given": "Shizhe"
			},
			{
				"family": "Guhur",
				"given": "Pierre-Louis"
			},
			{
				"family": "Tapaswi",
				"given": "Makarand"
			},
			{
				"family": "Schmid",
				"given": "Cordelia"
			},
			{
				"family": "Laptev",
				"given": "Ivan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/NLFYWFKD",
		"type": "article-journal",
		"abstract": "CB2 is a multi-agent platform to study collaborative natural language interaction in a grounded task-oriented scenario. It includes a 3D game environment, a backend server designed to serve trained models to human agents, and various tools and processes to enable scalable studies. We deploy CB2 at https://cb2.ai as a system demonstration with a learned instruction following model.",
		"DOI": "10.48550/ARXIV.2303.08127",
		"license": "arXiv.org perpetual, non-exclusive license",
		"note": "publisher: arXiv\nversion: 3",
		"source": "Semantic Scholar",
		"title": "CB2: Collaborative Natural Language Interaction Research Platform",
		"title-short": "CB2",
		"URL": "https://arxiv.org/abs/2303.08127",
		"author": [
			{
				"family": "Sharf",
				"given": "Jacob"
			},
			{
				"family": "Gul",
				"given": "Mustafa Omer"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/99JEU93J",
		"type": "article-journal",
		"abstract": "Vision-and-language navigation (VLN) enables the agent to navigate to a remote location following the natural language instruction in 3D environments. To represent the previously visited environment, most approaches for VLN implement memory using recurrent states, topological maps, or top-down semantic maps. In contrast to these approaches, we build the top-down egocentric and dynamically growing Grid Memory Map (i.e., GridMM) to structure the visited environment. From a global perspective, historical observations are projected into a unified grid map in a top-down view, which can better represent the spatial relations of the environment. From a local perspective, we further propose an instruction relevance aggregation method to capture fine-grained visual clues in each grid region. Extensive experiments are conducted on both the REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CE dataset in the continuous environments, showing the superiority of our proposed method.",
		"DOI": "10.48550/ARXIV.2307.12907",
		"license": "arXiv.org perpetual, non-exclusive license",
		"note": "publisher: arXiv\nversion: 4",
		"source": "Semantic Scholar",
		"title": "GridMM: Grid Memory Map for Vision-and-Language Navigation",
		"title-short": "GridMM",
		"URL": "https://arxiv.org/abs/2307.12907",
		"author": [
			{
				"family": "Wang",
				"given": "Zihan"
			},
			{
				"family": "Li",
				"given": "Xiangyang"
			},
			{
				"family": "Yang",
				"given": "Jiahao"
			},
			{
				"family": "Liu",
				"given": "Yeqi"
			},
			{
				"family": "Jiang",
				"given": "Shuqiang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/MFSFW3HA",
		"type": "article-journal",
		"abstract": "We examined the effects on users during collaboration with two types of virtual human (VH) agents in object transportation in an immersive virtual environment or virtual reality (VR). The two types of virtual humans we examined are leader and follower agents. The goal of the users is to interact with the agents using natural language and carry objects from initial locations to destinations. In each trial, the follower agent follows a user’s instructions to perform actions to manipulate the object. The leader agent determines the appropriate actions that the agent and the user should perform. We developed a system which enabled users and virtual agents to carry objects in an intuitive manner. We conducted a within-subjects study to evaluate the user behaviors under two conditions: (LVH) interaction with a leader virtual human and (FVH) interaction with a follower virtual human. We found that the participants in LVH required a higher workload than that in FVH. However, the users’ experiences, game experiences, and users’ impressions between the two conditions were not significantly different.",
		"container-title": "2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)",
		"DOI": "10.1109/VR51125.2022.00052",
		"note": "event-title: 2022 IEEE on Conference Virtual Reality and 3D User Interfaces (VR)\nISBN: 9781665496179\npublisher-place: Christchurch, New Zealand\npublisher: IEEE",
		"page": "330-339",
		"source": "Semantic Scholar",
		"title": "Investigating the Effects of Leading and Following Behaviors of Virtual Humans in Collaborative Fine Motor Tasks in Virtual Reality",
		"URL": "https://ieeexplore.ieee.org/document/9756732/",
		"author": [
			{
				"family": "Liu",
				"given": "Kuan-Yu"
			},
			{
				"family": "Wong",
				"given": "Sai-Keung"
			},
			{
				"family": "Volonte",
				"given": "Matias"
			},
			{
				"family": "Ebrahimi",
				"given": "Elham"
			},
			{
				"family": "Babu",
				"given": "Sabarish V."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2022",
					3
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FGMGS7PV",
		"type": "article-journal",
		"abstract": "Large Language Models (LLMs) are trained and aligned to follow natural language instructions with only a handful of examples, and they are prompted as task-driven autonomous agents to adapt to various sources of execution environments. However, deploying agent LLMs in virtual reality (VR) has been challenging due to the lack of efficiency in online interactions and the complex manipulation categories in 3D environments. In this work, we propose Voice2Action, a framework that hierarchically analyzes customized voice signals and textual commands through action and entity extraction and divides the execution tasks into canonical interaction subsets in real-time with error prevention from environment feedback. Experiment results in an urban engineering VR environment with synthetic instruction data show that Voice2Action can perform more efficiently and accurately than approaches without optimizations.",
		"DOI": "10.48550/ARXIV.2310.00092",
		"license": "Creative Commons Attribution 4.0 International",
		"note": "publisher: arXiv\nversion: 1",
		"source": "Semantic Scholar",
		"title": "Voice2Action: Language Models as Agent for Efficient Real-Time Interaction in Virtual Reality",
		"title-short": "Voice2Action",
		"URL": "https://arxiv.org/abs/2310.00092",
		"author": [
			{
				"family": "Su",
				"given": "Yang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/58IZMKXT",
		"type": "article-journal",
		"abstract": "Natural language is perhaps the most flexible and intuitive way for humans to communicate tasks to a robot. Prior work in imitation learning typically requires each task be specified with a task id or goal image -- something that is often impractical in open-world environments. On the other hand, previous approaches in instruction following allow agent behavior to be guided by language, but typically assume structure in the observations, actuators, or language that limit their applicability to complex settings like robotics. In this work, we present a method for incorporating free-form natural language conditioning into imitation learning. Our approach learns perception from pixels, natural language understanding, and multitask continuous control end-to-end as a single neural network. Unlike prior work in imitation learning, our method is able to incorporate unlabeled and unstructured demonstration data (i.e. no task or language labels). We show this dramatically improves language conditioned performance, while reducing the cost of language annotation to less than 1% of total data. At test time, a single language conditioned visuomotor policy trained with our method can perform a wide variety of robotic manipulation skills in a 3D environment, specified only with natural language descriptions of each task (e.g.\"open the drawer...now pick up the block...now press the green button...\"). To scale up the number of instructions an agent can follow, we propose combining text conditioned policies with large pretrained neural language models. We find this allows a policy to be robust to many out-of-distribution synonym instructions, without requiring new demonstrations. See videos of a human typing live text commands to our agent at language-play.github.io",
		"container-title": "Robotics: Science and Systems XVII",
		"DOI": "10.15607/RSS.2021.XVII.047",
		"note": "event-title: Robotics: Science and Systems 2021\nISBN: 9780992374778\npublisher: Robotics: Science and Systems Foundation",
		"source": "Semantic Scholar",
		"title": "Language Conditioned Imitation Learning Over Unstructured Data",
		"URL": "http://www.roboticsproceedings.org/rss17/p047.pdf",
		"author": [
			{
				"family": "Lynch*",
				"given": "Corey"
			},
			{
				"family": "Sermanet*",
				"given": "Pierre"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021",
					7,
					12
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QPGEIU9M",
		"type": "article-journal",
		"abstract": "Given a natural language, a general robot has to comprehend the instruction and find the target object or location based on visual observations even in unexplored environments. Most agents rely on massive diverse training data to achieve better generalization, which requires expensive labor. These agents often focus on common objects and fewer tasks, thus are not intelligent enough to handle different types of instructions. To facilitate research in open-set vision-and-language navigation, we propose a benchmark named MO-VLN, aiming at testing the effectiveness and generalization of the agent in the multi-task setting. First, we develop a 3D simulator rendered by realistic scenarios using Unreal Engine 5, containing more realistic lights and details. The simulator contains three scenes, i.e., cafe, restaurant, and nursing house, of high value in the industry. Besides, our simulator involves multiple uncommon objects, such as takeaway cup and medical adhesive tape, which are more complicated compared with existing environments. Inspired by the recent success of large language models (e.g., ChatGPT, Vicuna), we construct diverse high-quality data of instruction type without human annotation. Our benchmark MO-VLN provides four tasks: 1) goal-conditioned navigation given a specific object category (e.g., \"fork\"); 2) goal-conditioned navigation given simple instructions (e.g., \"Search for and move towards a tennis ball\"); 3) step-by-step instruction following; 4) finding abstract object based on high-level instruction (e.g., \"I am thirsty\").",
		"DOI": "10.48550/ARXIV.2306.10322",
		"license": "arXiv.org perpetual, non-exclusive license",
		"note": "publisher: arXiv\nversion: 2",
		"source": "Semantic Scholar",
		"title": "MO-VLN: A Multi-Task Benchmark for Open-set Zero-Shot Vision-and-Language Navigation",
		"title-short": "MO-VLN",
		"URL": "https://arxiv.org/abs/2306.10322",
		"author": [
			{
				"family": "Liang",
				"given": "Xiwen"
			},
			{
				"family": "Ma",
				"given": "Liang"
			},
			{
				"family": "Guo",
				"given": "Shanshan"
			},
			{
				"family": "Han",
				"given": "Jianhua"
			},
			{
				"family": "Xu",
				"given": "Hang"
			},
			{
				"family": "Ma",
				"given": "Shikui"
			},
			{
				"family": "Liang",
				"given": "Xiaodan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/EFDYUBCM",
		"type": "paper-conference",
		"abstract": "We propose to decompose instruction execution to goal prediction and action generation. We design a model that maps raw visual observations to goals using LINGUNET, a language-conditioned image generation network, and then generates the actions required to complete them. Our model is trained from demonstration only without external resources. To evaluate our approach, we introduce two benchmarks for instruction following: LANI, a navigation task; and CHAI, where an agent executes household instructions. Our evaluation demonstrates the advantages of our model decomposition, and illustrates the challenges posed by our new benchmarks.",
		"container-title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
		"DOI": "10.18653/v1/D18-1287",
		"event-place": "Brussels, Belgium",
		"event-title": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
		"language": "en",
		"page": "2667-2678",
		"publisher": "Association for Computational Linguistics",
		"publisher-place": "Brussels, Belgium",
		"source": "Semantic Scholar",
		"title": "Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction",
		"URL": "http://aclweb.org/anthology/D18-1287",
		"author": [
			{
				"family": "Misra",
				"given": "Dipendra"
			},
			{
				"family": "Bennett",
				"given": "Andrew"
			},
			{
				"family": "Blukis",
				"given": "Valts"
			},
			{
				"family": "Niklasson",
				"given": "Eyvind"
			},
			{
				"family": "Shatkhin",
				"given": "Max"
			},
			{
				"family": "Artzi",
				"given": "Yoav"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/D8WMLA8J",
		"type": "paper-conference",
		"abstract": "- Since most institutions and schools are switching to online or hybrid techniques due to the COVID-19 epidemic, e-learning has been expanding rapidly. With this technology of ours Mahima, we want to give students a real classroom education experience with complete efficiency in a 3D Environment. Classes or learning experiences that take place in unrealistic virtual reality settings are referred to as virtual reality classes. This indicates that the environment is an abstract representation of a real-world object or place rather than a direct simulation of it. These courses or learning opportunities can serve several objectives, including the teaching of abstract scientific or mathematical ideas as well as the promotion of artistic expression and creativity. Students can interact with digital objects and environments in novel ways that would not be possible in a traditional classroom setting, making abstract virtual reality courses very immersive and engaging. The following are some instances of abstract virtual reality courses: An interior-representative virtual reality setting. The primary goal of this research study is to improve the effectiveness of the online educational system, and we will be focusing on the technologies such as Video processing, Natural Language Processing and Artificial Neural Networks, to develop Virtual Reality (VR) Technology and reduce the workload for instructors and students. 3D Avatar Animation Processing is one of the most accurate ways to monitor human behavior. Eye-tracking is a technique for capturing real-time, objective user behavior. Eye motions are quick, subconscious movements communicating information that even the responder is unaware of. Subject and help them utilize their time in an effective manner with less effort while referring to a subject or preparing for the examination",
		"source": "Semantic Scholar",
		"title": "Mahima - Education Purpose Machine Learning Technology",
		"URL": "https://www.semanticscholar.org/paper/Mahima-Education-Purpose-Machine-Learning-Piyashkara-Ahamed/3b876b833b06da80db1a1b8834880b46373ce2c2",
		"author": [
			{
				"family": "Piyashkara",
				"given": "S."
			},
			{
				"family": "Ahamed",
				"given": "M. M."
			},
			{
				"family": "Thuvarakan",
				"given": "P."
			},
			{
				"family": "Gamage",
				"given": "A. I."
			},
			{
				"family": "Chamara",
				"given": "Didula"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2023"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HT2VZTIJ",
		"type": "webpage",
		"title": "Visualisation Approaches in Technology-Enhanced Medical Simulation Learning: Current Evidence and Future Directions. | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/Visualisation-Approaches-in-Technology-Enhanced-and-Dawidziuk-Miller/f8f32ba8832b171ed0b18eccd44c7da0870198a0",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/9TN5NUWW",
		"type": "webpage",
		"title": "[PDF] The NeurIPS '18 Competition: From Machine Learning to Intelligent Conversations | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/The-NeurIPS-'18-Competition%3A-From-Machine-Learning/f73a99c817a22c9aa0c4ffd38d0a1f08d541b2ef",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/68UUR6YU",
		"type": "webpage",
		"title": "[PDF] Twin Research and Human Genetics | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/Twin-Research-and-Human-Genetics-Herskind/042b5f45f014600624a637cb08f97bec8f99abcd",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/X3EK59B9",
		"type": "paper-conference",
		"abstract": "We describe project deLearyous, in which the goal is to develop a proof-of-concept of a serious game that will assist in the training of communication skills following the Interpersonal Circumplex (also known as Leary’s Rose) –a framework for interpersonal communication. Users will interact with the application using unconstrained written natural language input and will engage in conversation with a 3D virtual agent. The application will thus alleviate the need for expensive communication coaching and will offer players a non-threatening environment in which to practice their communication skills. We outline the preliminary data collection procedure, as well as the workings of each of the modules that make up the application pipeline. We evaluate the modules’ performance and offer our thoughts on what can be expected from the final “proof-of-concept” application. To get a firm grasp on the structure and dynamics of human-to-human conversations, we first gathered data from a series of “Wizard of Oz” experiments in which the virtual agent was replaced with a human actor. All data was subsequently transcribed, analysed and annotated. This data functioned as the basis for all modules in the application pipeline: the NLP module, the scenario engine, the visualization module, and the audio module. The freeform, unconstrained text input from the player is first processed by a Natural Language Processing (NLP) module, which uses machine learning to automatically identify the position of the player on the Interpersonal Circumplex. The NLP module also identifies the topic of the player’s input using a keyword-based approach. The output of the NLP module is sent to the scenario engine, which implements the virtual agent’s conversation options as a finite state machine. Given the virtual agent’s previous state and Circumplex position, it predicts the most likely follow-up state. The follow-up state is then realized by the visualization and audio modules. The visualization module takes care of displaying the 3D virtual agent’s facial and torso animations, while the audio module looks up and plays the appropriate pre-recorded audio responses. In terms of performance, the NLP module appears to be a bottleneck, as finding the position of the player on the Interpersonal Circumplex is a very difficult problem to solve automatically. However, we show that human agreement on this task is also very low, indicating that there isn’t always a single “correct” way to interpret Circumplex positions. We conclude by stating that applications like deLearyous show promise, but we also readily admit that technology still has a way to go before they can be used without human supervision.",
		"source": "Semantic Scholar",
		"title": "deLearyous : training interpersonal communication skills using unconstrained text input",
		"title-short": "deLearyous",
		"URL": "https://www.semanticscholar.org/paper/deLearyous-%3A-training-interpersonal-communication-Vaasen-Wauters/bee328fa4d32cb2ed79d08fcdaff76d651ef9de0",
		"author": [
			{
				"family": "Vaasen",
				"given": "F."
			},
			{
				"family": "Wauters",
				"given": "Jeroen"
			},
			{
				"family": "Broeckhoven",
				"given": "Frederik Van"
			},
			{
				"family": "Overveldt",
				"given": "M. V."
			},
			{
				"family": "Daelemans",
				"given": "Walter"
			},
			{
				"family": "Eneman",
				"given": "K."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/XJ2K4RDE",
		"type": "paper-conference",
		"abstract": "In this tutorial, we introduce a computational framework and modeling language (VoxML) for composing multimodal simulations of natural language expressions within a 3D simulation environment (VoxSim). We demonstrate how to construct voxemes, which are visual object representations of linguistic entities. We also show how to compose events and actions over these objects, within a restricted domain of dynamics. This gives us the building blocks to simulate narratives of multiple events or participate in a multimodal dialogue with synthetic agents in the simulation environment. To our knowledge, this is the first time such material has been presented as a tutorial within the CL community.This will be of relevance to students and researchers interested in modeling actionable language, natural language communication with agents and robots, spatial and temporal constraint solving through language, referring expression generation, embodied cognition, as well as minimal model creation.Multimodal simulation of language, particularly motion expressions, brings together a number of existing lines of research from the computational linguistic, semantics, robotics, and formal logic communities, including action and event representation (Di Eugenio, 1991), modeling gestural correlates to NL expressions (Kipp et al., 2007; Neff et al., 2008), and action event modeling (Kipper and Palmer, 2000; Yang et al., 2015). We combine an approach to event modeling with a scene generation approach akin to those found in work by (Coyne and Sproat, 2001; Siskind, 2011; Chang et al., 2015). Mapping natural language expressions through a formal model and a dynamic logic interpretation into a visualization of the event described provides an environment for grounding concepts and referring expressions that is interpretable by both a computer and a human user. This opens a variety of avenues for humans to communicate with computerized agents and robots, as in (Matuszek et al., 2013; Lauria et al., 2001), (Forbes et al., 2015), and (Deits et al., 2013; Walter et al., 2013; Tellex et al., 2014). Simulation and automatic visualization of events from natural language descriptions and supplementary modalities, such as gestures, allows humans to use their native capabilities as linguistic and visual interpreters to collaborate on tasks with an artificial agent or to put semantic intuitions to the test in an environment where user and agent share a common context.In previous work (Pustejovsky and Krishnaswamy, 2014; Pustejovsky, 2013a), we introduced a method for modeling natural language expressions within a 3D simulation environment built on top of the game development platform Unity (Goldstone, 2009). The goal of that work was to evaluate, through explicit visualizations of linguistic input, the semantic presuppositions inherent in the different lexical choices of an utterance. This work led to two additional lines of research: an explicit encoding for how an object is itself situated relative to its environment; and an operational characterization of how an object changes its location or how an agent acts on an object over time, e.g., its affordance structure. The former has developed into a semantic notion of situational context, called a habitat (Pustejovsky, 2013a; McDonald and Pustejovsky, 2014), while the latter is addressed by dynamic interpretations of event structure (Pustejovsky and Moszkowicz, 2011; Pustejovsky and Krishnaswamy, 2016b; Pustejovsky, 2013b).The requirements on building a visual simulation from language include several components. We require a rich type system for lexical items and their composition, as well as a language for modeling the dynamics of events, based on Generative Lexicon (GL). Further, a minimal embedding space (MES) for the simulation must be determined. This is the 3D region within which the state is configured or the event unfolds. Object-based attributes for participants in a situation or event also need to be specified; e.g., orientation, relative size, default position or pose, etc. The simulation establishes an epistemic condition on the object and event rendering, imposing an implicit point of view (POV). Finally, there must be some sort of agent-dependent embodiment; this determines the relative scaling of an agent and its event participants and their surroundings, as it engages in the environment.In order to construct a robust simulation from linguistic input, an event and its participants must be embedded within an appropriate minimal embedding space. This must sufficiently enclose the event localization, while optionally including space enough for a frame of reference for the event (the viewerâ€™s perspective).We first describe the formal multimodal foundations for the modeling language, VoxML, which creates a minimal simulation from the linguistic input interpreted by the multimodal language, DITL. We then describe VoxSim, the compositional modeling and simulation environment, which maps the minimal VoxML model of the linguistic utterance to a simulation in Unity. This knowledge includes specification of object affordances, e.g., what actions are possible or enabled by use an object.VoxML (Pustejovsky and Krishnaswamy, 2016b; Pustejovsky and Krishnaswamy, 2016a) encodes semantic knowledge of real-world objects represented as 3D models, and of events and attributes related to and enacted over these objects. VoxML goes beyond the limitations of existing 3D visual markup languages by allowing for the encoding of a broad range of semantic knowledge that can be exploited by a simulation platform such as VoxSim.VoxSim (Krishnaswamy and Pustejovsky, 2016a; Krishnaswamy and Pustejovsky, 2016b) uses object and event semantic knowledge to generate animated scenes in real time without a complex animation interface. It uses the Unity game engine for graphics and I/O processing and takes as input a simple natural language utterance. The parsed utterance is semantically interpreted and transformed into a hybrid dynamic logic representation (DITL), and used to generate a minimal simulation of the event when composed with VoxML knowledge. 3D assets and VoxML-modeled nominal objects and events are created with other Unity-based tools, and VoxSim uses the entirety of the composed information to render a visualization of the described event.The tutorial participants will learn how to build simulatable objects, compose dynamic event structures, and simulate the events running over the objects. The toolkit consists of object and program (event) composers and the runtime environment, which allows for the user to directly manipulate the objects, or interact with synthetic agents in VoxSim. As a result of this tutorial, the student will acquire the following skill set: take a novel object geometry from a library and model it in VoxML; apply existing library behaviors (actions or events) to the new VoxML object; model attributes of new objects as well as introduce novel attributes; model novel behaviors over objects.The tutorial modules will be conducted within a build image of the software. Access to libraries will be provided by the instructors. No knowledge of 3D modeling or the Unity platform will be required.",
		"event-title": "Conference of the European Chapter of the Association for Computational Linguistics",
		"source": "Semantic Scholar",
		"title": "Building Multimodal Simulations for Natural Language",
		"URL": "https://www.semanticscholar.org/paper/Building-Multimodal-Simulations-for-Natural-Pustejovsky-Krishnaswamy/b78a49b3758eec9440c91da41838d21c47214899",
		"author": [
			{
				"family": "Pustejovsky",
				"given": "J."
			},
			{
				"family": "Krishnaswamy",
				"given": "Nikhil"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017",
					4,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/W2MFH962",
		"type": "article-journal",
		"abstract": "Retrieval by contents of images from pictorial databases can be effectively performed through visual icon-based systems. In these systems, the representation of pictures with 2D strings, which are derived from symbolic projections, provides an efficient and natural way to construct iconic indexes for pictures and is also an ideal representation for the visual query. With this approach, retrieval is reduced to matching two symbolic strings. However, using 2D-string representations, spatial relationships between the objects represented in the image might not be exactly specified. Ambiguities arise for the retrieval of images of 3D scenes. In order to allow the unambiguous description of object spatial relationships, in this paper, following the symbolic projections approach, images are referred to by considering spatial relationships in the 3D imaged scene. A representation language is introduced that expresses positional and directional relationships between objects in three dimensions, still preserving object spatial extensions after projections. Iconic retrieval from pictorial databases with 3D interfaces is discussed and motivated. A system for querying by example with 3D icons, which supports this language, is also presented. >",
		"container-title": "IEEE Transactions on Software Engineering",
		"DOI": "10.1109/32.245741",
		"ISSN": "00985589",
		"issue": "10",
		"journalAbbreviation": "IIEEE Trans. Software Eng.",
		"page": "997-1011",
		"source": "Semantic Scholar",
		"title": "A three-dimensional iconic environment for image database querying",
		"URL": "http://ieeexplore.ieee.org/document/245741/",
		"volume": "19",
		"author": [
			{
				"family": "Del Bimbo",
				"given": "A."
			},
			{
				"family": "Campanai",
				"given": "M."
			},
			{
				"family": "Nesi",
				"given": "P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1993",
					10
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7AJHC9UN",
		"type": "paper-conference",
		"abstract": "Invited Papers.- From AI to Systemic Knowledge Management.- MINERVA: A Tour-Guide Robot that Learns.- Dynamics, Morphology, and Materials in the Emergence of Cognition.- Natural Language Description of Image Sequences as a Form of Knowledge Representation.- Knowledge Discovery in Spatial Databases.- Cooperative Distributed Vision: Dynamic Integration of Visual Perception, Action, and Communication.- Technical Papers-Section 1.- Computing Probabilistic Least Common Subsumers in Description Logics.- Revising Nonmonotonic Theories: The Case of Defeasible Logic.- On the Translation of Qualitative Spatial Reasoning Problems into Modal Logics.- Following Conditional Structures of Knowledge.- Section 2.- A Theory of First-Order Counterfactual Reasoning.- Logic-Based Choice of Projective Terms.- Knowledge Based Automatic Composition and Variation of Melodies for Minuets in Early Classical Style.- Inferring Flow of Control in Program Synthesis by Example.- Section 3.- Compilation Schemes: A Theoretical Tool for Assessing the Expressive Power of Planning Formalisms.- Generalized Cases: Representation and Steps Towards Efficient Similarity Assessment.- Be Busy and Unique - or Be History-The Utility Criterion for Removing Units in Self-Organizing Networks.- Section 4.- Development of Decision Support Algorithms for Intensive Care Medicine: A New Approach Combining Time Series Analysis and a Knowledge Base System with Learning and Revision Capabilities.- Object Recognition with Shape Prototypes in a 3D Construction Scenario.- Probabilistic, Prediction-Based Schedule Debugging for Autonomous Robot Office Couriers.- Section 5.- Collaborative Multi-robot Localization.- Object Classification Using Simple, Colour Based Visual Attention and a Hierarchical Neural Network for Neuro-symbolic Integration.- A Flexible Architecture for Driver Assistance Systems.- Short Papers.- A Theory for Causal Reasoning.- Systematic vs. Local Search for SAT.- Information Environments for Software Agents.- Improving Reasoning Efficiency for Subclasses of Allen's Algebra with Instantiation Intervals.- Agents in Traffic Modelling - From Reactive to Social Behaviour.- Time-Effect Relations of Medical Interventions in a Clinical Information System.",
		"collection-title": "Lecture Notes in Computer Science",
		"DOI": "10.1007/3-540-48238-5",
		"event-place": "Berlin, Heidelberg",
		"ISBN": "978-3-540-66495-6",
		"language": "en",
		"note": "DOI: 10.1007/3-540-48238-5",
		"publisher": "Springer Berlin Heidelberg",
		"publisher-place": "Berlin, Heidelberg",
		"source": "Semantic Scholar",
		"title": "KI-99: Advances in Artificial Intelligence: 23rd Annual German Conference on Artificial Intelligence Bonn, Germany, September 13–15, 1999 Proceedings",
		"title-short": "KI-99",
		"URL": "http://link.springer.com/10.1007/3-540-48238-5",
		"volume": "1701",
		"editor": [
			{
				"family": "Burgard",
				"given": "Wolfram"
			},
			{
				"family": "Cremers",
				"given": "Armin B."
			},
			{
				"family": "Cristaller",
				"given": "Thomas"
			}
		],
		"collection-editor": [
			{
				"family": "Goos",
				"given": "G."
			},
			{
				"family": "Hartmanis",
				"given": "J."
			},
			{
				"family": "Van Leeuwen",
				"given": "J."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1999"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/3MCMB8F5",
		"type": "paper-conference",
		"abstract": "Computer vision has made significant progress in locating and recognizing objects in recent decades. However, beyond the scope of this “what is where” challenge, it lacks the abilities to understand scenes characterizing human visual experience. Comparing with human vision, what is missing in current computer vision? One answer is that human vision is not only for pattern recognition, but also supports a rich set of commonsense reasoning about object function, scene physics, social intentions etc..I build systems for real world applications and simultaneously pursuing a long-term goal of devising a unified framework that can make sense of an images and a scene by reasoning about the functional and physical mechanisms of objects in a 3D world. By bridging advances spanning fields of stochastic learning, computer vision, cognitive science, my research tackles following challenges: (i) What is the visual representation? I develop stochastic grammar models to characterize spatiotemporal structures of visual scenes and events. The analogy of human natural language lays a foundation for representing both visual structure and abstract knowledge. I pose the scene understanding problem as parsing an image into a hierarchical structure of visual entities using the Stochastic Scene Grammar (SSG). With a set of production rules, the grammar enforces both structural regularity and flexibility of visual entities. Therefore, the algorithm is able to handle enormous number of configurations and large geometric variations for both indoor scenes and outdoor scenes. (ii) How to reason about the commonsense knowledge? I augment the commonsense knowledge about functionality, physical stability to the grammatical representation. The bottom-up and top-down inference algorithms are designed for finding a most plausible interpretation of visual stimuli. Functionality refers to the property of an object or scene, especially man-made ones, which has a practical use for which it was designed, and it's deeper than geometry and appearance and thus is a more invariant concept for scene understanding. We present a Stochastic Scene Grammar (SSG) as a hierarchical compositional representation which integrates functionality, geometry and appearance in a hierarchy. This represents a different philosophy that views vision tasks from the perspective of agents, that is, agents (humans, animals and robots) should perceive objects and scenes by reasoning their plausible functions. Physical stability assumption assumes objects in the static scene should be stable with respect to the gravity field. In other words, if any object is not stable on its own, it must be either grouped with neighbors or fixed to its supporting base. We pursue a physically stable scene understanding, namely ``a parse tree , by inferring object stability in the physical world. The assumption is applicable to general scene categories thus poses powerful constraints for physically plausible scene interpretation and understanding.(iii) How to acquire commonsense knowledge? I performed three case studies to acquire different kinds of commonsense knowledges: I teach the computer to learn affordance from observing human actions; to learn tool-use from single one-shot demonstration; and to infer containing relations by physical simulation without explicit training process. They provided some interesting perspectives on how to acquire and exploit commonsense knowledge. In general, the more prediction or simulation is performed, the less training data is needed. As a result, the acquired commonsense knowledge is more generalizable to new situations.Such sophisticated understanding of 3D scenes enables computer vision to reason, predict, interact with the 3D environment, as well as hold intelligent dialogues beyond visible spectrum.",
		"source": "Semantic Scholar",
		"title": "A Quest for Visual Commonsense: Scene Understanding by Functional and Physical Reasoning",
		"title-short": "A Quest for Visual Commonsense",
		"URL": "https://www.semanticscholar.org/paper/A-Quest-for-Visual-Commonsense%3A-Scene-Understanding-Zhao/48d8efdce087557ef9a25329bf3348e61aad1d89",
		"author": [
			{
				"family": "Zhao",
				"given": "Yibiao"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/X36H5KY5",
		"type": "paper-conference",
		"abstract": "The literature abounds with information about peer tutoring and the benefits that it can bring to student learning. This case study sought to explore ways of using peer tutoring to enhance the learning experience of a group of higher education students in a multimedia course, who had access to learning resources in an online environment. It illustrates how easily and effectively the basic principles of peer tutoring can be adapted and implemented following explicit guidelines from the literature. (Contains 18 references, 1 figure, and 1 table.) (Author) Reproductions supplied by EDRS are the best that can be made from the original document. 1 Promoting Student Learning through Peer Tutoring A Case PERMISSION TO REPRODUCE AND DISSEMINATE THIS MATERIAL HAS BEEN GRANTED BY G.-R.Marks Joe Luca, Edith Cowan University, Multimedia Department, Australia j.luca@cowan.edu.au Barney Clarkson, Edith Cowan University, Multimedia Department, Australia b.clarkson@cowan.edu.au Study U.S. DEPARTMENT OF EDUCATION Office of Educational Research and Improvement ' EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC) .40This document has been reproduced as received from the person or organization originating it. Minor changes have been made to improve reproduction quality. TO THE EDUCATIONAL RESOURCES INFORMATION CENTER (ERIC) Abstract: The literature abounds with information about peer tutoring and the benefits that it can bring to student learning. This case study sought to explore ways of using peer tutoring to enhance the learning experience of a group of higher education students in a multimedia course, who had access to learning resources in an on-line environment. It illustrates how easily and effectively the basic principles of peer tutoring can be adapted and implemented following explicit guidelines from the literature. The literature abounds with information about peer tutoring and the benefits that it can bring to student learning. This case study sought to explore ways of using peer tutoring to enhance the learning experience of a group of higher education students in a multimedia course, who had access to learning resources in an on-line environment. It illustrates how easily and effectively the basic principles of peer tutoring can be adapted and implemented following explicit guidelines from the literature. Introduction Points of view or opinions stated in this document do not necessarily represent official OERI position or policy. It is ironic that schools and tertiary institutions are often chided for not providing a real world experience for students, yet they can provide a perfectly realistic learning environment for students to tutor others. This permits authentic practice of a number of useful generic skills like working collaboratively with peers, which can enhance teamwork and interpersonal skills. This study presents support for using peer tutoring and peer assessment for students in higher education. After all, evidence suggests that peer tutoring can greatly enhance the learning experience of both the student tutor and learner(Goodlad, 1999; Topping, 1996). At a time when there is a push for higher education institutions \"to do more with less\" and promote the development of students' generic skills (Australian National Training Authority, 1998; Bennett, Dunne, & Carre, 1999; Candy, Crebert, & O'Leary, 1994; Dearing, 1997; Mayer, 1992), peer tutoring can provide an effective system which not only assists student tutors and tutees to learn better, but also helps promote the development of generic skills, as well as freeing up time for tutors (Topping, 1996). This provides an alternative teaching and learning approach in which students take a pro-active role in thinking, questioning and sharing knowledge. In this paper, we examine design issues needed when implementing a teaching program using peer learning, and also present the results of our evaluation. We begin by considering some theoretical underpinnings and design aspects of implementing peer tutoring and learning. Peer Tutoring theory and design aspects The concept of learning through peer tutoring is based on a social constructivist view of learning that emphasises the role of the students to generate learning where students coach peers through social interaction within their zones of proximal development (Vygotsky, 1978). Rather than applying a stimulus/response process, users are actively engaged in making meaning through cognitive accommodation and/or assimilation (Piaget, 1969). Vygotsky argued that learning comes about through social negotiation within a cultural context, with language as the primary enabling tool. This social constructivist philosophy has been expanded on recently, introducing the notion of cognitive apprenticeship (Brown, Collins, & Duguid, 1989) through which students learn in a manner similar to traditional apprenticeships. The students access expertise through mentors, whose role is to facilitate rather than teach, and the aim of learning is to solve realistic and practical problems in an authentic setting. For a peer tutor, this setting is a very realistic human setting. Just as in traditional apprenticeships, learners engage in activities 'on-the-job' rather than through the didactic teaching of abstract concepts. The argument is that students are better equipped to approach nonfamiliar problems and produce solutions that are appropriate to a given culture. Peer tutoring is aligned with these aspects of social constructivist theory by enhancing social negotiation with the student tutor and tutee, where knowledge construction is promoted through communication and dialogue, which is helpful for the tutees. 2 6. EST COPY MAR LA Peer tutoring is also valuable for the tutor, ie \"learning is enhanced through teaching\". In an evaluation study conducted by Hartman (1990), a reported outcome of peer tutoring was an increase in student motivation toward learning. These results are supported by Whitman (1982), Annis (1983) and Benware & Deci (1984) who argue that peer tutoring can be the most intellectually rewarding experience of a student's career, and that they perform better on higher order conceptual understanding scales than students who read the material simply for study purposes. The benefits of peer tutoring are summarised by Good lad (1999) as follows: Student tutees found lessons more interesting, easier to follow, more enjoyable and seemed to learn more; Student tutors practiced communication skills, felt that they were doing something useful with their knowledge, got to know people from different social backgrounds, gained insights into how other students saw subjects, increased self-confidence and reinforced subject knowledge; Teachers found lessons easier to handle, teaching was more enjoyable and reported that pupils seemed to learn more. Implementing a peer-tutoring program is not a trivial process, as there are many salient issues. Topping (1996) describes nine different peer tutoring formats suiting different circumstances ie cross-year small-group tutoring, a personalised system of instruction, supplemental instruction, same year dyadic fixed-role tutoring, same year dyadic reciprocal peer tutoring, dyadic cross year fixed-role tutoring, same year group tutoring, peer assisted writing and peer assisted distance learning. In our case study, the personalised system of instruction was akin to type seven, ie implemented at the same year group level, where tutors assisted tutees who were working at their own pace on set exercises. Even though different formats meet different needs, there is a commonality of purpose as well. Goodlad (1999) lists seven \"golden rules\" as criteria for designing and implementing peer-tutoring schemes. His criteria are: 1. Clearly define the aims of the tutoring scheme by writing a statement of intent which shows \"who is teaching what to whom and for what purpose\"; 2. Define roles and responsibilities in the scheme being implemented, which may include rules for matching or pairing students by sex, friendship or ethnicity; 3. Train the tutors in task/content requirements and also in tutoring techniques such as \"pause, prompt and praise\"; 4. Structure the content so that there are clearly defined, meaningful tasks for the tutees which involve maximum participation and reinforcement; 5. Support the tutors with regular feedback through de-briefing sessions and well structured materials; 6. Keep logistics as simple as possible ie make the scheduled time and space for meetings convenient to all parties; 7. Evaluate the scheme.",
		"source": "Semantic Scholar",
		"title": "Promoting Student Learning through Peer Tutoring A Case PERMISSION TO REPRODUCE AND DISSEMINATE THIS MATERIAL HAS BEEN GRANTED BY G.-R.Marks",
		"URL": "https://www.semanticscholar.org/paper/Promoting-Student-Learning-through-Peer-Tutoring-A-Luca-Cowan/59c50a0fa02e8b4ca611235600e1f293ea3c93af",
		"author": [
			{
				"family": "Luca",
				"given": "J."
			},
			{
				"family": "Cowan",
				"given": "E."
			},
			{
				"family": "Clarkson",
				"given": "B."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JK2V57LM",
		"type": "paper-conference",
		"abstract": "LEA (Learning Environments Agent) is a web-based software system for advanced multimedia and virtual-reality education and training. LEA consists of three fully integrated components: (1) unstructured knowledge-base engine for lecture delivery; (2) structured hierarchical process knowledge-base engine for step-by-step process training; and (3) hierarchical rule-based expert system for natural-language understanding. In addition, LEA interfaces with components which provide the following capabilities: 3D near photo-realistic interactive virtual environments; 2D animated multimedia; near-natural synthesized text-to-speech, speech recognition, near-photorealistic animated virtual humans to act as instructors and assistants; and socket-based network communication. LEA provides the following education and training functions: multimedia lecture delivery; virtual-reality based step-by-step process training; and testing capability. LEA can deliver compelling multimedia lectures and content in science fields (such as engineering, physics, math, and chemistry) that include synchronized: animated 2D and 3D graphics, speech, and written/highlighted text. In addition, it can be used to deliver step-by-step process training in a compelling near-photorealistic 3D virtual environment. In this paper the LEA system is presented along with typical educational and training applications.",
		"container-title": "Volume 3: 26th Computers and Information in Engineering Conference",
		"DOI": "10.1115/DETC2006-99292",
		"event-place": "Philadelphia, Pennsylvania, USA",
		"event-title": "ASME 2006 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference",
		"ISBN": "978-0-7918-4257-7",
		"page": "403-412",
		"publisher": "ASMEDC",
		"publisher-place": "Philadelphia, Pennsylvania, USA",
		"source": "Semantic Scholar",
		"title": "LEA: Software System for Multimedia and Virtual-Reality Web-Based Education and Training",
		"title-short": "LEA",
		"URL": "https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings/IDETC-CIE2006/42578/403/317860",
		"author": [
			{
				"family": "Wasfy",
				"given": "Tamer M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2006",
					1,
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6V3D8RZH",
		"type": "webpage",
		"title": "[PDF] Cross-Language Interfacing and Gesture Detection with Microsoft Kinect | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/Cross-Language-Interfacing-and-Gesture-Detection-Kieu/8ec56c3b91e96f59a91293e7b6d1522d347fad18",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/PZ4S8UUK",
		"type": "webpage",
		"title": "[PDF] An Environment for Deaf Accessibility to Educational Content | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/An-Environment-for-Deaf-Accessibility-to-Content-Efthimiou-Fotinea/4bc94cc2d0c77ee07f9c326328d7eaa4db215846",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6GPDBZ7H",
		"type": "webpage",
		"title": "Service Manual Generation (SMG) | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/Service-Manual-Generation-(SMG)-Wampler/9d0cbd79d6b6176d1be6780892d984c3f7d3191d",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/TQN9YRRL",
		"type": "paper-conference",
		"abstract": "In the field of architecture, tangible virtual reality interfaces allow architects to design and construct large complex structures in a three-dimensional space, and interact with the 3D models using the most natural means of computer human interaction: the two-handed system. The main goal of this project is to design and create an interactive, tangible, virtual reality interface for constructing various kinds of bridges. This project involves constructing basic elements of a bridge and integrating these elements with the help of the existing tangible tools so that the user can conveniently interact with the building elements and design a broad variety of bridge structures by using the provided primitives. The user is provided with some physical tools that are often seen in our life to interact with the digital objects. He can use a tong to grab and therefore select any bridge component and move it, a handle to place a copy of a selected component in space, and a ray gun to create cables in a bridge or change the shape of the cables. All of the bridge elements and functions of the tools are built using the principle of object-oriented programming. Introduction Computers have been major tools for designers and engineers from all kinds of industries to construct models of complex objects and interact with these models to predict the quality of their designs. As chip designers in Silicon Valley are making painstaking efforts to develop faster but cheaper chips to maximize the utility of computers, traditional computer interfaces can no longer satisfy the great demand for a more convenient user-interactive interface mechanism. With the 30 years old Keyboard-Mouse-Monitor interface and the Window-Icon-Menu-Pointer interaction metaphor, the designers have little freedom to play with these objects in different angles therefore creativity is seriously limited [8]. What the users really need for object modeling is an interface with which they can freely construct objects in space and interact with these constructed visual objects in the same way as they interact with real objects. Instead, tangible virtual reality interfaces for creating three-dimensional shapes offer a natural means of human computer interaction and provide the users a far more intuitive way than the traditional two-dimensional interfaces in designing and assembling complex objects. A tangible interface is one that allows the user to use some physical tools to grasp and manipulate digital information. Via the use of physical tools such as tongs and ray gun, the motions of the user’s two hands become the input of the computer program [2]. The idea of tangible user interfaces first originated from Hiroshi Ishii from the Tangible Media Group at the MIT Media Lab. He and his team established their own “vision of Human Computer Interaction (HCI): ‘Tangible Bits’” and are seeking to “change the ‘painted bits’ of graphical user interface into ‘tangible bits’ by giving physical form to digital information.”[2]. Tangible user interfaces have many applications in various industries. In their paper “Evolution of the talking dinosaur: the (not so) natural history of a new interface for children”, Kristin Alexander and Erik Strommen from Microsoft present an example of tangible interactive tools as toys for children, ActiMates Barney: a 13” animated doll that can interact with children via touch sensors in its hands and feet and a light sensor in its left eye [1]. In the field of education, the invention of AlgoBlock allows students to compose computer programs without understanding the complex programming language syntax, instead using some physical blocks connected with computers as programming input [3]. In Biology, tangible interfaces can be used to navigate a 3D model of the human body to serve research purposes. Figure 1: Various examples of tangible user interface. Top: the interface doll ActiMates Barney that can function as a playmate for children [1]. Bottom: the new learning tool called ArgoBlock which enables the students to learn how to construct computer programs without knowing any programming language [3]. Dr. Schröder and his research group in the Computer Science Department at Caltech have recently deployed a semi-immersive head-tracked stereoscopic display, which can be used as a virtual reality interface for spatial construction. In this virtual interactive environment, the physical motions of the user are mapped to interface commands through tangible props, and objects can be constructed and manipulated directly in 3D space [5]. The physical tools built in the existing interface in the lab, indicated in Figure 2, include a tong, a ray gun, a light saber and a glove. Figure 2: The tools used in our interface, including a tong, a ray gun and a light saber, are illustrated. The glove is a part of the existing interface in the lab that can be used to draw any free shape in space. [7] Left: the black tool on the left most is a tong, which is used for grabbing and moving objects freely in space. The ray gun is the one in the middle, which is mainly used for binding objects together. And the light saber on the right most is used for breaking these bonds [7]. Right: illustrates the use of a glove to enable the user to draw any shape of his choice n space [7] The current work in the lab has been focused on developing a 3D interface for spatial construction for DNA molecules and complexes. The idea of applying the tangible virtual reality interfaces to architectures has just been developed recently and no such interface has been developed before in this lab. The scope of my project was to design and create a similar tangible virtual reality 3D interface for constructing complex architectural structures, focusing on various kinds of bridges. The interface is called 3D Bridge Construction Interface and is based on the existing interface in the lab. The main objective of this project was to construct basic elements of a bridge and integrate the tools and objects, so that the user can conveniently interact with them and design a broad variety of complex bridge structures by using the provided primitives. The final interface enables the user to create mainly three kinds of bridge: cable-stayed bridges, beam bridges, and suspension bridges. The user can easily perform various types of adjustments and modifications on the resulting bridge structure and view the bridge from any angle to have a much better understanding of their design. It provides the architecture designers a much more intuitive way to interact with the bridge models and more flexibility to make changes in their bridge models than the traditional 2D interface. The Interface The 3D Bridge Construction Interface provides the user a variety of common primitives of bridges, including pier, beam, anchor, cable, main tower, etc. The user is able to use the tangible tools to select their desired components to design and build bridges. These tangible tools used in the 3D Bridge Construction Interface are those described above: two tongs, a glue gun and a light saber. However, additional functions are added to these physical tools for a better adaptation to their use in the construction of bridges. Most of the solid bridge components, such as beam, pier, anchor, main tower, can be placed in space by allowing the user to drop copies of these components with the light saber. As a result, the light saber becomes a bridge dropper handle. The bridge dropper handle, when triggered by pressing a menu button on the handle, displays a menu of various bridge components and allows the user to select their desired component. After a selection is made, although not physically shown, the handle is then attached to the selected bridge component. Pressing the main button on the handle enables the user to place a copy of that component in space. Other string type components, such as the main cable and supporting cables of a cable-stayed bridge or suspension bridge, can be drawn in space with a ray gun. When the user presses the main button of the gun, a ray comes out of the gun. The user can drag the tip of the ray between two other components of a bridge, such as two anchors, therefore draw the cable from its start position to its ending position. Figure 3 shows how a straight cable is formed. Step 1: The user uses the tip of the ray to specify the starting position of the cable. Step 2: The user drags the tip of the ray to the desired ending position of the cable Step 3: A cable is then built with its specified starting and ending positions. Figure 3: A cable between two anchors is formed by dragging the tip of a ray from its starting position to its ending position. After a component is placed in space, the user can use the available tools to edit the properties of the component. Tongs enable the user to move a single component or a group of components in space, or change the dimensions of each component. The trigger button on each tong displays its various functions which the user wants to use. If the user selects the mover function, using a tong to grab any component close to the tong attaches that component to the tong, then moving the tong to any desired position moves the attached component to the same position as well. If a number of components are grouped together, using the tong in the same way as moving a single component results in moving the whole group of components. The following figure shows how an object is moved using the Tong. Figure 4: Use a tong to grab an object and move it in space [7]. If the user selects the stretcher function, using the tong to grab its nearest bridge component in space results in grabbing the part of a component to which the tong is closest, and moving the tong changes the corresponding dimension of that component. For example, placing the tong on the top of a pier, pressing the tong and moving it upward increases the height of the pier. The following graph illustrates how t",
		"source": "Semantic Scholar",
		"title": "Bridge Construction Author : Yuan Xie Mentor :",
		"title-short": "Bridge Construction Author",
		"URL": "https://www.semanticscholar.org/paper/Bridge-Construction-Author-%3A-Yuan-Xie-Mentor-%3A-Schr%C3%B6der-Schkolne/a3c92aadaa230cd6706dccd1389136eb36cff3a3",
		"author": [
			{
				"family": "Schröder",
				"given": "Dr P."
			},
			{
				"family": "Schkolne",
				"given": "Steven"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2003"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/P6ZDISXN",
		"type": "paper-conference",
		"abstract": "This paper presents a project developed by the authors at Institute of Advanced Studies – United Nations University, Tokyo. It gives steps on how to develop a web based virtual tour using virtual reality technology by the use of technologies like HTML, QTVR and VRML. It also gives the situation of a real work developed, giving solutions and warnings during a development like this. 1. What is Virtual Reality? Virtual Reality is a technology that has been under development during last 40 years, since the publication of the paper entitled Ultimate Display [4] by E. Sutherland, on early 60’s. It has different definitions presented by several authors [2][6][9]. The initial uses of VR were on the military, medical and scientific fields for purpose of training and dynamics visualization [2]. Nowadays VR is applied on several fields of knowledge, and it could already be found over the Internet [10]. The VR definition used on this work was based on the following: Virtual Reality is a technology to interface man and computer, using interactive environments that use tri-dimensional computer graphics, sound, video and other media. 2. What is IAS Virtual Tour? On mission of the Institute of Advanced Studies (IAS) at United Nations University/Tokyo is affirmed: “...it (IAS) shall serve as a bridge between the scientific and scholarly communities in Japan and in other parts of the world”. As this part of mission, IAS brings researchers from all around the world to work on site, at Tokyo. Based on this, the IAS Virtual Tour was created to provide advanced information on IAS facilities to those researchers. This tour will provide information on several aspects, presenting all facilities and data related to IAS and its projects. 3. Applied Technologies This tour was based on Virtual Reality as a mean to provide a more natural interface on navigation of the building. On this tour were applied two different Virtual Reality technologies, VRML [10] and QTVR [3]. • Virtual Reality Modeling Language or VRML is a computer language created to provide VR over the World Wide Web – WWW. It was initially started on 1994, with its initial version VRML 1.0 released on 1995. Its last version is dated 1997, VRML97. VRML allows creating interactive virtual worlds. This technology can integrate different sorts of technologies as most common HTML and JAVA. A VRML file can embed links to different media formats such as video, sound, pictures, and text. • QuickTime VR is a photographic based technology developed by Apple Computers to recreate the reality. This kind of VR technology is more realistic because the environment is presented through real pictures. The differences between VRML and QTVR are: VRML QTVR Real Time 3D Yes No Realism X Cost High Low Development Time Long Short Interactivity More possibilities Less possibilities Movement Free Restricted Requires 3D acceleration Yes No Table 1. Differences between VRML and QTVR The use of these technologies depends on the kind of the project that will be developed, considering at least the aspects of the table above. On the IAS Virtual Tour project, QTVR was used to show all the facilities in a generic tour, and VRML was used to show the most important parts of the building, in detail, on an interactive environment. 4. IAS Tour Project Design A virtual reality project doesn’t differ too much from a software project [9]. For both cases, the project has different phases [5] to be accomplished. These phases could be grouped as: 1) Define Requirements: On this phase all the requirements related to the project must be set to achieve its conceptual integrity. This could be reached through many aspects. Some of these aspects are: • What are the objectives? • Who is the target user? In addition, which are his/her aptitudes, motiva tions and necessities? • What is the estimate development time and cost? • What will be accessed? • Why it will be accessed? 2) Design System: At this point, the system will be described in details to allow the next phase, its implementation. On this phase the following steps should be considered: • What is the content? • Who will develop the project? • How the system will be accessed by the end-user? • What is needed to develop the project? (Media creation, editing, etc.) • What platform for this project will be developed? (Windows, Unix, WWW) • Is this project part of another one? (Definition of interfaces) 3) System Implementation: Since everything is already set and determined, this is the project implementation time. Some tasks that are listed bellow are developed at this time: • Create the media content; • Adapt the media to the project requirements; • Implement the code; 4) System Test: This phase exists to compare the final product to the initial requirements. Either at this time, all system functionality is tested. If the final product differs from the requirements, all the process should be revised and modifications must be done. The idea to create this tour was to introduce the Virtual Reality technology on IAS community and open a door to use it on education and training. By the other hand will be a helpful source of information about the facilities to the Institute’s newcomers. This project was a little bit different of the usual projects, because the technologies and the distribution media were initially known. This occurred by the need of low cost distribution and worldwide audience. The IAS Virtual Tour followed some of the steps described above. The first step of the project was objective’s definition. With the objectives in mind, the following step was to find out audience and content. After this steps defined, the project took off. The basic tour was defined as QTVR oriented. The significant points were identified on the project’s requirement phase and with these locations in mind, all the work was done. Some points (nodes) of main importance were marked as needed independently of any difficulty. Due to the importance of these nodes, VRML were also developed allowing more explorative and interactivity freedom [7]. The use of VRML should be considered due to the fact of it s superior cost over QTVR technology. As the nodes were identified, the tour was adapted to involve and reach each of them. Some nodes were created just as connectors such as the elevators, with a very low importance on showing something. It is also important to define the user interaction with the environment. VRML allows user to interact with virtual objects in a much more enhanced manner than QTVR. 5. How the VRML files of this project were created To create a VRML file requires the following steps: • Plan all the site that will be covered; • Collect all the data referred to the objects that will be represented: o If the object or scene includes a building, the best way is to get the blueprints, plans and all data related to dimensions and materials used; o Other kind of objects, the best way is to get manuals, pictures, and measures (time consuming task). • Create a three-dimensional model of the objects and scenes that will be on the virtual environment: o Use any software that could help on this activity, such as 3D Studio Max, AutoCAD, Maya and SoftImage;(was used 3D Studio Max) o Do the models on an optimized way, as shown on the wire-frame example on figure 1 and 2. Figure 1. Non optimized Figure 2. Optimized • Create textures to put on the virtual objects to give more realism (see figures 3 and 4): o Take pictures of the object on different angles (as much as possible); o Edit them with a photo editor software such as Adobe PhotoShop, Corel PhotoPaint and PaintShop Pro;(was used PhotoShop) o Optimize the textures. Figure 3. 3D model without texture Figure 4. Textured 3D model • Apply the textures on the scenes through the modeler editor; o Do the adjusts to fit it on the object; o Fine tuning of the textures size. • Generate VRML world: o Inside the modeler editor, create the VRML file. • Interactions and VRML fine tuning [10]: o Edit it on a VRML editor (was used Cosmo Worlds) or on a ASCII editor; o Incorporate videos, sounds, links and other media; o Provide all the functions that it should have such as translations, touches, animations, rotations and more. • Test: o Using the VRML target player, do tests to check integrity and functionality. 6. How the QTVR files of this project were created • Plan all the site that will be covered; o Create a table to do the panorama. This table is has the following it ems: § OK: checked if the node’s shooting is finalized with all requirements achieved; § Time & Date: time and date of shooting point; § F.N.: Folder name of the node; § Description: description of the physically location of the node; § Detail: problems on shooting, e.g., strong light, dynamical location; § W: location of the node that could be: internal, view of outside and external. This affects shooting time. OK Time & Date F. N. Description Detail W OK 12:30 – 16/11/1999 001B3F Hall – Elevators None In OK 13:00 – 16/11/1999 002B3F Corridor – front hall None In OK 12:00 – 18/11/1999 008F1 Right Entrance Vw OK 12:20 – 18/11/1999 009F1 Right Side Street Ext Table 2. Example of the QTVR panorama-shooting table • Calculate how long will take the photo taking operation; o Depends on the situation [1]: movement on the scene, light, number of shots, localization, and distance between nodes (location), battery availability (photographic equipment), and data storage availability. All these factors contribute to the task’s duration. • Create the panoramas pictures: o Using the software The VR Worx [12] to Windows system, create the panorama (figure 5) and export it to a bitmap file, at the time, the only software available to Windows Platform; o If necessary, use a photo editor to retouch the panorama picture. The most common actions are to align some objects (figure 6), erase some shadows (figure 7), and lens flare (figure 8); Figure 5. Panorama with problems Figure 6. Misalignment F",
		"source": "Semantic Scholar",
		"title": "Institute of Advanced Studies Virtual Tour",
		"URL": "https://www.semanticscholar.org/paper/Institute-of-Advanced-Studies-Virtual-Tour-Luz/0e96634b9bc826ffe817370b6b515db81c2baa7d",
		"author": [
			{
				"family": "Luz",
				"given": "R. A. D."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2000"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/S4QVWTLF",
		"type": "paper-conference",
		"abstract": "One problem in current help systems for programming utility packages is their inability to provide information within the context of the task at hand. The relationship of a user's goal to the user's plan to accomplish that goal is not taken into account. Furthermore, even a goal/plan based help system should be informative by following rules of discourse. This paper puts established rules of discourse into the context of a programming environment, and describes how a report based on an analysis of a user's intended goal and stated plan can be generated. The programming environment under discussion is a programmer's tool kit for graphically exploring complex hierarchical data structures. A program that we have developed called the Plan Analyst is described that finds the relationship between a users intended goal and stated plan by mapping the goal to the functions of the tool kit. The report generated is informative because discourse rules have been in coded in the knowledge representation that is searched by the Plan Analyst. The intent of this paper is to demonstrate the potential for applying methods of discourse behavior from Natural Language Processing research to 3D interactive programming environment.",
		"DOI": "10.7916/D8FR04MC",
		"publisher": "Columbia University",
		"source": "Semantic Scholar",
		"title": "Analyzing User Plans to Produce Informative Responses by a Programmer's Consultant",
		"URL": "https://academiccommons.columbia.edu/doi/10.7916/D8FR04MC",
		"author": [
			{
				"family": "Wolz",
				"given": "Ursula"
			}
		],
		"contributor": [
			{
				"literal": "Columbia University. Computer Science"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"1985"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/HHJCAUDS",
		"type": "paper-conference",
		"abstract": "A comprehensive introduction to machine learning that uses probabilistic models and inference as a unifying approach. Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package—PMTK (probabilistic modeling toolkit)—that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students. The phenomenon of the small world, as in two new acquaintances discovering that they have an acquaintance in common, is of great scientific interest. The samll world phenomenon appears to be a fundamental property of social structure and function. Understanding it, its origin, and its implications can shed light on problems in sociology, sociometrics, political science, social psychology, and anthropology. This volume brings together much of what is understood about the small world problem, and the chapters indicate the quality, vitality, and scope of this area. A look at the rebellious thinkers who are challenging old ideas with their insights into the ways countless elements of complex systems interact to produce spontaneous order out of confusion Discover New Methods for Dealing with High-Dimensional Data A sparse statistical model has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations presents methods that exploit sparsity to help recover the underlying signal in a set of data. Top experts in this rapidly evolving field, the authors describe the lasso for linear regression and a simple coordinate descent algorithm for its computation. They discuss the application of l1 penalties to generalized linear models and support vector machines, cover generalized penalties such as the elastic net and group lasso, and review numerical methods for optimization. They also present statistical inference methods for fitted (lasso) models, including the bootstrap, Bayesian methods, and recently developed approaches. In addition, the book examines matrix decomposition, sparse multivariate analysis, graphical models, and compressed sensing. It concludes with a survey of theoretical results for the lasso. In this age of big data, the number of features measured on a person or object can be large and might be larger than the number of observations. This book shows how the sparsity assumption allows us to tackle these problems and extract useful and reproducible patterns from big datasets. Data analysts, computer scientists, and theorists will appreciate this thorough and up-to-date treatment of sparse statistical modeling. The rapid conversion of land to urban and suburban areas has profoundly altered how water flows during and following storm events, putting higher volumes of water and more pollutants into the nation's rivers, lakes, and estuaries. These changes have degraded water quality and habitat in virtually every urban stream system. The Clean Water Act regulatory framework for addressing sewage and industrial wastes is not well suited to the more difficult problem of stormwater discharges. This book calls for an entirely new permitting structure that would put authority and accountability for stormwater discharges at the municipal level. A number of additional actions, such as conserving natural areas, reducing hard surface cover (e.g., roads and parking lots), and retrofitting urban areas with features that hold and treat stormwater, are recommended. The first two chapters of this frequently cited reference provide background material in Riemannian geometry and the theory of submanifolds. Subsequent chapters explore minimal submanifolds, submanifolds with parallel mean curvature vector, conformally flat manifolds, and umbilical manifolds. The final chapter discusses geometric inequalities of submanifolds, results in Morse theory and their applications, and total mean curvature of a submanifold. Suitable for graduate students and mathematicians in the area of classical and modern differential geometries, the treatment is largely self-contained. Problems sets conclude each chapter, and an extensive bibliography provides background for students wishing to conduct further research in this area. This new edition includes the author's corrections. During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book’s coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression & path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for “wide” data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/SPLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting. College-level text for elementary courses covers the fifth postulate, hyperbolic plane geometry and trigonometry, and elliptic plane geometry and trigonometry. Appendixes offer background on Euclidean geometry. Numerous exercises. 1945 edition. Designed to meet the requirements of UG students, the book deals with the theoretical as well as the practical aspects of the subject. Equal emphasis has been given to both 2D as well as 3D geometry. The book follows a systematic approach with adequate examples for better understanding of the concepts. A Course in Modern Geometries is designed for a junior-senior level course for mathematics majors, including those who plan to teach in secondary school. Chapter 1 presents several finite geometries in an axiomatic framework. Chapter 2 introduces Euclid's geometry and the basic ideas of non-Euclidean geometry. The synthetic approach of Chapters 1 2 is followed by the analytic treatment of transformations of the Euclidean plane in Chapter 3. Chapter 4 presents plane projective geometry both synthetically and analytically. The extensive use of matrix representations of groups of transformations in Chapters 3 4 reinforces ideas from linear algebra and serves as excellent preparation for a course in abstract algebra. Each chapter includes a list of suggested",
		"source": "Semantic Scholar",
		"title": "Patrick J Ryan Euclidean And Non Euclidean Geometry An Analytical Approach Book Ebooks File",
		"URL": "https://www.semanticscholar.org/paper/Patrick-J-Ryan-Euclidean-And-Non-Euclidean-Geometry-Ryan/99b2cd31c71ea680000a2bdca05eaefed10dfb69",
		"author": [
			{
				"family": "Ryan",
				"given": "P."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2021"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WTY5K7HY",
		"type": "webpage",
		"title": "[PDF] Personal dynamic memories are necessary to deal with meaning and understanding in human-centric AI | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/Personal-dynamic-memories-are-necessary-to-deal-and-Steels/6748fa68d4cf6e8edfd2e4e95c4cce76263a6672",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/2A67NXM3",
		"type": "paper-conference",
		"abstract": "Keynote Talks.- Speech Man-Machine Communication.- Stochastic Effects in Signaling Pathways in Cells: Interaction between Visualization and Modeling.- Rough-Granular Computing in Human-Centric Information Processing.- Discovering Affinities between Perceptual Granules.- Human-Computer Interactions.- A Psycholinguistic Model of Man-Machine Interactions Based on Needs of Human Personality.- Adaptable Graphical User Interfaces for Player-Based Applications.- Case-Based Reasoning Model in Process of Emergency Management.- Enterprise Ontology According to Roman Ingarden Formal Ontology.- Hand Shape Recognition for Human-Computer Interaction.- System for Knowledge Mining in Data from Interactions between User and Application.- Computational Techniques in Biosciences.- Analyze of Maldi-TOF Proteomic Spectra with Usage of Mixture of Gaussian Distributions.- Energy Properties of Protein Structures in the Analysis of the Human RAB5A Cellular Activity.- Fuzzy Weighted Averaging of Biomedical Signal Using Bayesian Inference.- Fuzzy Clustering and Gene Ontology Based Decision Rules for Identification and Description of Gene Groups.- Estimation of the Number of Primordial Genes in a Compartment Model of RNA World.- Quasi Dominance Rough Set Approach in Testing for Traces of Natural Selection at Molecular Level.- Decision Support, Rule Inferrence and Representation.- The Way of Rules Representation in Composited Knowledge Bases.- Clustering of Partial Decision Rules.- Decision Trees Constructing over Multiple Data Streams.- Decision Tree Induction Methods for Distributed Environment.- Extensions of Multistage Decision Transition Systems: The Rough Set Perspective.- Emotion Recognition Based on Dynamic Ensemble Feature Selection.- Rough Fuzzy Investigations.- On Construction of Partial Association Rules with Weights.- Fuzzy Rough Entropy Clustering Algorithm Parametrization.- Data Grouping Process in Extended SQL Language Containing Fuzzy Elements.- Rough Sets in Flux: Crispings and Change.- Simplification of Neuro-Fuzzy Models.- Fuzzy Weighted Averaging Using Criterion Function Minimization.- Approximate String Matching by Fuzzy Automata.- Remark on Membership Functions in Neuro-Fuzzy Systems.- Capacity-Based Definite Rough Integral and Its Application.- Advances in Classification Methods.- Classifier Models in Intelligent CAPP Systems.- Classification Algorithms Based on Template's Decision Rules.- Fast Orthogonal Neural Network for Adaptive Fourier Amplitude Spectrum Computation in Classification Problems.- Relative Reduct-Based Selection of Features for ANN Classifier.- Enhanced Ontology Based Profile Comparison Mechanism for Better Recommendation.- Privacy Preserving Classification for Ordered Attributes.- Incorporating Detractors into SVM Classification.- Bayes Multistage Classifier and Boosted C4.5 Algorithm in Acute Abdominal Pain Diagnosis.- Pattern Recognition and Signal Processing.- Skrybot - A System for Automatic Speech Recognition of Polish Language.- Speaker Verification Based on Fuzzy Classifier.- Support Vector Classifier with Linguistic Interpretation of the Kernel Matrix in Speaker Verification.- Application of Discriminant Analysis to Distinction of Musical Instruments on the Basis of Selected Sound Parameters.- Computer Vision, Image Analysis and Virtual Reality.- Spatial Color Distribution Based Indexing and Retrieval Scheme.- Synthesis of Static Medical Images with an Active Shape Model.- New Method for Personalization of Avatar Animation.- Multidimensional Labyrinth - Multidimensional Virtual Reality.- Shape Recognition Using Partitioned Iterated Function Systems.- Computer Vision Support for the Orthodontic Diagnosis.- From Museum Exhibits to 3D Models.- Advances in Algorithmics.- A Method for Automatic Standardization of Text Attributes without Reference Data Sets.- Internal Conflict-Free Projection Sets.- The Comparison of an Adapted Evolutionary Algorithm with the Invasive Weed Optimization Algorithm Based on the Problem of Predetermining the Progress of Distributed Data Merging Process.- Cumulation of Pheromone Values in Web Searching Algorithm.- Mining for Unconnected Frequent Graphs with Direct Subgraph Isomorphism Tests.- Numerical Evaluation of the Random Walk Search Algorithm.- On Two Variants of the Longest Increasing Subsequence Problem.- Computing the Longest Common Transposition-Invariant Subsequence with GPU.- Databases and Data Warehousing.- Usage of the Universal Object Model in Database Schemas Comparison and Integration.- Computational Model for Efficient Processing of Geofield Queries.- Applying Advanced Methods of Query Selectivity Estimation in Oracle DBMS.- How to Efficiently Generate PNR Representation of a Qualitative Geofield.- RBTAT: Red-Black Table Aggregate Tree.- Performing Range Aggregate Queries in Stream Data Warehouse.- LVA-Index: An Efficient Way to Determine Nearest Neighbors.- Embedded Systems Applications.- Basic Component of Computational Intelligence for IRB-1400 Robots.- Factors Having Influence upon Efficiency of an Integrated Wired-Wireless Network.- FFT Based EMG Signals Analysis on FPGAs for Dexterous Hand Prosthesis Control.- The VHDL Implementation of Reconfigurable MIPS Processor.- Time Optimal Target Following by a Mobile Vehicle.- Improving Quality of Satellite Navigation Devices.",
		"collection-title": "Advances in Intelligent Systems and Computing",
		"DOI": "10.1007/978-3-319-02309-0",
		"event-place": "Cham",
		"ISBN": "978-3-319-02308-3",
		"language": "en",
		"note": "DOI: 10.1007/978-3-319-02309-0",
		"publisher": "Springer International Publishing",
		"publisher-place": "Cham",
		"source": "Semantic Scholar",
		"title": "Man-Machine Interactions 3",
		"URL": "https://link.springer.com/10.1007/978-3-319-02309-0",
		"volume": "242",
		"editor": [
			{
				"family": "Gruca",
				"given": "Dr. Aleksandra"
			},
			{
				"family": "Czachórski",
				"given": "Tadeusz"
			},
			{
				"family": "Kozielski",
				"given": "Stanisław"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/88Q72DJW",
		"type": "paper-conference",
		"abstract": "The theory and practice of agent-based modeling is reviewed, and agent-based modeling toolkits are evaluated and discussed. A tractable selection of toolkits, RepastPy, Repast Simphony, and breve are then employed to develop and visualize a series of increasingly sophisticated agent-based models, starting with a simple network-interaction diagram and proceeding onto the Boids 3D flock simulation, a 3D collision and gravity system, the chaotic Gray Scott diffusion reaction, a sophisticated agent behaviors game of Capture the Flag, finally culminating with an complex Boids evolutionary swarm simulation. To accomplish the latter, genetic programming techniques are briefly reviewed. Finally, an overview is presented with future directions. INTRODUCTION The use of computer systems to solve problems of interest in physics, biology, chemistry, economics, and social sciences has been well-established for decades. The great advances in computing power, software development, computer graphics, communications networks, and a host of other technologies have elevated the domain of applicability to problem solving from simple arithmetic calculations to advanced numerical methods and the creation of large simulations solving myriad systems of complex equations in real-time. A recent development has been the introduction of the Agent-Based Modeling paradigm, which has proven useful for a wide range of problems in physics and chaotic attractors [1], biology [2], economics [3], social science [4], and geospatial simulations [5]. Commensurate with this new paradigm has been the development of a range of toolkits to apply Agent-Based modeling to a particular range of problems. This purpose of this paper is to provide a general introduction to Agent-Based modeling and demonstrate its use in the modeling of a chaotic system. Organization is as follows: Section I begins with a general discussion of Agent-Based Modeling (hereafter ABM), including definitions and characteristics of systems amenable to this approach. Section II will provide an up-to-date review of the various Swarm modeling toolkits currently available at this time of writing. Section III will show the use of a particular toolkit, Repast, using both the simplified RepastPy interface and the more complex RepastS integrated development environment (IDE), and cover the development of a simple swarm model. Section IV will cover the simulation of the chaotic system ______, and Section V will wrap-up with conclusions and further references. SECTION I OVERVIEW Following the general overview by Castle and Crooks [6] and elsewhere [7], a dynamic model is defined as a simplified representation of reality that evolves over time. The unstated art behind this method is the intuition/experience in creating a simulation that is sufficiently complex enough to show interesting dynamics of the system, while being simple enough to have a tractable software representation on a computing system. In particular, “dynamics” is defined in as the study of change and evolving systems [8], while “tractable” is an artifact of the particular tool used. As we shall see, ABM makes particular classes of problems very easy to solve, that would otherwise be very difficult to attack using more traditional dynamical methods. For the purposes of this paper, an Agent, as part of an ABM system, is defined with the following characteristics:  Activity: Each agent independently acts according the rules of the simulation and their own preprogrammed behaviors. These rules and behaviors can take one or more of the following features: o Goal-direction: The agent acts in such a way as to achieve a particular goal, which can be either a relative or extremal value. For example, an agent may be designed to maximize accumulation of a particular resource. o Reactivity/Perceptivity: The agent senses its surroundings, or is supplied with a map such that it is aware of its environment. For example, an agent could be aware of resource node locations. o Bounded Rationality: Generally, goal-direction in agents operates on the rational-choice principle, which generally implies unlimited access to information and computational resources. However, experimental evidence suggests that non-optimal decisions are often closer to reality. Therefore, in order to provide greater predictive power, the agents can be constrained in terms of information resources or analytical ability. For example, an agent might be able to sense only those resource nodes within a finite range, or possess a map of resources that does not take into account the actions of other agents. o Interactivity: Continuing on the principle of bounded rationality, agents may interact or exchange information with other agents. These interactions may have particular effects on the agent, including its destruction or change in goal-seeking behavior. o Mobility: Interactivity with the environment and other agents is vastly improved if the agent can roam the model space independently. o Adaptation: Alteration of an agent’s current state based upon interactions with the environment or other agents provide a useful form of learning or memory. This adaptation can be provided for at the level of the individual agent, or groups of nearby agents, all the way up to the population level of the entire set of agents .  Autonomy: Each agent is free for activity as defined above, with the ability to make independent decisions.  Heterogeneity: Although each agent may begin as a member of a limited set of common templates, develops individuality through autonomous activity in the sense describe previously. Agents need a stage for their behaviors, and this is taken as the definition of the Environment. Although the environment may itself change dynamically according to the actions of the agents, these changes occur passively, rather than in the active fashion of agent time evolution. That is, the state of the environment evolves dynamically, but only in response to the actions of the agents, rather than as a result of particular goal-seeking or adaptive behavior. As an example, given an environment populated with resource nodes and a population of agents each looking to maximize intake of resources, the agents alter (consume) the distribution of resources, while the environment passively adjusts resource distribution based upon agent action. Granted, more complex environments are possible which dynamically alter their own resource distribution, but this alteration is typically given by a simple rule, rather than the result of goal-seeking behavior. (Note that more complex environments can be modeled in turn by the addition of a new group of agents to act as the “resources”). In sum, agents are active, while the environment is passive. As a final note, Agent Based Modeling (with particular distinctions) can also be found under the terms Agent-Based Computational Modeling [9], Agent-Based Social Simulation [4], Multi-Agent systems [10] [11], Distributed Artificial Intelligence [12] [13], and Swarm Intelligence [14] [15][16]. However, Agent Based Modeling and Swarm Intelligence appear to be the more contemporary of the terms used in the literature. SECTION II – AGENT BASED MODELING TOOLS There are good summaries in the literature which compare and contrast the various ABM tools [17] [18]. However, they are somewhat dated in terms of the versions of the tools that were reviewed. For example, de Smith et. al., though updated in 2008, referred to versions of Repast from 2006. The rapid pace of software, therefore, warranted a fresh look at the current state of the art. Given the previous reviews of ABM frameworks and libraries, this paper considers the following software:  Swarm [19] [20] – Latest stable release is Swarm 2.2 released in February 2005.  MASON [21] – Latest stable release is MASON Version 12, released in the July 2007 timeframe.  NetLogo [22] – Latest stable release is NetLogo version 4.0.2, released December 5, 2007.  RePast [23] – Latest stable release is Repast Simphony 1.0 released December 3, 2007.  MetaABM [24] – Latest stable release is  Breve [25] – Latest stable release is 2.7.2 released February 19, 2008. With the exception of NetLogo, all of these frameworks are both open source (hence, free for use and source code inspection) and based upon an object-oriented language (usually Java). This is due in large part because object oriented frameworks and programming is a natural fit for ABM. Swarm was originally written in Objective-C, and then ported to Java. Written as a library and framework of simulation tools, rather than as a finished application per se, Swarm is one of the oldest agent-based modeling toolkits, and there are hundreds of example applications and demos, and several of the newer toolkits are based upon it. However, given the age of the last stable release, and the existence of newer toolkits with more friendly environments (especially Repast), Swarm is not considered further in this paper. Nonetheless, the documentation and research papers on Swarm established many of the foundational concepts and ideas in ABM, and reading over these materials serves as an excellent introduction to the large and growing field of agent-based modeling. MASON, or Multi-Agent Simulator of Neighborhoods/Networks is a multiagent simulation library in Java, designed to serve as the base class structure for custom Java simulations. It also includes a model library and suite of 2D and 3D visualization tools, and is developed with an emphasis on speed and portability. Although well-regarded, at the time of this writing, the Windows batch files for starting and running MASON 12 did not work on Windows Vista 64-bit, so MASON is not considered further in this paper. NetLogo, though not open source in the strictest sense, is freeware, and designed for educational use, being based upon a simple Logo-type language. Originally developed in 1999 by Uri Wilensky, NetLogo has been u",
		"DOI": "10.1142/9685",
		"ISBN": "978-981-4699-48-8",
		"language": "en",
		"note": "DOI: 10.1142/9685",
		"publisher": "WORLD SCIENTIFIC",
		"source": "Semantic Scholar",
		"title": "Selforganizology: The Science of Self-Organization",
		"title-short": "Selforganizology",
		"URL": "http://www.worldscientific.com/worldscibooks/10.1142/9685",
		"author": [
			{
				"family": "Zhang",
				"given": "WenJun"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016",
					1
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/MGMG2DR6",
		"type": "article-journal",
		"abstract": "According to the United States National Institute of Dental and Craniofacial Research, craniofacial anomalies are the most common birth defects. Genetic, epigenetic, and environmental causes lead to craniofacial anomalies which can range from cleft lip and palate to major defects in the development of the skull, face, brain, eyes, ears, and nose. Oral and maxillofacial surgeries are performed on individuals with craniofacial defects, but success of surgery is dependent on nature of the defect. Some defects are too complex which need multiple surgeries, but still not be completely cured by traditional methods. In this direction, 3 dimensional or 3D bioprinting has emerged as a promising modern technology that can significantly benefit the field. A personalized medicine approach to address craniofacial defects is provided by 3D-printing technologies which integrate doctors, engineers and researchers to work for a common goal [1]. Hence, this mini review of literature on 3D bioprinting discusses the technology in the context of craniofacial, and hence oral and maxillofacial therapeutics. In the field of oral and maxillofacial therapeutics, researchers and surgeons are aiming to develop a 3D scaffold by direct 3D printing technology to fabricate complicated tissue grafts. They need to possess all the necessary biological properties and environment for cell division and tissue regeneration. Hence, the current and future challenges of 3D bioprinting will be to achieve the above criteria is a cost-effective and timely manner. Check for updates significantly convenient, greatly improves their outcome, and enhances the quality of life of patients [5]. The Process of 3D Bioprinting The process of 3D bioprinting can be categorized into pre-bioprinting, bioprinting, and post-bioprinting [6-8]. At the very beginning, suitable cells for synthesizing the bioink are isolated and cultured to obtain a large amount of viable starting material. In the pre-bioprinting step, imaging is performed on the tissue that will be bioprinted. The imaging is done by technology like Computed tomography (CT) or cone beam CT in Digital Imaging and Communications in Medicine (DICOM) format and magnetic resonance [6,8,9]. Standard Triangle Language (STL) format is used to provide the image as input to the bioprinter [6,8,9]. Hence, a Computer Aided Design (CAD) model of the target tissue is obtained [6]. Introduction 3D bioprinting has addressed the problems associated with traditional surgeries and therapy. The emergence of 3D printing technology occurred in the 1990’s when synthetic inks were used to generate fabricating scaffolds, leading up to the invention of bioprinting in the 21st century [2]. Bioprinting employs bioinks which are composed of biocompatible substances made of cells or matrices derived from natural sources which have applications in tissue engineering [3,4]. The technology evolved from additive manufacturing where biomaterials are used to develop scaffolds which precisely fit into the dimensions of a craniofacial defect (Figure 1). There are several advantages of 3D bioprinting in oral and maxillofacial surgery since traditional methods like bone grafting are not always convenient. This is because several different kinds of bones and cartilages, derived from various progenitor stem cells, are complexly organized to form the craniofacial skeleton. In this direction, 3D bioprinting has immensely improved the field of craniofacial surgeries because it allows for the selection of specific kinds of cells to bio print the target tissue. Another very important advantage of 3D bioprinting is that success of the craniofacial surgery is no longer solely dependent on the surgeon. Further, 3D printing adopts a personalized therapeutic approach by recapitulating the physical and aesthetic properties of the patient’s target tissue in the prostheses, which was labor-intensive earlier [1]. Hence, 3D bioprinting makes the process of oral and maxillofacial surgeries Citation: Sen R (2020) Recreating the Face A Mini-Review of Current Studies on 3D Bioprinting in Oral and Maxillofacial Surgery. Archives Oral Maxillofac Surg 3(1):86-89 Sen. Archives Oral Maxillofac Surg 2020, 3(1):86-89 Open Access | Page 87 | for 3D bioprinting where biomaterial is laid in patterned layers to achieve the ultimate conformation [11]. A recent review of 3D bioprinting in maxillofacial surgery reported 297 publications from 35 countries where 2889 patient outcomes were improved using 3D bioprinting [12]. These publications show that highest number of clinical indications occur for dental implantations and reconstruction of the mandible [12]. Overall, 3D bioprinting leads to improved surgery in terms of precision and reduced time, but high costs and production time remain as disadvantages [12]. Further applications of 3D bioprinting in craniofacial surgeries include trauma surgery, orthognathic surgery, facial prosthetics, Temporo Mandibular Joint (TMJ) and complex facial reconstruction [13-18]. 3D bioprinting shows several benefits in the above areas. In trauma surgery, 3D printed titanium mesh cures postoperative enophthalmos or diplopia caused by inefficient orbital wall reconstruction. 3D bioprinting solves issues caused by blowout fractures of orbital floor and walls [16]. 3D bioprinting helps in precision diagnosis during orthognathic surgeries. 3D bioprinting has several advantages over traditional prosthesis. A common problem in TMJ reconstruction is called autorotation which is an instability of condyle and fossa of TMJ that affects proper placement of maxilla. Personalized orthognathic surgical guide (POSG) system helps solved this problem using 3D bioprinting [19]. Advantages of 3D bioprinting in complex facial reconstruction include accurate plate adaptation, precise harvest of bones, low bone-plate distance and blood loss, and reduced time of surgery and anesthesia [13,20]. The implications of 3D bioprinting are extensively discussed by Dr. Devid Zille in his article. Dr. Zille leads the Patient-Specific Implant initiative for Osteomed, which is among the world’s largest small-bone implant manufacturers for Maxillofacial, Neuro, and Extremities surgery. The bioprinting process performs a layer after layer printing of the tissue image that is acquired in the previous step, using the bioink made of cells, nutrients, and matrix loaded onto the printer cartridge [6]. The bioprinted cell-based entity is called pre-tissue which is moved into an incubator for maturation [10]. Following maturation, cells are apportioned onto the biocompatible scaffold in a layer after layer manner in succession leading to the development of 3D biological constructs that resemble tissues [6]. Post-bioprinting contributes to the process of developing and maintaining the mechanical stability and functionality of the 3D construct [8]. Cells in the bioprinted entity need mechanical and chemical signals for remodeling and development of tissues for proper sustainability of the construct. In this direction, bioreactors provide the ideal environment and simulations that are necessary for the tissue to survive, mature and vascularize [6,7]. 3D Bioprinting in Oral, Maxillofacial, and Facial Reconstructive Surgery In oral, maxillofacial, and facial reconstructive surgery, the technology of 3D bioprinting encompasses certain criteria like developing prototypes of facial anatomy, improving contour symmetry of the face after surgery, bioprinting pre-contoured grafts, developing superior quality prostheses for patients having scars, asymmetry, and malformations [11]. Another area to apply 3D bioprinting technology in surgery is the development of advanced simulation models for medical students [11]. Using imaging techniques like CT and magnetic resonance imaging (MRI), anatomical scans are prepared and saved in standard format like DICOM. Next, CAD software is used to generate virtual 3-dimensional prototypes with STL Figure 1: Schematic outline of 3D Bioprinting. The damaged tissue is imaged by CAD, followed by bioprinting of the image using bioink that are derived of cells and other factors. Next, the bioprinted tissue is surgically placed in the patient as a therapy for repair, reconstruction, regeneration, or protheses. Citation: Sen R (2020) Recreating the Face A Mini-Review of Current Studies on 3D Bioprinting in Oral and Maxillofacial Surgery. Archives Oral Maxillofac Surg 3(1):86-89 Sen. Archives Oral Maxillofac Surg 2020, 3(1):86-89 Open Access | Page 88 | Conclusion Although promising and successful so far, the process of 3D bioprinting still require considerable development to be applied on a large scale to patients. Further research is required to expand its applications to a wider range of therapeutics. Nonetheless, 3D bioprinting has shown success in reconstructive, repair, and protheses related to oral and maxillofacial surgery. Advances in stem cell research, superior engineering, and computational techniques will further advance this elegant technology. At the regulatory level, care needs to be taken to ensure proper administration, affordability, accessibility, and health insurance amenability for this technology.",
		"container-title": "Archives of Oral and Maxillofacial Surgery",
		"DOI": "10.36959/379/360",
		"ISSN": "26898772",
		"issue": "1",
		"journalAbbreviation": "Archives Oral Maxillofac Surg",
		"source": "Semantic Scholar",
		"title": "Recreating the Face - A Mini-Review of Current Studies on 3D Bioprinting in Oral and Maxillofacial Surgery",
		"URL": "https://scholars.direct/Articles/oral-and-maxillofacial-surgery/aoms-3-015.php?jid=oral-and-maxillofacial-surgery",
		"volume": "3",
		"author": [
			{
				"family": "Rwik",
				"given": "Sen"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020",
					12,
					31
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WRFISWQ5",
		"type": "paper-conference",
		"abstract": "UV lights can be used for the disinfection of hospital rooms and with the help of 3d room models, the cleaning can be optimized. The task is to develop a system which enables and handles the communication between an arduino and a web interface. The arduino manipulates the UV-lights and should be controllable by a user from a browser. Additionally, the system should also allow the user to update the room model. To do so, a Flask backend application will connects to the arduino via websockets and that accepts requests from the user via HTTP. The requests from the user are sent by the server (Flask app) to the arduino, while also updating the database (mongodb) entries. Abstract Muscle segmentation in 3D ultrasound volumes can be performed in less than 2 minutes with the Ifss-net network instead of the two hours it would take an expert. However, there is no end product for clinicians to use. The goal of this project is to program and evaluate the IFSS-net in Pytorch as an \"app\" for MONAI in imfusion/slicer. To in the near future, make it available and evaluated by physicians. Abstract Nowadays, there exist papers that describe how to create a challenge. During this project, the student will develop the challenge website using strong programming skills, as well as critical and scientific thinking to research and analyze the current state of the art and generate the definition of the requirements expressed in some scientifically usable guidelines. The student will search and analyze the structure, techstack and data visualization methods used for a successful challenge website and will try to condensate it in an useful guideline. Abstract Multi-modal registration is a required step for many image-guided procedures, especially ultrasound- guided interventions that require anatomical context. While a number of such registration algorithms are already available, they all require a good initialization to succeed due to the challenging appearance of ultrasound images and the arbitrary coordinate system they are acquired in. In this paper, we present a novel approach to solve the problem of registration of an ultrasound sweep to a pre-operative image. We learn dense keypoint descriptors from which we then estimate the registration. Our method is designed to overcome the challenges inherent to registration tasks with freehand ultrasound sweeps, namely, the multi-modality and multidimensionality of the data in addition to lack of precise ground truth and low amounts of training examples. The registration method that is fast, generic, fully automatic, does not require any initialization and can naturally generate Abstract Intraoperative Optical Coherence Tomography (iOCT) is an emerging imaging technology that allows acquisition of volumetric data during ophthalmic surgeries. Previous studies showed the feasibility of performing retinal surgery under 4D iOCT guidance. Volume rendering techniques could aid emerging procedures, such as subretinal injections, by supporting the insertion of a microsurgical cannula into the subretinal space. However, advanced volume rendering techniques need further investigation to improve the spatial perception of the surgical instruments in relation to anatomical structures. This project aims to explore focus and context visualization techniques for volume rendering of iOCT data to support subretinal injection procedures. Abstract Intraoperative Optical Coherence Tomography (iOCT) is an emerging imaging technology that allows acquisition of volumetric data. Recent advances in scanning speed have even enabled the acquisition of temporal volumetric sequences. In the last decade, this technology has been introduced to the ophthalmic operating rooms providing micron-resolution imaging of retinal anatomy and surgical instruments. In addition to the surgeon’s conventional view onto the operating area through a microscope, intraoperative OCT provides depth information and visualization of retinal layers, which are not visible from the microscopic enface view. Since swept-source OCT systems provide volumetric frame rates that allow retinal surgery under exclusive OCT guidance, new paradigms for visualization can be investigated, which differ from the conventional microscopic view. One potential way towards visualization of full 4D iOCT guided ophthalmic surgery is to visualize OCT volume sequences in a virtual reality environment. This project aims to implement state-of-the-art visualization techniques that can be implemented for real-time rendering of iOCT volumes in a VR environment. Abstract In this thesis, a VR hand rehabilitation system is implemented in Unity. For this purpose, the importance of rehabilitation, commonly used tracking technologies and required methods in rehabilitation system are investigated first. Afterwards, important features in hand rehabilitation systems are elaborated and included in the implementation. A data glove, called the Kinfinity Glove, is used to track the finger joints of the user and is evaluated against the commonly used Leap Motion Controller. An online survey to investigate whether the implemented system could improve the rehabilitation process was performed. The results from the evaluation and the survey are presented and then discussed, showing that the implemented system could positively affect the rehabilitation process, but still requires a lot of future work. Abstract The goal of the project is to build a program that is able to automatically segment the vertebrae in MR volumes.To achieve this we use an deep-learning approach. For this we need to research and choose an architecture for, and then build and train a neuronal network, based on a provided dataset. Abstract Cardiovascular disease is one of the leading causes of death worldwide. However, heart surgery is difficult even for experienced surgeons. Physicians demand a better, patient-specific planning of the surgery to deliver tailored approaches. Vitonomy.io is focusing on providing individualized therapy by utilizing data- driven clinical trials on virtual patients to minimize both the resources and risk during animal or human trials. One of the major step is whole-heart segmentation, which is the prerequisite for further processing, visualizing and analyzing. In this Master thesis, we are going to solve two existing challenges for whole-heart segmentation by adapting state-of-the-art deep learning methods. Firstly, current method use two different segmentation networks for images of different contrasts. The main goal of this thesis is providing a unified whole-heart segmentation framework for both low- and high-contrast cardiac CT images. A two-step segmentation method is used, where a Cycle-GAN for image transformation is trained for translation between the low- contrast and high-contrast domains and a subsequent segmentation network responsible for segmenting the transformed images. Another challenge is that current related methods did not fully take the advantage of the information provided by the segmentation mask, during the training of Cycle-GAN. Here we also provided and compared methods for integrating this information which brings better quality of the transformed image. Abstract \"Alzheimer’s Disease (AD) is the most common form of dementia and often difficult to diagnose due to the multifactorial etiology of dementia. Recent works on neuroimaging-based computer-aided diagnosis with deep neural networks (DNNs) showed that fusing structural magnetic resonance images (sMRI) and fluorodeoxyglucose positron emission tomography (FDG-PET) leads to improved accuracy in a study population of healthy controls and subjects with AD. However, this result conflicts with the established clinical knowledge that FDG-PET better captures AD-specific pathologies than sMRI. Therefore, we propose a framework for the systematic evaluation of multi-modal DNNs and critically re-evaluate single- and multi-modal DNNs based on FDG-PET and sMRI for binary healthy vs. AD, and three-way healthy /mild cognitive impaired/AD classification. Our experiments demonstrate that a single-modality network using FDG-PET performs better than MRI (accuracy0.91 vs 0.86) and does not show improvement when combined. This conforms with the established clinical knowledge on AD biomarkers, but raises questions about the true benefit of multi-modal DNNs. We argue that future work on multi-modal fusion should systematically assess the contribution of individual modalities following our proposed evaluation framework. Finally, we encourage the community to go beyond healthy vs.AD classification and focus on differential diagnosis of dementia, where fusing multi-modal image information conforms with a clinical need.\" Abstract Treatment planning techniques for radionuclide therapy can have problems to deal with the high concentration of radioactivity accumulated in the bladder. To avoid this, the bladder needs to be detected, removed from the PET volume and replaced with the average value of the PET. An in-house Deep Learning-based pelvis segmentation is used to find the pelvic region. Then, the bladder is be detected and segmented using Deep Learning-based methods, trained on public dataset. The network will be also testes on an in-house dataset provided by clinical partner at Klinikum Rechts der Isar. Abstract Self-supervised learning (SSL) has recently attracted a lot of attention due to the high cost and data limitations of training supervised models. The workflow of SSL is divided into two stages. The first stage is called the pretext task, which forces the model to map different augmentations of the same data so that it learns better representation of the input data. The second stage is called the downstream task, which utilizes the result model from the first stage for fine-tuning. Self-supervised learning is widely applicable to various domains (e.g., computer vision, natural language processing, and reinforcement learning). Curr",
		"source": "Semantic Scholar",
		"title": "Upcoming oberseminars Deep Learning-Based Whole-Heart Segmentation from Low-Contrast Scans therapy Self-Supervised Learning Technique Based on Mixed-up Augmentation for Image Data Learning-based for",
		"URL": "https://www.semanticscholar.org/paper/Upcoming-oberseminars-Deep-Learning-Based-from-on-Fedoseev-Khakzar/ae7709a6b93c414068b3b0666d328ecfc1dc721c",
		"author": [
			{
				"family": "Fedoseev",
				"given": "Fedor"
			},
			{
				"family": "Khakzar",
				"given": "Ashkan"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2020"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FCSJ3IE8",
		"type": "article-journal",
		"abstract": "Abstract. Nowadays, when visualization is one of the main channels of communication, maps, geo-images and infographics can help develop the concept of space and expand knowledge about our world. But most spatial data are not available to the visually impaired and need to be converted to a tactile format. Tactile and tiflographic maps are becoming more and more popular because they help overcome information barriers for those who cannot see, making it easier to navigate in everyday life.Blind or visually impaired people need to know more about their immediate environment to navigate in the room, building, city or country. The ability to read and understand a tactile map is not an automatic skill for visually impaired people. The user of the map must be trained to recognize and understand the relief material, symbols in the form of points and lines, use the texture and legend that reveal the information presented on a particular map.The last decade witnessed a great technological leap towards the creation and replication of tactile and tiflographic maps. In addition to such traditional methods of creating relief graphic maps as thermoforming of plastic on the engraved surface and printing on specialized paper, new methods have been added – quickly solidifying varnishes, paper stamping devices and 3D printers. However, the process of drawing up high-quality relief-graphic maps is strongly influenced by the technology of publication. It depends on the tactile distinctiveness and clarity of drawing elements of the map and conventional signs. This has an impact on the choice of tactile variables that can be operated on when mapping. In addition, the selected technology depends on the final price of the final product. In fact, the creators of tactile and tiflographic maps have the choice between a triade: \"method of production – material – format edition\". These technological moments subsequently influence and determine the scale of the maps, the image methods and the level of generalization. According to some teachers, tactile maps are a special case of relief-graphic materials (tactile graphics) so requirements, content, design, application signatures which are applicable to tactile graphics should also apply to tactile maps. However, for cartographers, tactile and tiflographic maps are a particular area of cartography that uses its own language describing space with a combination of specific rules and regulations.Laboratory of cartography at the Institute of geography RAS has been engaged in the creation of thematic tactile maps at different scale levels and areas for 7 years already. A set of thematic maps for the territory of Russia was created to ensure the educational process in specialized institutions (schools and colleges for the blind and visually impaired). This set of thematic maps consists of the following maps: components of the natural environment, climate, minerals, soil cover and land resources, vegetation. In total, the set includes 34 different thematic maps with a total circulation of more than 700 copies. More than 400 maps have been transferred to specialized agencies and are already being used at geography lessons.All created tactile maps are made at once by 4 methods: micro-capsule paper, stamping, thermoforming, printing on a 3D printer. With the help of 3D printing and embossing technology, each thematic map is made in a single copy, and the maps on microcapsular paper and plastic are made in large quantities.When creating tactile thematic maps for the territory of Russia many factors were taken into the account that are not important in the creation of conventional, traditional maps. In addition to the selection of suitable subjects, scale, projections in the first place the special requirements for the creation of tactile graphics and manufacturing technology were taking into the account primarily, which greatly affect the tactile readability.",
		"container-title": "Abstracts of the ICA",
		"DOI": "10.5194/ica-abs-1-243-2019",
		"ISSN": "2570-2106",
		"journalAbbreviation": "Abstr. Int. Cartogr. Assoc.",
		"language": "en",
		"page": "1-1",
		"source": "Semantic Scholar",
		"title": "Tactile thematic maps for the territory of Russia: multivariate representation of geographic information",
		"title-short": "Tactile thematic maps for the territory of Russia",
		"URL": "https://ica-abs.copernicus.org/articles/1/243/2019/",
		"volume": "1",
		"author": [
			{
				"family": "Medvedev",
				"given": "Andrey"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2019",
					7,
					15
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/S93JPZ3T",
		"type": "paper-conference",
		"abstract": "Application of educational and interactive simulation games to teach important concepts is an area that has attracted several Software Engineering researchers and educators attention. Previous research and studies on usage of simulation games in classroom to train students have demonstrated positive learning outcomes. Peer code review is a recommended best practice during software development which consists of systematically examining the source code of peers before releasing the software to Quality Assurance (QA). Practitioners and Researchers have proposed several best practices on various aspects of peer code review such as the size of code to be reviewed (in terms of lines of code), inspection rate and usage of checklists. We describe a single player educational simulation game to train students on best practices of peer code review. We define learning objectives, create a scenario in which the player plays the role of a project manager and a scoring system (as a factor of time, budget, quality and technical debt). We design a result screen showing the trace of events and reasoning (learning through success and failure as well as discovery) behind the points awarded to the player. We conduct a survey of the players by conducting a quiz before and after the game play and demonstrate the effectiveness of our approach. Keywords—Peer Code Review, Simulation Game, Software Engineering Education and Training, Teaching Critical Decision Making I. RESEARCH MOTIVATION AND AIM Software Engineering (SE) being a practice-oriented and applied field is taught primarily (at University Level) using instructor-driven classroom lectures as well as team-based projects requiring hands-on skills. In comparison to classroom lectures, team-based hands-on projects require more active participation and experiential learning. SE Educators have proposed and shown positive student learning outcomes by teaching certain concepts using simulation games. Some of the advantages of teaching using simulation games are incorporation of real-world dynamics such as critical decision making under multiple and conflicting goals, encountering unexpected and unanticipated events, allowing exploration of alternatives (discovery learning) and allowing incorporation of learning through doing and failure [1][2]. Peer code review consists of reviewing and critiquing team members source code in order to detect defects and improve the quality of the software [3][4][5][6][7]. Software code review is practiced in several open-source and closed-source software project settings. There are several best practices on various aspects of peer code review such as the code review size, coverage and rate. Code reviewer expertise, reviewer checklist and usage of tools (such as mailing lists, Gerrit or Rietveld) also play an important role in influencing the impact of code review on software quality [5][6]. The work presented in this paper is motivated by the need to teach the importance and best practices of peer code review to students using a simulation game. The research aim of the work presented in this paper is the following: 1) To develop a web-based interactive educational SE simulation game or environment for teaching benefits and best-practices of peer-code review process. 2) To investigate a learning framework and model based on discovery learning, learning from failure, evidence and reasoning for teaching concepts on the practice of peer code review. 3) To evaluate the proposed learning framework and tool by conducting experiments and collecting feedback from users. II. RELATED WORK & RESEARCH CONTRIBUTIONS In this Section, we discuss closely related work and state our novel research contributions in context to the related work. We conduct a literature survey of papers published on the topic of teaching Software Engineering concepts using simulation games. Table I shows the result of our literature review. Table I lists 9 papers in reverse chronological order and reveals that teaching Software Engineering concepts using simulation games is an area that has attracted several researchers attention from the year 2000 until 2013. We characterize 9 papers based on the tool name, year, simulation topic, University and interface. We infer that teaching software engineering processes and project management are the two most popular target areas for simulation games. The game interfaces various from simple command line and menu driven model to animated and 3D interfaces. Researchers have also experimented with board games in addition to computer-based games. In context to closely related work, the study presented in this paper makes the following novel contributions: 1) While there has been work done in the area of teaching Software Engineering processes and project management skills, our work is the first in the area 1st International Workshop on Case Method for Computing Education (CMCE 2015) 63 TABLE I. RELATED WORK (SORTED IN REVERSE CHRONOLOGICAL ORDER) AND TOOLS ON TEACHING SOFTWARE ENGINEERING CONCEPTS USING SIMULATION GAMES SNo. Tool Name Year Simulation Topic University Interface 1 AMEISE [8] 2013 [Bollin’13] Managing a SE project (focussing on software quality) Alps-Adriatic University, Carinthia Tech Inst. and Linz University Menu based, Command Interface 2 DELIVER [9] 2012 [Wangenheim’12] Earned Value Management Federal University of Santa Catarina Board Game 3 ProMaSi [10] 2011 [Petalidis’11] Project Management T.E.I. of Central Macedonia, Serres-Greece Java based, Desktop environment 4 SimVBSE [11] 2006 [Jain’06] Value-based Software Engineering University of Southern California Animated, 3D interface 5 Problems and Programmers [12] 2005 [Baker’05] Software Engineering Processes University of California, Irvine Card Game 6 SimjavaSP [13] 2005 [Shaw’05] Software Process Knowledge University of Tasmania Menu-based, Graphical Interface 7 Incredible Manager [14] 2004 [Dantas’04] Project Management Experiential Learning Brazilian University Button Driven, Command Interface 8 SimSE [1] 2004 [Navarro’04] Software Engineering Processes University of California, Irvine Web-based, Graphical Interface 9 SESAM [15] 2000 [Drappa’00] Software project Management Stuttgart University, Germany Text Based, Pseudo-Natural Language Command Interface of building and investigating simulation games for teaching best practices for peer code review. 2) We propose a simulation game to teach peer code review practices based on learning by failure, success and discovery. We demonstrate the effectiveness of our approach, discuss the strengths and limitations of our tool based on conducting user experiments and collecting their feedback. III. GAME ARCHITECTURE AND DESIGN A. Learning Objectives In this game we define 12 learning objectives covering multiple aspects of peer code review. These learning objectives are captured in our pre game questions1 and post game questions2. Table II shows 6 of the 12 learning objectives (due to space constraint), corresponding decisions and the questions which we asked to the player in game. Table II shows the structure that we follow to design the game questions. Each question is designed keeping in mind the learning objectives of the game. Based on the learning objectives we come up with a situation or a decision which best questions that objective. An evaluation question is then formed around this decision which challenges the player’s knowledge to its best. We therefore present different situations to the player where they have to make decisions like whom to assign the task of review process (Figure 1), what inspection rate to choose (Figure 2), when to start with review, steps required to foster good code review culture in team etc. A decision can map to one or more learning objectives. Similarly two or more evaluation questions may map to one decision. B. Unexpected Events We introduce unexpected events and unforeseen circumstances (such as internal conflicts between the team members, attrition and change in deadline or demand from the customer) in the game to make it more realistic. Unexpected events are unobservable and our goal is to examine the response and the decision making ability of the player to unexpected circumstances. Figure 3 shows a screenshot for the simulation game in which the player is presented with an unexpected 1http://bit.ly/1ddyitO 2http://bit.ly/1GEdBBX situation. As shown in Figure 3, the project manager is encountered with a situation wherein a developer quits the team or organization just one month before the release date.",
		"event-title": "QuASoQ/WAWSE/CMCE@APSEC",
		"source": "Semantic Scholar",
		"title": "Anukarna: A Software Engineering Simulation Game for Teaching Practical Decision Making in Peer Code Review",
		"title-short": "Anukarna",
		"URL": "https://www.semanticscholar.org/paper/Anukarna%3A-A-Software-Engineering-Simulation-Game-in-Atal-Sureka/f3934a481b85bfe1b44edb08f9cd2d077e09f491",
		"author": [
			{
				"family": "Atal",
				"given": "R."
			},
			{
				"family": "Sureka",
				"given": "A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015",
					7,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/DCXPZ8YU",
		"type": "paper-conference",
		"abstract": "Robosense 2.0: Robotic Sensing and Architectural Ceramic Fabrication” demonstrates a generative design process based on collaboration between designers, robotic tools, advanced software, and nuanced material behavior. The project employs fabrication tools that are typically used in highly precise and predetermined applications, but uniquely thematizes the unpredictable aspects of these processes as applied to architectural component design. By integrating responsive sensing systems, this paper demonstrates real-time feedback loops that consider the spontaneous agency and intuition of the architect (or craftsperson) rather than the execution of static or predetermined designs. This paper includes new developments in robotics software for architectural design applications, ceramic-deposition 3D printing, sensing systems, materially driven pattern design, and techniques with roots in the arts and crafts. Considering the increasing accessibility and advancement of 3D printing and robotic technologies, this project seeks to challenge the erasure of materiality: when mistakes or accidents caused by inconsistencies in natural material are avoided or intentionally hidden. Instead, the incorporation of material and user-input data yields designs that are imbued with more nuanced traces of making. This paper suggests the potential for architects and craftspeople to maintain a more direct and active relationship with the production of their designs. INTRODUCTION Unlike robots in industry, which execute predefined and repetitive tasks in a controlled environment, robotics in design and architecture are becoming increasingly involved with uncertain tasks within more complex and dynamic contexts. Manufacturing machines and robots have now started to gain intelligence—actively communicating, monitoring, and sensing—and with this, the ability to react (Menges 2014). To facilitate the feedback between design and robotic fabrication, a Python-based interface, encapsulating communication protocols and robotic manipulation libraries, was created in Robosense 1.0 (Moorman, Liu, and Sabin 2016). The interface seamlessly bridges the gap between physical and digital environments and allows for a feedback-oriented robotic fabrication paradigm. Building upon Robosense 1.0, Robosense 2.0 steps forward and integrates the interface into design software Rhinoceros 3D and Grasshopper. Since 2009, the Sabin Design Lab has innovated digital ceramics through 3D-printed ceramic bricks and nonstandard componentry (Sabin 2010). The plastic nature of clay offers a potent material solution to contemporary generative design processes in architecture, which frequently feature organic and natural forms of increasingly complex expression and ornamentation (Sabin et al. 2014). The use of clay to integrate the designer’s intuition and to rationalize complex data and geometries has been incorporated into the design process in alternate industries such as car and boat design for decades, but professionals in the broader field of architecture have yet to widely explore the potential for these materials and tools to augment the efficiency and quality of built design work. This paper explores the transfer of information and data, including real-time feedback via sensing technologies, through the hybridization of crafts-based ceramic techniques with contemporary digital design, robotic 3D printing, and nonstandard component–based architectural assemblies. We propose the development of a responsive feedback system that provides the designer/maker with information about the material, allowing for intuitive, on-the-fly modifications to the design process during the course of fabrication. Reciprocally, the designer’s choices and changes are registered, and the software responds in real time to create a fluid workflow that informs and unlocks the potential for more a nuanced understanding of 3D-printed clay as an architectural fabrication technique. BACKGROUND Robotically Fabricated Ceramics Robosense 2.0 uses the material language of clay deposition 3D printing, the extruded clay bead, as a medium for developing physical case studies (Figure 1). Existing precedents that investigate clay deposition printing tend to use a technique borrowed from other forms of 3D printing: predetermined extruder motions create fine stacked layers to produce volumetric forms, similarly to how coil pots have traditionally been constructed, and to how low-cost plastic deposition 3D printers are able to produce objects with high efficiency. The Institute for Advanced Architecture Catalonia (IAAC) is innovating the use of robotically fabricated clay components for largescale applications with exceptional results for the purpose of creating architectural enclosures. While their work is innovative in scale, clay body, and precision, the production methodology and extrusion techniques do not significantly vary from typical 3D-printing operations. The outcomes of these prints have a high level of predictability because all of the material deposited is fully supported, constraining designs to entirely enclosed volumetric forms that lack more advanced material intelligence (Chronis et al. 2017). Other precedents have used the extruded clay bead to develop more expressive architectural screens that allow for deviation from preprogrammed behavior, but often with difficult-to-control outcomes. As a result, the complexities of drying, firing, and glazing these components at a large scale of production make them difficult to envision as applied to architectural building systems. Both Harvard GSD’s Woven Clay project and Cornell University’s Clay Non-Wovens use techniques influenced by textile manufacturing in order to develop these patterning and screen systems, which can negotiate circumstances of light through thin ceramic panels. Both projects are successful in challenging ideas of patterning and demonstrating potential for robotic fabrication in clay (deviating from the process of printing layer on top of layer). The challenges discovered in these projects relate to unpredictable tolerances and warping due to the firing process. Furthermore, joinery and connection detail in both projects are unresolved, inhibiting either from becoming truly scalable (Rosenwasser, Mantell, and Sabin 2017; Friedman, Kim, and Mesa 2014). Ron Rael and Virginia San Fratello of Emerging Objects have worked extensively with 3D-printed ceramic material, both powder-based using Z Corp 3D printers and with delta bot machines (2018). Emerging Objects’ emphasis on G-code “glitch” focuses on code manipulation of traditional 3D-printing methods to create textures and coded mistakes within clay material. This paper presents an alternative to preprogrammed “glitches” through a sensing-based process that engages the mistake, error, and inconsistencies through human interaction and natural material response during the process of fabrication. Existing 6-Axis Robotics Software When engaging in architectural fabrication using a 6-axis robot, a designer or technician must first create a toolpath using computer-aided machining (CAM) and or/computeraided design (CAD) software for the robot to execute. There are a handful of robotics software programs for Rhinoceros and Grasshopper, including HAL, TACO and KUKA|prc. HAL provides reverse kinematics solving, simulation, and code generation. TACO is similar to HAL and offers the ability to generate code to coordinate multiple robots and upload RAPID code to the robot controller directly. KUKA|prc provides a similar feature set to HAL for KUKA robots. While these software products are excellent for simulating and generating code, there are no existing software interfaces readily accessible to the design and architecture community that allow human or environmental input to adjust and redesign the toolpath as the robot is executing code. Thus, opportunities to understand the vast potential of designing alongside robotic tools remain largely inaccessible to design professionals; unincorporated into the typical workflow of an architectural design practice. METHODS Software for Robotic Motion Design In order to produce a ceramic building component using a material deposition system (a three-dimensional or 3D printing system) on the end of a robotic arm, design intent must be translated from the architect’s ideas into data that controls the motion of the robotic arm. The transition from design to code (executable by a robotic tool) is often understood by architects and nonexperienced users on a very rudimentary level. Generating the Toolpath To fluidly translate design intent into robotic movement, a script for generating bespoke warp and weft bead patterns is developed. Instead of starting with a three-dimensional model that is then translated into a toolpath, the designer understands the design process beginning with the toolpath itself, informed by the nature of the continuous bead. By using an easily accessible visual scripting interface such as Grasshopper, one is able to generate continuous-line toolpaths for ceramic extrusion while simultaneously considering the limitations and constraints of the robot’s motions. A script is developed to function in the following way, depicted visually in Figure 2: • A bounding volume is assigned, either sourced from the Rhinoceros 3D modeling workspace or generated using the script. This volume may be modified and adapted throughout the design process. • The Grasshopper script produces a series of horizontal layers within the bounding volume at an assignable interval, each consisting of a line that weaves back and forth across the bounding volume (like the warp and weft of a textile). The script ensures that each layer reaches the extents of the bounding volume. • Each layer is manipulable in three dimensions; the print is not constrained to flat, two-dimensional layers. In the script developed for these tests, points on each layer’s edges are fixed to",
		"source": "Semantic Scholar",
		"title": "Robotic Sensing and Architectural Ceramic Fabrication",
		"URL": "https://www.semanticscholar.org/paper/Robotic-Sensing-and-Architectural-Ceramic-Norman-Rosenwasser/bf3cf4237cc8ca8a9effffd1850d0941fc6b8c9b",
		"author": [
			{
				"family": "Norman",
				"given": "B."
			},
			{
				"family": "Rosenwasser",
				"given": "David"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/D5DACYTY",
		"type": "paper-conference",
		"abstract": "Spread Page is our code name for a new, more efficient way of conveying technical information and scientific knowledge – freed form the text-centered mindset and focused on graphical, interactive, multidimensional representation. The article presents an overview of current concepts and solutions that seem applicable in crafting the idea of Spread Page. In our discussion we begin with novel, abstract, organizational ideas regarding the process of creating and disseminating scientific knowledge, that break up with the traditional model of (paper) publishing. Then we turn to analyzing methods and conventions used in (graphically) modeling real and abstract constructs, and finally review existing software solutions, technologies and exemplary, concrete products that implement certain functionalities instrumental to our cause. We reach the conclusion that, in certain areas (dealing with read-world entities, e.g. mechanics or anatomy), such desired “Spread-Page” way of representing knowledge is already within our reach. In more abstract fields, like law and legislature, political science, etc. we are still far off, mostly due to lack of appropriate standards and (graphical) notation. The paper is as a part of a larger set of articles presenting the proposed concept of Spread Page.",
		"container-title": "Computer Science and Mathematical Modelling",
		"DOI": "10.5604/01.3001.0010.8237",
		"note": "ISSN: 2450-0054\nissue: 6/2017",
		"page": "33-44",
		"source": "Semantic Scholar",
		"title": "Foundations for Spread Page: review of existing concepts, solutions, technologies capabile of improving effectiveness of conveying knowledge",
		"title-short": "Foundations for Spread Page",
		"URL": "https://isi-wat.publisherspanel.com/gicid/01.3001.0010.8237",
		"volume": "0",
		"author": [
			{
				"family": "Tarnawski",
				"given": "Tomasz"
			},
			{
				"family": "Kasprzyk",
				"given": "Rafał"
			},
			{
				"family": "Waszkowski",
				"given": "Robert"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					1,
					30
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6SHVSEAB",
		"type": "paper-conference",
		"abstract": "Purpose: The objective of this study was to determine how effective three different parent generalization support strategies (annotated books, log keeping or simple book-giving) are in getting parents to maintain a trained behavior. Method: Fifteen parents of preschool children with a diagnosed speech or language disorder completed three book readings and a training on dialogic reading. During the first book reading, parents were asked to read to their child as they normally would. Parents then participated in a twenty-minute training on five dialogic reading strategies called CROWD prompts. During the second book reading parents were asked to use the prompts they had just learned. One of three generalization support strategies was then implemented depending on which group the participant was assigned to. Groups were decided using an algorithm and were assigned in the order participants signed up for the study. At the end of the five weeks, parents completed a third book reading to determine how well they had maintained the CROWD prompts. The number of total prompts used and the number of different prompts used operated as our dependent variables. Results: On average, parents in the annotated and book only groups used a larger total number of prompts than parents in the log keeping group. On average, parents in the annotated and book only groups used a wider variety of CROWD prompts than parents in the log keeping group. Conclusions: Short-term training can increase parents’ dialogic reading behaviors. However, some support strategies promote generalization and maintenance better than others. Introduction Although parent training is a growing trend in early intervention service provision (e.g., Roberts et al., 2014), studies of caregiver training demonstrate that generalization is not easily maintained. In other words, even though SLPs can teach language-stimulation strategies to parents in a short amount of time (Lund, 2018), parents do not typically maintain those changes a month later, and do not generalize them to new activities easily (Roberts et al., 2014; Lund, 2018). It is important for speech-language pathologists to determine how best to promote generalization because caregiver involvement significantly affects a child’s speech and language skills and ultimately, success in speech therapy (Roberts & Kaiser, 2011). The purpose of this study was to explore how parent-support strategies affect parents’ abilities to generalize and maintain a skill learned in a parent training context following a five-week interval. Parent-Child Interactions and Language Development Parent input to children with language impairment differs from input to children who are typically developing (Rezzonico et al., 2014; Lund & Schuele, 2015). This may be a natural consequence of the transactional nature of language: the actions of one linguistic partner affect the actions of the other (Samerof, 1975). Early on in development, children with language impairment do not give their communication partners, often parents, many utterances to react to and consequently receive less contingent input than their typically developing peers (Paul & Elwood, 1991). Parents of children with hearing loss tend to shorten their utterances to match the language level of their children but do not provide as many language-learning cues to those children as do parents of children with normal hearing matched for vocabulary size (Lund & Schuele, 2014). Differences in language input behaviors may be maintained as children with language impairment get older. Rezzonico and colleagues (2014) studied mother-child interactions in fiveto seven-year-old children with primary language impairment (n =17) and peers with typically developing language matched for age (n = 17). Mothers of children with language impairment were more likely to correct their child’s utterances than were mothers of children with typically developing language, but those corrections are often corrections of speech sounds rather than vocabulary. Evidence of transactional communication among parents and their children with language impairment provides an avenue for intervention: if parent input can change to support language development, we might expect child language to change as well. Data from studies of parent-led interventions support this idea. Allen and Marshall (2011) investigated the effects of parentchild interaction therapy on eightto ten-year-old children with primary language impairment and found that the treatment improved verbal initiation and mean length of utterance. In a largescale study of caregiver-implemented intervention, Roberts and Kaiser (2015) found that teaching caregivers to use language facilitation strategies significantly increases receptive language in toddlers with language impairment over the course of three months. Systematic reviews of parent-implemented speech and language therapy indicate that parent training may be an effective and efficient means of changing child language, provided parents maintain the trained behavior over time (e.g., Roberts & Kaiser, 2011; Tosh, Arnott & Scarinci, 2017). However, studies of parent training seem to indicate a pattern: for a parent, learning a new behavior in a short-term time period is easy, but maintaining the behavior over time and generalizing that behavior to a new activity is difficult. A follow-up study of the children from the Roberts and Kaiser (2015) randomized controlled trial indicated that adults who received the caregiver training were not using as many strategies at 6and 12-months post-intervention as they were immediately following the study (Hampton, Roberts & Kaiser, 2017). At the 12-month follow-up mark, children in the intervention group no longer demonstrated language skills that differed from the control group. Studies using single-case design that track individual caregiver performance during intervention and maintenance intervals indicate that many adults find it difficult to maintain and generalize new behaviors once treatment has been removed (e.g., Lund, 2018; Peredo, Zelaya, & Kaiser, 2018; Roberts, Kaiser, Wolfe, Bryant, & Spidalieri, 2014). Thus, professionals need to begin to ask: how can we support caregiver maintenance and generalization of behaviors? Changing mother-child interactions to improve language outcomes for children with language delays is crucial, and research should consider how parent training can lead to sustained changes in parent behavior. Parents, language and literacy. Along with language skills, literacy skills are affected by the child’s home interactions. A child’s home literacy environment involves the literacy activities a child is engaged in, children’s access to literacy materials, and frequency of literacy activities. Multiple studies indicate that a child’s home literacy environment impacts his or her language and literacy skills (e.g., Tambyraja et al, 2016; what others?). However, children with language impairments typically have a poorer home literacy environment than their peers (Skibbe et al. 2008). Training can teach caregivers the importance of caregiver/child interactions and how to make literacy interactions meaningful. However, if parents do not maintain the trained behaviors at home, children’s progress could slow substantially. Dialogic reading is a method of reading to preschoolers that enriches caregiver/child reading interactions and promotes literacy skills (e.g., Whitehurst, 1999). One method of engaging in dialogic reading includes five prompts, Compretion, Recall... (CROWD) prompts, to guide caregivers through joint book reading. The first prompt is the Completion prompt. The caregiver leaves out the end of a sentence and allows the child to fill in the blank. For example, a caregiver might say “The frog jumped over the big, brown_____,” and then allow the child to finish the sentence with “log.” The second prompt is called the Recall prompt. With Recall prompts, the caregiver asks the child questions about a book that he or she has already read. The caregiver could ask, “What happened to the rainbow fish in the story?” The third prompt is the Open-ended prompt. These prompts are most often used with books that have detailed and illustrative pictures. The child uses the picture to describe what is going on at that particular point in the story. Open-ended prompts work best with pictures that the child is already familiar with. The fourth prompt is the Whprompt. Like Open-ended prompts, Wh-prompts are often used in accordance with pictures. The last prompt is called the distancing prompt. Distancing prompts help children relate what they read about in books to real life experiences. For example, when you are reading about playing in a park, you could ask the child “Do you remember when we played outside at recess today? Which of these games did we play?” Distancing prompts are good for working on fluency, conversational abilities, and narrative skills (Whitehurst, 2009). All of these prompts are useful tools for caregivers to use when engaging in joint book reading with a child with language impairment. If it is clear that dialogic reading techniques would benefit children with language impairment or children who are at-risk for language impairment, research should determine how SLPs, who may teach dialogic reading, could best support maintenance and generalization of these techniques. Generalization and Maintenance Support Strategies Generalization and maintenance of behavior change has traditionally been addressed by experts studying behavior modifications (such as in Applied Behavior Analysis; e.g., Walker & Buckley, 1972). Within this literature, maintenance and generalization support strategies emphasize reinforcing behavior change via prompting, positive reinforcement and continued feedback (Chandler, Lubeck, & Fowler, 1992). However, little information is available to guide maintenance of interactive language-supporting behaviors l",
		"source": "Semantic Scholar",
		"title": "A Comparison of Generalization Strategies to Support Parent Training",
		"URL": "https://www.semanticscholar.org/paper/A-Comparison-of-Generalization-Strategies-to-Parent-Carlson/e7570f104b700e5007fbf0dfd0a733d4cecf939e",
		"author": [
			{
				"family": "Carlson",
				"given": "Emma"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2018",
					5,
					19
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/7L23ZQ37",
		"type": "paper-conference",
		"abstract": "The present study examined the acquisition of anaphora in English by Iranian EFL learners as well as Persian speaking children. To do so, the study was conducted in three phases. In the first phase, 40 intermediate female and male EFL learners were selected from Puyan Institute in Takestan, Iran. Then, an off-line based Grammatical Judgment Task was administered. In the second phase, 40 female and male children from Nazanin Kindergarten in Takestan, Iran with the average age of 5 were selected and were asked to participate in an on-line based Grammatical Judgment Task. In the third phase, 40 female and male children from Shadooneh Kindergarten in Takestan, Iran with the average age of 4 were selected and were handed picture selection task. The results of the study revealed that both EFL learners and Persian speaking children had the Chomsky knowledge A. This study may have implications for teachers, students and curriculum designers. Key word: Chomsky Principle A, Chomsky Principle B, Reflex Introduction Concerning the importance of linguistic, Chomsky (1981) has established two practical principles (well-known as principle A & B) on distribution and interpretation of reflexives and pronouns. Principle A deals with reflexives and Principle B deals with pronouns. Principle A: Reflexives must be bound in their binding domain. Principle B: Pronouns must be free in their binding domain. In fact, Principle A allows people to use reflexives when subject and object are co-reference. It is not allowed to refer to any other individuals. Principle B is used when subject and object are disjoint. That is, the pronoun cannot refer to subject and it must refer to other individual. Prince and Smolensky (2004) established Optimality Theory (OT) claiming that by regarding a given input, a set of possible outputs will be generated. These possible outputs are evaluated on the basis of constraints. Constraints in OT are potentially conflicting, soft (i.e. violable) and ordered in a hierarchy according to their strength. If two constraints are in conflict, it is more important to satisfy the stronger constraint than the weaker constraint. The candidate that performs best in this competition is the optimal candidate. This is the output for the given input. All other candidates must be rejected. Because the constraints are potentially conflicting, it is possible that the optimal candidate also violates one or more of the constraints. Therefore, constraints in OT must be violable: a constraint violation is not always fatal. It only renders a candidate suboptimal if its competitors do not violate this constraint and behave similarly with respect to stronger constraints. For the present purposes, an important property of OT is that it can model both language production and language comprehension. In language production, the input is a meaning and the output is a form. Conversely, in language comprehension, the input is a form and the output is a meaning (Hendriks & Spenader, 2005). Based on OT, children’s understanding of reflexives and pronouns is unidirectional; that is, there is an asymmetry between comprehension and production of reflexives and pronouns while adult understanding on reflexives and pronouns is bidirectional (Chien & Wexler, 1990; Grimshaw & Rosen, JOURNAL OF TEACHING ENGLISH LANGUAGE STUDIES, Vol. 2, NO. 2, Fall 2013 107 1990; Jakubowicz, 1984; Kaufman, 1992; Koster & Koster, 1986; McDaniel & Maxfield, 1992, McDaniel, Smith Cairns & Hsu, 1990; McKee, 1992). According to Guasti (2002), children are able to interpret reflexives in an adult manner when they are at the age of 3 and they are able to interpret pronouns around age 6. A well-established finding from previous research on child language acquisition concerns an asymmetry in children’s offline referential interpretations of reflexives and (non-reflexive) pronouns. Several studies on languages such as English, French, and Dutch using offline picture-matching, truth-value judgment, and act-out tasks have been conducted. Consequently, children interpret reflexives in an adult-like manner, whereas their interpretation of pronouns remains non-adult-like until around six years of age. In many countries, there are large value of investigation on reflexives and pronouns, but there are not enough studies on the process of comprehension and pronouns of Iranian L1 and L2. In this regard, the objectives of this research are to shed light on the evaluation of adult Iranian English learners (L1) and Iranian children’s (L2) knowledge of Chomsky’s Principle A. In fact, there is an attempt to study the comprehension and production of reflexives and pronouns by Iranian L1 and L2. Statement of problem For a language user what matters above all linguistic points is the natural use of a language. Since individuals are under the influence of their first languages and their own cultures when learning a second/foreign language, acquiring some structures of that language is a very difficult task. In addition, the grammars of English and Persian languages have both similarities and differences. Through these similarities and differences, Persian speakers face some difficulties and misunderstandings in their English learning process. In fact, existence of these difficulties is caused by lack of students’ knowledge. One of these difficulties is related to learning and comprehending anaphora because most learners have difficulty in distinguishing the appropriate one. In other words, some students \"do not know the exact place to put it\" (Mirhassani, 2001, p. 101). This cause \"a tendency towards a learner’s preference in selecting a special category while avoiding another and this causes different types of errors\" (Rahbarian, Oroji & Fatahi, 2013, p. 212). In this regard, this study aims to find out what is the role of Chomsky principle A knowledge on the acquisition of anaphora. The contribution of grammar knowledge to the production and comprehension of reflexives and pronouns may be beneficial for teachers, material developers, as well as EFL learners. First, teachers can understand the necessity of having each Chomsky’s knowledge and can equip their curricula with them in order to help students’ problems and improve their comprehension, by working on students’ grammar knowledge. Second, material developers can add relevant grammar notes to their syllabus at class. Third, EFL learners can improve their comprehension of reflexives and pronouns by building up their grammar knowledge. They can also find the roots of some of their comprehension problems in their lack of grammar knowledge. Research question This study is aimed to answer the following questions: Do Persian speaking children have knowledge of Chomsky’ principle A? Do Iranian EFL learners have knowledge of Chomsky’ principle A? JOURNAL OF TEACHING ENGLISH LANGUAGE STUDIES, Vol. 2, NO. 2, Fall 2013 108 First and second language acquisition models In the study of first language acquisition, there exist two extreme poles. One extreme is behaviorist model pole and the other is constructivist model pole. The behaviorists claim that children come into the world with a tabula rasa, a clean slate bearing no perceived notions about the world, and that these children are then shaped by their environment and slowly conditioned through various types of reinforcement (Brown, 1987). The constructivists claim that children come into the world with very specific innate knowledge, predisposition, and biological timetables and learn to function a language chiefly through interaction and discourse (Richards, 1985). However, learning a second language does not follow these models. There are literally many millions of individuals engaged in the learning of a language which is different from their mother tongue. Some may pursue this activity independently outside formal classroom, but most foreign/second language learners enroll in institutionalized instruction of some sort. Nevertheless, second/foreign language learning will follow one of these models: monitor model and holistic model. In monitor model, there is a conscious knowledge of language which monitors the language user’s use of language. This conscious knowledge cannot produce the language, it only screens or edits. It screens before the learner produce and edit after production (Krashen, 1988). The holistic model, presented by Rinzo Titone (1993), focuses on the combination of positive points of different approaches including behavioristic, cognitivistic and humanistic psychologies. This model expresses that language learning happens in three hierarchical layers: tactic, strategic and egodynamic. The tactic layer involves acquiring or getting skills. The strategic layer is responsible for rule learning and rule formation and application. The ego-dynamic layer insists on the personality of the learners. Therefore, learners are emotional beings so if they do not like or they are not interested in the learning situations they do not learn. Anaphora A term used in grammatical description for the process or result of a linguistic unit deriving its interpretation from some previously expressed unit or meaning. More specifically, they are words which look back in the text for their interpretation. In fact, anaphora is one way of marking the identity between what is being expressed and what that has already been expressed. Anaphoric words refer backwards (O’ Grady, 2013). Example 7: Ali painted this picture in Tehran. In fact, he did that there. According to Williams (1985), anaphors are sentence-internal antecedents. That is, they must have an antecedent in the sentence. Regarding this feature, anaphors have the following properties: First, they cannot be used in a nominative position (Guasti, 2002). Example 8: *Herself arrived. Second, the antecedents must have compatible feature with their anaphors (O’Grady, 2013). Example 9: *Reza liked herself. Third, the antecedent of anaphor must c-command (",
		"source": "Semantic Scholar",
		"title": "Acquisition of English anaphora by Iranian EFL learners",
		"URL": "https://www.semanticscholar.org/paper/Acquisition-of-English-anaphora-by-Iranian-EFL-Rahamny-Takestan/ebddfbc79c35ee6d877cb76351bcf20c3636d790",
		"author": [
			{
				"family": "Rahamny",
				"given": "Manizhe"
			},
			{
				"family": "Takestan",
				"given": ""
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2017"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/JERWMPQD",
		"type": "paper-conference",
		"abstract": "Ergonomics is the scientific discipline that studies the interactions between human beings and the elements of a system and presents multiple applications in areas such as clothing and footwear design or both working and household environments. In each of these sectors, knowing the anthropometric dimensions of the current target population is fundamental to ensure that products suit as well as possible most of the users who make up the population. Anthropometry refers to the study of the measurements and dimensions of the human body and it is considered a very important branch of Ergonomics because its considerable influence on the ergonomic design of products. Human body measurements have usually been taken using rules, calipers or measuring tapes. These procedures are simple and cheap to carry out. However, they have one major drawback: the body measurements obtained and consequently, the human shape information, is imprecise and inaccurate. Furthermore, they always require interaction with real subjects, which increases the measure time and data collecting. The development of new three-dimensional (3D) scanning techniques has represented a huge step forward in the way of obtaining anthropometric data. This technology allows 3D images of human shape to be captured and at the same time, generates highly detailed and reproducible anthropometric measurements. The great potential of these new scanning systems for the digitalization of human body has contributed to promoting new anthropometric studies in several countries, such as United Kingdom, Australia, Germany, France or USA, in order to acquire accurate anthropometric data of their current population. In this context, in 2006 the Spanish Ministry of Health commissioned a 3D anthropometric survey of the Spanish female population, following the agreement signed by the Ministry itself with the Spanish associations and companies of manufacturing, distribution, fashion design and knitted sectors. A sample of 10415 Spanish females from 12 to 70 years old, randomly selected from the official Postcode Address File, was measured. The two main objectives of this study, which was conducted by the Biomechanics Institute of Valencia, were the following: on the one hand, to characterize the shape and body dimensions of the current Spanish women population to develop a standard sizing system that could be used by all clothing designers. On the other hand, to promote a healthy image of beauty through the representation of suited mannequins. In order to tackle both objectives, Statistics plays an essential role. Thus, the statistical methodologies presented in this PhD work have been applied to the database obtained from the Spanish anthropometric study. Clothing sizing systems classify the population into homogeneous groups (size groups) based on some key anthropometric dimensions. All members of the same group are similar in body shape and size, so they can wear the same garment. In addition, members of different groups are very different with respect to their body dimensions. An efficient and optimal sizing system aims at accommodating as large a percentage of the population as possible, in the optimum number of size groups that better describes the shape variability of the population. Besides, the garment fit for the accommodated individuals must be as good as possible. A very valuable reference related to sizing systems is the book Sizing in clothing: Developing effective sizing systems for ready-to-wear clothing, by Susan Ashdown. Each clothing size is defined from a person whose body measurements are located toward the central value for each of the dimensions considered in the analysis. The central person, which is considered as the size representative (the size prototype), becomes the basic pattern from which the clothing line in the same size is designed. Clustering is the statistical tool that divides a set of individuals in groups (clusters), in such a way that subjects of the same cluster are more similar to each other than to those in other groups. In addition, clustering defines each group by means of a representative individual. Therefore, it arises in a natural way the idea of using clustering to try to define an efficient sizing system. Specifically, four of the methodologies presented in this PhD thesis aimed at segmenting the population into optimal sizes, use different clustering methods. The first one, called trimowa, has been published in Expert Systems with Applications. It is based on using an especially defined distance to examine differences between women regarding their body measurements. The second and third ones (called biclustAnthropom and TDDclust, respectively) will soon be submitted in the same paper. BiclustAnthropom adapts to the field of Anthropometry a clustering method addressed in the specific case of gene expression data. Moreover, TDDclust uses the concept of statistical depth for grouping according to the most central (deep) observation in each size. As mentioned, current sizing systems are based on using an appropriate set of anthropometric dimensions, so clustering is carried out in the Euclidean space. In the three previous proposals, we have always worked in this way. Instead, in the fourth and last approach, called kmeansProcrustes, a clustering procedure is proposed for grouping taking into account the women shape, which is represented by a set of anatomical markers (landmarks). For this purpose, the statistical shape analysis will be fundamental. This contribution has been submitted for publication. A sizing system is intended to cover the so-called standard population, discarding the individuals with extreme sizes (both large and small). In mathematical language, these individuals can be considered outliers. An outlier is an observation point that is distant from other observations. In our case, a person with extreme anthopometric measurements would be considered as a statistical outlier. Clothing companies usually design garments for the standard sizes so that their market share is optimal. Nevertheless, with their foreign expansion, a lot of brands are spreading their collection and they already have a special sizes section. In last years, Internet shopping has been an alternative for consumers with extreme sizes looking for clothes that follow trends. The custom-made fabrication is other possibility with the advantage of making garments according to the customers' preferences. The four aforementioned methodologies (trimowa, biclustAnthropom, TDDclust and kmeansProcrustes) have been adapted to only accommodate the standard population. Once a particular garment has been designed, the assessing and analysis of fit is performed using one or more fit models. The fit model represents the body dimensions selected by each company to define the proportional relationships needed to achieve the fit the company has determined. The definition of an efficient sizing system relies heavily on the accuracy and representativeness of the fit models regarding the population to which it is addressed. In this PhD work, a statistical approach is proposed to identify representative fit models. It is based on another clustering method originally developed for grouping gene expression data. This method, called hipamAnthropom, has been published in Decision Support Systems. From well-defined fit models and prototypes, representative and accurate mannequins of the population can be made. Unlike clothing design, where representative cases correspond with central individuals, in the design of working and household environments, the variability of human shape is described by extreme individuals, which are those that have the largest or smallest values (or extreme combinations) in the dimensions involved in the study. This is often referred to as the accommodation problem. A very interesting reference in this area is the book entitled Guidelines for Using Anthropometric Data in Product Design, published by The Human Factors and Ergonomics Society. The idea behind this way of proceeding is that if a product fits extreme observations, it will also fit the others (less extreme). To that end, in this PhD thesis we propose two methodological contributions based on the statistical archetypal analysis. An archetype in Statistics is an extreme individual that is obtained as a convex combination of other subjects of the sample. The first of these methodologies has been published in Computers and Industrial Engineering, whereas the second one has been submitted for publication. The outline of this PhD report is as follows: Chapter 1 reviews the state of the art of Ergonomics and Anthropometry and introduces the anthropometric survey of the Spanish female population. Chapter 2 presents the trimowa, biclustAnthropom and hipamAnthropom methodologies. In Chapter 3 the kmeansProcrustes proposal is detailed. The TDDclust methodology is explained in Chapter 4. Chapter 5 presents the two methodologies related to the archetypal analysis. Since all these contributions have been programmed in the statistical software R, Chapter 6 presents the Anthropometry R package, that brings together all the algorithms associated with each approach. In this way, from Chapter 2 to Chapter 6 all the methodologies and results included in this PhD thesis are presented. At last, Chapter 7 provides the most important conclusions.",
		"source": "Semantic Scholar",
		"title": "Development of statistical methodologies applied to anthropometric data oriented towards the ergonomic design of products",
		"URL": "https://www.semanticscholar.org/paper/Development-of-statistical-methodologies-applied-to-Vis%C3%BAs/1481c9d9de949766c0a76121670bc111a3d99fbb",
		"author": [
			{
				"family": "Visús",
				"given": "Guillermo Vinué"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2014"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/QANPG9FS",
		"type": "article-journal",
		"abstract": "Using listener gaze to augment speech generation in a virtual 3D environment Maria Staudte Saarland University Alexander Koller University of Potsdam Abstract Listeners tend to gaze at objects to which they resolve referring expressions. We show that this remains true even when these objects are presented in a virtual 3D environment in which lis- teners can move freely. We further show that an automated speech generation system that uses eyetracking information to monitor listener’s understanding of referring expressions outperforms comparable systems that do not draw on listener gaze. Introduction In situated spoken interaction, there is evidence that the gaze of interlocutors can augment both language comprehension and production processes. For example, speaker gaze to ob- jects that are about to be mentioned (Griffin & Bock, 2000) has been shown to benefit listener comprehension by direct- ing listener gaze to the intended visual referents (Hanna & Brennan, 2007; Staudte & Crocker, 2011; Kreysa & Knoe- ferle, 2011). Even when speaker gaze is not visible to the listener, however, listeners are known to rapidly attend to mentioned objects (Tanenhaus, Spivey-Knowlton, Eberhard, & Sedivy, 1995). This gaze behavior on the part of listeners potentially provides speakers with useful feedback regarding the communicative success of their utterances: By monitor- ing listener gaze to objects in the environment, the speaker can determine whether or not a referring expression (RE) they have just produced was correctly understood or not, and po- tentially use this information to adjust subsequent production. In this paper we investigate the hypothesis that speaker use of listener gaze can potentially enhance interaction, even when situated in complex and dynamic scenes that simulate physical environments. In order to examine this hypothesis in a controlled and consistent manner, we monitor listener per- formance in the context of a computer system that generates spoken instructions to direct the listener through a 3D virtual environment with the goal of finding a trophy. Successful completion of the task requires listeners to press specific but- tons. Our experiment manipulated whether or not the com- puter system could follow up its original RE with feedback based on the listener’s gaze or movement behavior, with the aim of shedding light on the following two questions: • Do listener eye movements provide a consistent and useful indication of referential understanding, on a per-utterance basis, and when embedded in a dynamic and complex, goal-driven scenario? • What effect does gaze-based feedback have on listeners’ (gaze-)behavior and does it increase the more general ef- fectiveness of an interaction? We show that the listeners’ eye movements are a reliable predictor of referential understanding in our virtual environ- Konstantina Garoufi University of Potsdam Matthew Crocker Saarland University ments. A natural language generation (NLG) system, that ex- ploited this information to provide direct feedback, commu- nicated its intended referent to the listener more effectively than similar systems that did not draw on listener gaze. Gaze- based feedback was further shown to increase listener atten- tion to potential target objects in a scene, indicating a gen- erally more focused and task-oriented listener behavior. This system is, to our knowledge, the first NLG system that adjusts its referring expressions to listener gaze. Related work Previous research has shown that listeners align with speak- ers by visually attending to mentioned objects (Tanenhaus et al., 1995) and, if possible, to what the speaker attends to (Richardson & Dale, 2005; Hanna & Brennan, 2007; Staudte & Crocker, 2011). Little is known, however, about speaker adaptation to the listener’s (gaze) behavior, in particular when this occurs in dynamic and goal-oriented situations. Typi- cally, Visual World experiments have used simple and static visual scenes and disembodied utterances and have analyzed the recorded listener gaze off-line (e.g., Altmann & Kamide, 1999; Knoeferle, Crocker, Pickering, & Scheepers, 2005). Although studies involving an embodied speaker inherently include some dynamics in their stimuli, this is normally con- strained to speaker head and eye movements (Hanna & Bren- nan, 2007; Staudte & Crocker, 2011). Besides simplifying the physical environment to a static visual scene, none of these approaches can capture the reciprocal nature of interac- tion. That is, they do not take into account that the listeners’ eye movements may, as a signal of referential understanding to the speaker, change the speaker’s behavior and utterances on-line and, as such, affect the listener again. One study that emphasized interactive communication in a dynamic environment was conducted by Clark and Krych (2004). In this experiment, two partners assembled Lego models: The directing participant advised the building par- ticipant on how to achieve that goal. It was manipulated whether or not the director could see the builder’s workspace and, thus, use the builder’s visual attention as feedback for directions. Clark and Krych found, for instance, that the vis- ibility of the listener’s workspace led to significantly more deictic expressions by the speaker and to shorter task com- pletion times. However, the experimental setting introduced large variability in the dependent and independent variables, making controlled manipulation and fine-grained observa- tions difficult. In fact, we are not aware of any previ- ous work that has successfully integrated features of natu- ral environments—realistic, complex and dynamic scenes in which the visual salience of objects can change as a result of the listener’s moves in the environment—with the reciprocal",
		"container-title": "Cognitive Science",
		"source": "Semantic Scholar",
		"title": "Using listener gaze to augment speech generation in a virtual 3D environment",
		"URL": "https://www.semanticscholar.org/paper/Using-listener-gaze-to-augment-speech-generation-in-Staudte-Koller/b1cf0e388f04b1f0ca3b7fb82de0f1d712edccf7",
		"author": [
			{
				"family": "Staudte",
				"given": "Maria"
			},
			{
				"family": "Koller",
				"given": "Alexander"
			},
			{
				"family": "Garoufi",
				"given": "Konstantina"
			},
			{
				"family": "Crocker",
				"given": "M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FE7PFX54",
		"type": "paper-conference",
		"abstract": "Among the many corrective feedback techniques at ESL/EFL teachers' disposal, recasting has been identified the most frequent and preferred type of feedback in response to students’ pronunciation, vocabulary and grammar errors. According to the extensive literature, recasts can be effectively used to help students improve their linguistic accuracy in meaning-oriented classrooms. However, it is noteworthy that students do not always succeed in identifying recasts as corrections when their linguistic errors do not directly cause communication breakdown and/or when they do not have much second language (L2) knowledge to notice and self-correct their non-targetlike production after teachers’ recasts. To promote the continued growth of students’ L2 abilities, it is recommended that teachers increase the saliency of feedback by providing it in the context of form-focused tasks with metalinguistic information; in this way, the pedagogical potential of recasts can be maximized. To appear in The TESOL encyclopedia of ELT (Wiley-Blackwell) 2 Framing the Issues When classroom learners are encouraged to speak a second language (L2), they inevitably make a wide range of pronunciation, vocabulary, and grammar errors. How teachers should correct these linguistic errors to optimally enhance their students' speaking proficiency is thus an important question. On the one hand, in classrooms which focus on the accurate use of the language from the onset of learning (e.g., audio lingual methods), teachers can provide explicit correction via some form of metalinguistic explanation. On the other hand, it remains open to debate how teachers should correct students’ linguistic errors in meaning-oriented classrooms (e.g., communicative language, content-based teaching methods). Most teachers let students talk freely without much concern for the errors they make, since message conveyance is prioritized. Teachers tend to provide interactional feedback only when their students’ errors substantially hinder successful comprehension (Lightbown & Spada, 2012). Among the many feedback techniques at ESL teachers' disposal, recasts have received by far the most attention in the field of L2 education studies. Recasts are defined as “the teacher’s reformulation of all or part of a student’s utterance minus the error” (Lyster & Ranta, 1997). One such example is as follows: Example (Ellis & Sheen, 2006, p. 581) Student: What do you spend with your wife? (← trigger) Teacher: Ah, how do you spend? (← recasts) Student: How do you spend? (← repair) In this teacher-student interaction, the teacher had difficulty understanding what the student intended to say due to his/her linguistic error (i.e., trigger). Subsequently, the teacher reformulated the student’s non-targetlike production (i.e., recasts). Finally, the student demonstrated some kind of noticing and learning by immediately repeating the teacher’s recast (i.e., repair). Making the Case In the first language literature, there is some evidence that babies tend to selectively repeat their parents’ recasts (but not explicit corrections) (e.g., Farrar, 1992). Following this line of thought, some L2 acquisition scholars strongly advocate that recasts are the most ideal corrective feedback technique, precisely because of their implicitness (e.g., Long, 2007). Recasts are assumed to enable teachers to not only signal that their students have committed linguistic errors (i.e., negative evidence), but also to provide a model form (i.e., positive evidence) without interrupting the communicative flow of the meaningful teacher-student interaction. This entire conversational move is believed to promote students’ noticing and awareness of the gap between their current linguistic level and the target language—the first step towards successful L2 learning (Goo & Mackey, 2013). In addition to their potential benefits, many classroom observational studies have identified recasts as the most frequent type of corrective feedback in a wide range of instructional settings all over the world (e.g., Sheen, 2004). There is also some research evidence showing that students likely prefer recasts to other types of corrective feedback because they create a supportive, meaning-focused environment where students can work on their linguistic errors (Yoshida, 2008). However, other researchers have argued that recasts might not always lead to successful L2 learning due to their ambiguity. That is, it has been highly contentious to what degree To appear in The TESOL encyclopedia of ELT (Wiley-Blackwell) 3 classroom learners who are mainly focused on communication can actually succeed in perceiving recasts as corrections (e.g., Ellis & Sheen, 2006). Importantly, it is even possible that recasts can mistakenly lead L2 students to fossilize non-targetlike forms, since they might appear to be identical or alternative ways of saying the same thing in order to confirm message comprehensibility (Lyster, 1998a). Subsequently, L2 education studies have been conducted in order to descriptively and experimentally examine when and how teachers can enhance the noticeability and saliency of recasts, and thus maximize their pedagogical potential. First and foremost, the nature of classroom discourse takes on an important role. For example, recasts can be highly salient when they are used in a tutored setting, where students receive individualized attention from teachers (Li, 2010). Lyster and Mori (2006) also found that the pedagogical and organizational features of L2 instruction relate to the effects of recasts on eliciting students’ uptake and repair (i.e., self-correction). In this comparison study, while young French immersion students' attentional focus was exclusively on meaning, young Japanese immersion students were more analytically orientated due to a number of repetition and readaloud activities, even during their content-based lessons. The emphasis on repetition and accurate oral production in the latter immersion program led to more uptake and repair of feedback (i.e., students’ self-correction) than the former. Second, the linguistic characteristics of recasts are significantly predictive of their degree of saliency. For example, reformulating only the erroneous parts of learners’ linguistic errors (i.e., partial recasts) with a falling intonation (i.e., declarative recasts) tends to trigger learner noticing (Sheen, 2006). Furthermore, several empirical studies have confirmed the importance of adopting more pedagogically-oriented corrective feedback techniques. Sheen (2007) examined the pedagogical potential of metalinguistic correction (providing metalinguistic explanation while reformulating L2 learners’ errors) on the use of English articles (e.g., “You should use the definite article the because you’ve already mentioned fox”). The results showed that the metalinguistic correction group significantly outperformed both the recast-only group and the control group. A great deal of research attention has also been directed towards the effectiveness of prompts as a feedback technique. When using prompts, teachers withhold correct forms and push learners to make self-corrections via clarification requests (“Pardon?” “I don’t understand”), elicitations (“How do you say that in English?”) and/or repetition of students’ non-targetlike production. The relative efficacy of prompts over recasts has been confirmed, especially in classroom settings (Lyster & Saito, 2010). Another crucial variable concerns the linguistic targets of recasts. Several observational studies have found that learners tend to generate more successful repair following pronunciationfocused recasts than morphosyntax-focused recasts (Sheen, 2006); and tend to perceive the corrective intention of these recasts (e.g., Mackey, Gass, & McDonough, 2000). In his descriptive study of French immersion classrooms, Lyster (1998b) noted that students showed a higher rate of successful repair in response to pronunciation-focused recasts than to grammarfocused recasts; similar patterns have been also observed in various L2 classroom settings (Sheen, 2006). In a laboratory setting, Mackey et al. (2000) found that, when asked to watch the video clips of their task-based interaction with native speaking interlocutors (i.e., stimulated recall sessions), two groups of learners (ESL and Italian as a foreign language) recognized pronunciation-focused corrective feedback more accurately than morphosyntax-focused corrective feedback. Importantly, Mackey et al. (2000) argued that the learners’ sensitivity to phonological errors might be due to the fact that inaccurate pronunciation has “more potential to seriously interfere with understanding” than morphosyntactic errors do (p. 493). Indeed, several To appear in The TESOL encyclopedia of ELT (Wiley-Blackwell) 4 quasi-experimental studies have empirically shown the amenability of recasts to L2 pronunciation development, owing especially to their perceived saliency (Saito, 2013). The final affecting variable is learners’ individual characteristics. In terms of language aptitude, some research has shown that students with high working and phonological memory, attention control, and analytic ability tend to demonstrate high sensitivity to recasts (e.g., Trofimovich, Ammar, & Gatbonton, 2007). Since it is difficult to precisely measure and change students’ innate language aptitude through instruction, what is more directly relevant to pedagogy is the role of their differential L2 proficiency in determining the effectiveness of recasts. That is, recasts can facilitate the reinforcement and automatization of what learners already know rather than assist the acquisition of new knowledge. Following the well-established developmental sequence of English question formation, Mackey and Philps (1998) found that recasts positively influenced learners who almost mastered the target feature. In this regard, Nichola, Lightbown and Spada (2001) pointed out tha",
		"source": "Semantic Scholar",
		"title": "To appear in The TESOL encyclopedia of ELT (Wiley-Blackwell) 1 RECASTING",
		"URL": "https://www.semanticscholar.org/paper/To-appear-in-The-TESOL-encyclopedia-of-ELT-1-Saito/40dd2fbf4e658b5e5f99829f2d2777f8a5ef3c01",
		"author": [
			{
				"family": "Saito",
				"given": "Kazuya"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2016"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/6URYHZQA",
		"type": "paper-conference",
		"abstract": "Virtual reality technology is constantly developing, dedicated to computer graphics and multimedia information processing, high-performance chip can increase a hundred times the processing power, three-dimensional graphics algorithms and parametric modeling algorithm enables virtual reality technology is more mature. This paper, the application of virtual reality technology in civil engineering were studied, the results from the demonstration and validation, planning and design, engineering, construction technology and engineering safety management are discussed in this paper. Introduction With the steady growth of China's economy and infrastructure added scale, the scale of construction projects is growing, increasingly complex structure, scientific management of civil engineering, precision increasingly demanding. Realization of civil engineering, information technology, intelligence, visualization and integration become civil construction project management demands of modernization and research focus in this field [1]. Virtual Reality (Virtual Reality, VR) is a comprehensive and highly integrated high-tech, in many areas the military, medicine, design, art, entertainment and other fields have been widely used. Civil Engineering Virtual reality technology in various disciplines involved in civil engineering, has demonstrated some practical, technical potential is huge, very broad application prospects [2-3]. In civil engineering, a long time people had to use abstract concepts represent very rich content, such as using plans, sections, elevations, floor plan and other provisions the formation of some symbols to represent the three dimensional architecture, with more abstract graphics and concise The language used to describe complex scenarios to deliver a lot of information. But this kind of information processing and delivery methods affected employment, knowledge and understanding of the structure of the recipients are engaged in information, communication is very difficult. VR technology development for us to overcome this difficulty provides an extremely effective means. Virtual reality both represent the real world, it can also represent a virtual world. Virtual Reality Technology Overview Virtual reality (VR) a blend of digital image processing, computer graphics, multimedia technology, sensor technology and other information technology branch, greatly promoted the development of computer technology. VR technology is abstract, complex computer data space into an intuitive, user-familiar things. Its essence is to provide an advanced technology of man-machine interface. It is in an analog mode for the user to create a real-time interaction with physical objects reflect changes in the three-dimensional image of the world, in lifelike experience vision, hearing, touch, smell and other acts of perception, so that the participants can directly participate and explore virtual objects and changes in the role of the environment, like being in the real world [4-5]. The architecture is shown in figure 1. International Conference on Materials Engineering and Information Technology Applications (MEITA 2015) © 2015. The authors Published by Atlantis Press 1014 Figure 1.Generic virtual reality software systems architecture Virtual reality technology has the characteristics of the following three aspects. Is sex. Virtual reality technology is based on human vision, hearing the physiological and psychological characteristics, lifelike three-dimensional image generated by the computer, users wear the helmet mounted display and data gloves and other interactive devices, and can be yourself in a virtual environment, become a member of the virtual environment. The user interaction with various objects in the virtual environment, like in the real world, all feeling is so real, have a feeling of intimacy. Interactivity. Human-computer interaction in virtual reality system is a kind of close to natural interaction, users not only can use the computer keyboard, mouse to interact, but also through the special helmet, such as data glove sensing devices to interact. Users through their natural skills such as language, body movement or action, can for inspection or operation of objects in the virtual environment. More perceptual. Due to see, hear, touch, are installed in the virtual reality system, and kinesthetic The virtual reality on civil engineering design based on 3D modeling Virtual reality on civil engineering based on 3D modeling technology using geometric design, geometric virtual reality technology, is through the 3D and 3D object model scene, usually with the help of a professional modeling software (3ds Max, Maya, etc.) to complete civil engineering design approach that more practical performance scenes and objects in the real world [6], but also to create animations. The effect of virtual reality display rich, powerful, and the interactive is strong. Virtual reality on civil engineering design based on 3D modeling Multimedia virtual interaction design 3D virtual scene design Make 3D model",
		"container-title": "Proceedings of the 2015 International Conference on Materials Engineering and Information Technology Applications",
		"DOI": "10.2991/meita-15.2015.192",
		"event-place": "Guilin, China",
		"event-title": "International Conference on Materials Engineering and Information Technology Applications (MEITA 2015)",
		"ISBN": "978-94-6252-103-2",
		"language": "en",
		"publisher": "Atlantis Press",
		"publisher-place": "Guilin, China",
		"source": "Semantic Scholar",
		"title": "A New Research Approach on the Application of Virtual Reality Technology in Civil Engineering",
		"URL": "http://www.atlantis-press.com/php/paper-details.php?id=25838618",
		"author": [
			{
				"family": "Fu",
				"given": "Changchang"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2015"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/FEFGAA9Q",
		"type": "paper-conference",
		"abstract": "Autistic children often show lack of socially expressive skills that would allow them to engage with others more successfully and to facilitate their inclusion. Therefore, this study aims at investigating the impact of using video modeling on improving social skills for autistic children in inclusive school. Video-modeling (VM) is a widely used instructional technique that has been applied to teach children with developmental disabilities. The sample of this study consists of ten children; participants were randomly assigned to an experimental group (5) and control group (5). They aged between (5-7) years. In the present study, video modeling was used to promote appropriate social skills of the experiential group. In treatment program, each child watched a videotape of two persons interacting in a play setting. One person acted as the therapist and presented the social cues. The second acted as the child, and provided models of appropriate responses. Mann-Whitne, U and Wilcoxon were used for data analysis. Results indicated that there were significant differences between the sample of this study in favor of the experimental group. Introduction Autism is a neurodevelopment disorder defined by impairments in social and communication development, accompanied by stereotyped patterns of behavior and interest (Al Zyoudi, 2008). The focus of this paper is on the early development of social skills in autism and social skills intervention for impairments in social skills associated with this disorder. Social skills deficits in children with autism include: lack of orientation towards a social stimulus and inadequate use of eye contact, problems initiating social interaction, difficulty interpreting both verbal and non-verbal social cues, inappropriate emotional response and lack of empathy to others’ distress (Zriqat & Amam, 2009; Bellini & Akullian, 2007; Hine & Wolery, 2006). Several centers have developed social skill interventions to address the needs of children with autism, Among these methods are direct teaching, social reinforcement, feedback, cooperative learning, providing cues, opportunity teaching, shaping, modeling, behavioral rehearsal, peer tutoring, social stories, and video modeling (MacDonald, Sacramone, Mansfield, Wiltz & Ahern, 2009; Baker, 2007). The effective use of video modeling to help remediate the social skill deficits of children with autism is well documented. This strategy has been shown to help establish a variety of skills, including those related to play (e.g., MacDonald et al, 2009; Bellini & Akullian, 2007; D’Ateno, Mangiapanello, & Taylor, 2003), self help (e.g., Shipley-Benamou, Lutzker & Taubman, 2002), academic instruction (e.g., Kinney, Vedora, & Stromer, 2006), communication (Wert & Neisworth, 2003). Video modeling is a technique that involves demonstration of desired behaviors through active video presentation of the behavior. A video modeling intervention typically involved an individual watching a video demonstration and then imitating the behavior of the model. Video modeling is a specific application of video modeling that allows the individual to imitate targeted behaviors by observing herself/himself successfully performing a behavior (Wang, Cui & Parrial, 2011; ShuklMehta, & Callahan, 2010, Bellini & Akullian, 2007). Researchers have indicated that video modeling is potentially more effective than teaching through traditional method (Reichow & Volkmar, 2010; McCoy, & Hermansen, 2007), and can improve the effectiveness of instructional prompts (Cotugno, 2009). The use of videos to teach social skills has been examined in a recently expanding body of literature. The majority of studies investigating social skills instruction via video models, however, focused on relatively simple behaviors. For example, Paterson & Arco (20070; Bidwell & Rehfeldt (2004) used video models and contingent praise to teach adults with severe disabilities to initiate an interaction by bringing a cup of coffee to an adult peer. Nikopoulos & Keenan (2007) demonstrated that video models alone were sufficient for teaching three children with autism to initiate an interaction by gesturing or vocally requesting an adult to join the child in play. A few studies investigated video-based training for more complex social skills. Using video models alone, Maione & Mirenda (2006) obtained increases in the frequency of social initiations and responses of a young boy with autism during two different play contexts. The participant watched videos of two adults engaging in appropriate verbalizations and playing with the target activities. With the implementation of video modeling, the frequency of the participant’s use of both scripted and unscripted verbalizations (including initiations and responses) increased during these play sessions. However, reinforcement, video feedback, and prompting were needed to increase behavior in a third play context. The authors reported that some of the modeled statements were novel, while others already existed in the child’s repertoire. Serra & Dorothea (2010) indicated that video models for teaching social skills for three children with autism increased these skills. Several authors suggest that video modeling is effective because it reduces the amount of irrelevant stimuli in the learning environment, increasing the likelihood that the participant will focus on the most relevant cues (MacDonal et al, 2009; Hine & Wolery, 2006) If so, video formats that further reduce irrelevant stimuli may help promote learning. One format that may serve to reduce additional irrelevant stimuli in the learning environment is view modeling. In this type of modeling, the camera angle is presented at the participant’s eye level and shows only what the participant might see within the context of the targeted activity, skill, or context (i.e., from his or her own viewpoint). Depending on the target skill, the participant might view a specific setting or a pair of hands completing a task. Teaching with the video model may be performed in four ways: (i) modeling with video, (ii) feedback with video, (iii) cue with video, and (iv) computer-aided video teaching (Scattone, 3008; Mechling, 2005). Modeling with video is the process where the individual watches the video recordings in which all sub-steps of a skill is displayed by a peer, adult, or herself/himself/ and then repeats these behaviors (Banda, Matuszny, & Turkan, 2007). In feedback with video, the individual watches her/his own performance in a non-edited videotape; may notice her/his appropriate and inappropriate behaviors; may discuss these behaviors with the practitioner; and make adjustments in future performance (Maione, & Mirenda, 2006). Video modeling which provides individuals with the opportunity to carry out the skill step immediately on the basis of the cue given by the video and which actively involves the individual in the process is called cue with video (Mechling, 2005), implementations in which texts, graphics, animations, sound, music, slides, films and movie recordings are presented within a single system are called computer aided video training (Shukl-Mehta, & Callahan, 2010). There are a few review studies relating to the video modeling in the literature. These studies (i.e. Wang et al, 2011; Delano, 2008;McCoy, & Hermansen, 2007; Mechling, 2005) were examined according to video modeling types. The present study differs from other review studies due to some factors: First, this study analyzed video model practices used in social skills training in terms of details such as subjects, environment, research model, whether or not maintaining and generalization were targeted; in terms of fundamental categories such as social validity of the social skills selected for training; reasons for such selection; and the practice and its effectiveness. Secondly, it was based on studies conducted with individuals diagnosed with autism and other individuals with developmental disabilities. Finally, this study aimed to examine the benefits of using of video modeling intervention increasing the social skills of autistic children. The present study focused on social engagement in a natural setting. The present study also addressed a limitation of existing research by studying the effect of video modeling alone without the use of other intervention strategies. Study hypotheses: The current study aims at investigating the following hypotheses: 1. There would not be significant changes between the experimental and control groups due to the video modeling program 2. There would not be significant changes among the experimental group due to the video modeling program. 3. There would not be significant changes among the experimental group on post-test and follow up due to the video modeling program. Methods Participants Participants were 10 children diagnosed with autism by psychologist and classroom teacher. All participants were enrolled in a center-based program that provided behavioral intervention for autistic children. They were selected based on diagnosis and reports of their lack of appropriate social skills. Those children ranged in age from 5-7. They were randomly assigned to an experimental group (5) and control group (5). All sessions were conducted in a small room at the day treatment center. Study design For the purpose of this study, a pretest-post test experimental design for an experimental and control groups was used to examine the effect of using video molding on improving social skills for autistic students . Experimental group: Randomly selectedpretesttreatment program (video molding)posttestfollow up test",
		"source": "Semantic Scholar",
		"title": "The impact of using video modeling on improving social skills for autistic children in inclusive schools",
		"URL": "https://www.semanticscholar.org/paper/The-impact-of-using-video-modeling-on-improving-for/eb0ad79bcd4935ff28150419c27eb49cf01b1b16",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2012"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/WNSDE8EA",
		"type": "paper-conference",
		"abstract": "This thesis presents AEINS, Adaptive Educational Interactive Narrative System, that supports teaching ethics for 8-12 year old children. AEINS is designed based on Keller's and Gagne's learning theories. The idea is centered around involving students in moral dilemmas (called teaching moments) within which the Socratic Method is used as the teaching pedagogy. The important unique aspect of AEINS is that it exhibits the presence of four features shown to individually increase effectiveness of edugames environments, yet not integrated together in past research: a student model, a dynamic generated narrative, scripted branched narrative and evolving non-player characters. The student model aims to provide adaptation. The dynamic generated narrative forms a continuous story that glues the scripted teaching moments together. The evolving agents increase the realism and believability of the environment and perform a recognized pedagogical role by helping in supplying the educational process. \n \nAEINS has been evaluated intrinsically and empirically according to the following themes: architecture and implementation, social aspects, and educational achievements. The intrinsic evaluation checked the implicit goals embodied by the design aspects and made a value judgment about these goals. In the empirical evaluation, twenty participants were assigned to use AEINS over a number of games. The evaluation showed positive results as the participants appreciated the social characteristics of the system as they were able to recognize the genuine social aspects and the realism represented in the game. Finally, the evaluation showed indications for developing new lines of thinking for some participants to the extent that some of them were ready to carry the experience forward to the real world. However, the evaluation also suggested possible improvements, such as the use of 3D interface and free text natural language.",
		"source": "Semantic Scholar",
		"title": "Interactive narrative for adaptive educational games : architecture and an application to character education",
		"title-short": "Interactive narrative for adaptive educational games",
		"URL": "https://www.semanticscholar.org/paper/Interactive-narrative-for-adaptive-educational-%3A-an-Hodhod/e5fad6af8cbf049e493a5ed013a4061ec86ad71b",
		"author": [
			{
				"family": "Hodhod",
				"given": "R."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2010",
					6,
					17
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/YQ663H6J",
		"type": "paper-conference",
		"abstract": "The work presented in this paper describes the design and user evaluation of a 3D construction mobile game. The game, iCube, works on iPhone and iPod touch devices consisting of an application where the student performs a set of exercises about building models with unit-sized cubes in a 3D environment. There are two game modes: training and competition. The training exercises are designed so that the user can get used to the game environment. The competitions are sets of tasks, created by the teacher or administrator, which the user has to download, by logging in to the system with a username and password. Competition results are sent to the server with data on times and scores. In this work, a trial version was brought out and evaluated by twenty two testers. Users were asked to complete the training tasks available in the device as well as two competitions. About positive tool’s aspects they pointed out that game is useful for improvement of spatial ability being a fun application. However about negative aspects of it, some difficulties arose with use of tactile screen as the fingerprints caused problems while interacting with the game’s 3D environment. The results revealed that it’s necessary having in mind this item in games’ design where screen action is continuous. Introduction Play, in its diverse forms, constitutes an important part of children’s cognitive and social development . Several authors have analyzed the impact of games on education and there is wide empirical evidence supporting the positive effects of computer games as instructional tools . Spatial skills may be associated with success in scientific areas. Non-academic activities, such as playing with construction toys as a young child and playing three dimensional computer games seem to have strong relationship with spatial visualization ability. The potential of video games or computer games for improving spatial skills have been analyzed by numerous researchs . Most recent research in the field of spatial abilities focuses on how these relate to new technologies 8, 9, 10 . Recent studies have brought attention to the educational potential of handheld devices. Some tools have been tested on these devices, indicating they strengthen and support learning in fields such as languages 12, , science and natural history, and also provide an additional tool in common learning . The game, iCube, is an iPhone game where users can perform a set of exercises about building models with unit-sized cubes in a 3D environment. This paper describes our experience from developing iCube and results from a user test followed by a questionnaire. This game could help middle and high school students to improve their understanding of the 2D-3D relationship in non formal educational format. Touchscreen handheld devices were selected for the implementation of the game, because they provide an intuitive, natural and flexible interface. In actual study, we have just evaluated the game with 22 undergraduate students with two targets: (1) improving user experience with application and (2) spotting bugs and troubles on application for future versions’ improvement. The data obtained from the use of this application by the students, as described in this article, only constitute an initial P ge 22425.2 trial version of the mobile game and, therefore, cannot provide definitive conclusions. Although the idea that these applications are really useful for enhancing and practicing the students’ spatial reasoning may be derived from the comments left by the students, they cannot be taken as the basis for a study of the students’ spatial skills. System Architecture Features Figure 1 shows a summary of the system architecture and below is a brief description of how it works. The architecture we designed includes a mobile game (iCube) (1), a PC application to be used by the teacher or administrator (Building 3D Desktop) (2), a server application (3) and a database (4) to manage data about users, tasks, and results. Figure 1. System Architecture iCube (3D Mobile Game) iCube mobile game is an application for iPhone and iTouch devices where user must solve a set of exercises about building models with unit-sized cubes in a 3D environment. Exercises are set on competitions which user can download once logged into the system. Desktop application (Building 3D Desktop) allows teacher registration of new users, creation of exercises, creation of competitions and activation once they are available for download. Figure 2 shows the mobile game interface. The mobile game consists of an application where the student performs a set of exercises created by the teacher. The exercises are about building models with unit-sized cubes in a 3D environment. There are two game modes: training and competition. The training exercises are designed so that the user can get used to the game environment. They are available locally on the mobile device. The competitions are sets of tasks, created by the teacher or administrator, which the user has to download, by logging in to the system with a username and password. Users can only complete a competition once, and then go on to other competitions that the teacher or system administrator has activated. Competition results are sent to the server with data on times and scores. The application has a help function available for explaining how the game works. Each exercise has an overall score of one point if exercise is correct and zero if it’s wrong as well as time used for completing it. P ge 22425.3 Figure 2. Mobile Game Interface: (a) Login (b) Menus (c) Training access (d) Exercises, (e) An example of a Type 1 exercise Two types of exercises have been developed so far, which basically consist of building 3D models with cubes: • Type 1: Coping 3D Model. This consists of copying the proposed 3D model (as seen on Figure 3). • Type 2: Three views. This consists of building a 3D model using three orthogonal views, front, top and right. To develop this project we used the first angle projection, the ISO standard primarily used in Europe with three standard views: front view, top view and right view (see Figure 4). Figure 3. Type 1: (a) Screen 1: suggested task (b) Screen 2: 3D plan where the student solves the task Figure 4. Type 1: (a) Screen 1: suggested task (b) Screen 2: 3D plan where the student solves the task P ge 22425.4 Building 3D Desktop (PC Application) Desktop application allows system administrator managing the following operations: • Users’ management: Registering, revising and deleting users as well as consulting results of competitions made by participants (Figure 5). • Consulting, creating, revising and deleting exercises (Figure 6). Exercises are created by teacher in a 3D interactive environment in the desktop application. • Consulting, creating, revising and deleting competitions (Figure 7). Teacher will be able to manage information from competitions and exercises in a 3D environment. • Viewing competition results graphics: Desktop application allows generation of graphics showing results of both scoring and times for certain competitions (Figure 8). • Data export: Users’ results from competitions (scores and times) can be exported in a .txt file for managing them later with any spreadsheet and data analysis software. Besides graphics can be saved in .jpg or .pdf format. • Connection parameters configuration of both IP and port server in case it’s necessary. Figure 5. Users management in the PC application Figure 6. Creating exercises in the PC application Figure 7. Creating competitions in the PC application Figure 8. Users’ statistics in the PC application The specifications of the hardware and software are listed in Table 1. We used a iPod Touch device for testing the initial version of the mobile application. P ge 22425.5 Table 1. Hardware and Software Specifications Mobile Device iPod Touch iPhone PC Application Windows XP; 2.00GHz; 1 GB RAM Java Virtual Machine / Java 3D virtual machine Libraries: iText-2.1.3, jcommon-1.0.13, jdom, jfreechart-1.0.10 Server and Database MySQL and Apache Tomcat",
		"container-title": "2011 ASEE Annual Conference & Exposition Proceedings",
		"DOI": "10.18260/1-2--17706",
		"event-place": "Vancouver, BC",
		"event-title": "2011 ASEE Annual Conference & Exposition",
		"page": "22.425.1-22.425.10",
		"publisher": "ASEE Conferences",
		"publisher-place": "Vancouver, BC",
		"source": "Semantic Scholar",
		"title": "Design and Evaluation of a 3D Construction Mobile Game for the iPhone/iPod Touch Platform",
		"URL": "http://peer.asee.org/17706",
		"author": [
			{
				"family": "Martin-Dorta",
				"given": "Norena"
			},
			{
				"family": "Berriel",
				"given": "Isabel"
			},
			{
				"family": "Rodríguez",
				"given": "David"
			},
			{
				"family": "Amado",
				"given": "Héctor"
			},
			{
				"family": "Saorin",
				"given": "Jose"
			},
			{
				"family": "Contero",
				"given": "Manuel"
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2011",
					6
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/V2S4UYD8",
		"type": "paper-conference",
		"abstract": "3D Virtual Environments (VEs) have the potential to reach beyond the limitations of CAD systems and can be utilised as design tools for architecture. This paper introduces a framework of semantic-based Virtual Design Environment(VDE) that aims to provides designers of VEs with virtual observers of designers’ actions (intelligent design agents and collaborative assistant agent) to investigate the current design and respond to these actions when the need arises. The paper presents the development of a representation structure of building-objects and their relationships to be used in constructing building designs in the 3D VDE and outlines sets of design semantics to be incorporated within the VDE. interactive virtual environment application which attempts to address the issue of 3D design in general and immersive design in particular. Users of CDS can create simple conceptual building designs in an interactive, intuitive manner, simply by choosing vertices on the ground, then adding the third dimension by specifying a height for each vertex. The walls and ceiling are created automatically by CDS. Once the basic structure is in place, users may experiment with different colours, textures, add furniture to the interior of the space, or change the roof line, for example. Within CDS users will not only be able to inspect and inhabit their buildings, but will also have the ability to modify them, add details, or create new designs, all while immersed in the VE. The paper introduces a semantic-based Virtual Design Environment (VDE), that is distinguished from existing Virtual Design Studios, such as ETH (Engeli, 2001), COVEN (COVEN), MASSIVE (Greenhalgh 1995), DIVE (Frécon et al. 1991), by providing a new dimension to the use of the 3D Virtual Environments – i.e. to provide an intelligent, interactive and creative medium for designing. COVEN and MASSIVE projects are two systems aimed at improving rich group support and spatial modelling. They are not, however, primarily concerned with real-time (synchronous) multi-user representation of information and dynamic models. The environment offers document sharing and multiple interpretations of designs but does not enable collaborative designing on a single shared model in real time. The DIVE platform focuses on developing support of standardised tools and multi-user applications for networked participation. Collaborative Virtual Environments (CVEs) of the nature of COVEN support a wide range of disciplines, e.g. design, visualisation, simulation, training, education and entertainment. The DIVE technology provides the platform on which the MASSIVE networked virtual environment technologies operate. MASSIVE is essentially a teleconferencing system. The collaboration in these environments occurs primarily between human-human users, rather than between humans and agent-filtered knowledge. The intention of the DIVE and COVEN platforms is extensive visualisation but the systems are seemingly more concerned with delivery than designing and collaborative processes themselves. Furthermore, these systems offer no intelligence at a system level. Most importantly, these systems demonstrate an evolved protocol for online 3D environments but neither offer agent-advice nor semantics arising from multiple sources (Reffat and Beilharz, 2003). Hitherto VEs for architecture are lacking appropriate tools for 3D design. One would expect such VEs for architectural design to have knowledge about the designed entities. Navigating and manipulating in 3D requires not only 3D geometrical primitives, but also a set of 3D design tools. The focus of the semantic-based agents in a Virtual Environment is constructive informational and design-shaping feedback. The semantic-based framework proposed in this paper is distinguished by delivering intelligent response and feedback to the designer during the initial design phase as well as at evaluation stages of designing. 2. A conceptual framework of semantic-based virtual design environments The significance of the proposed semanticbased approach to virtual design environment is that aims to provide users and designers of virtual environments with virtual observers of designers’ actions (intelligent design agents and collaborative assistant agent) to investigate the current design and respond to these actions when the need arises. The response of intelligent design agents and collaborative assistant agent is 134 eCAADe 21 digital design dependent on the set of relationships between building elements (e.g. rooms and spaces) on a semantically high level. If such relationships of building elements can be formalised then the behaviour of VEs will be more natural. Natural behaviour in this context means that the VDE responds to the designer's expectations. Moreover, the set of relationships should be accessible using a vocabulary that is close to the designers' natural language. This research provides the potential to develop an intelligent and interactive VDE for architectural design through providing an interactive counterpart (virtual observers) to the designer and offering useful assistance in designing by supporting behaviour and semantic-based concepts within VEs (Reffat and Beilharz, 2003). The semantic-based VDE allows the designer to inspect design ideas and to discover new solutions to a design problem via related design concepts provided by intelligent design agents and triggered by designer’s actions. Such capabilities would benefit the experienced designers and strengthen the novice designer’s ability to gain depth in designing. Also the semantic-based VDE provides a platform for inspiration and creativity through providing various design moves by the collaborative assistant agent. The semantic-based Virtual designing environment is particularly important to support the designer not only with potentials for collaboration and information sharing (common to existing 3D VEs) but also to facilitate semantic-based support to the designer during the designing process. This design support is based learned design semantics and design concepts while being involved in designing. The proposed semantic-based VDE has the potential to provide opportunities for community feedback and contribution to the design outcomes in real-time. Two main attributes of the proposed VDE are its ability to provide useful design knowledge that may aid and support the designer in real-time and the capacity to combine influences from human sources and formalise an appropriate conception to the designer based on this. Other systems do not perform this role of an intelligent conduit between multiple human opinion and information sources and do not offer instructive, constructive design suggestions The conceptual framework of developing a semantic-based virtual design environment to support e-designing in architecture incorporates an integration of the following: • Develop a representation structure of building-objects and their relationships to be used in constructing building designs in the 3D virtual environment. • Develop a formalisation of design semantics of buildings (housing) including functional, spatial, esthetical, environmental and contextual design knowledge to form an initial knowledge base of intelligent design agents. • Develop intelligent behaviour and semanticbased design agents within the VDE to: (a) track designers actions in the VDE and construct an internal model of the current design situation to provide relevant design knowledge; (b) learn from designers actions, client and community responses and update the knowledge base of design semantics; and (c) find patterns of design semantics’ relationships and classify sets of concepts to enrich the agents response to changes in the virtual design environment. • Develop a real-time collaborative assistant agent that provides innovate design moves to enrich the process of design exploration through intelligent collaboration. This paper focuses on the first two elements of the proposed framework as presented in the",
		"source": "Semantic Scholar",
		"title": "Semantic-Based Virtual Design Environments for Architecture",
		"URL": "https://www.semanticscholar.org/paper/Semantic-Based-Virtual-Design-Environments-for-Reffat/a5643f83b8f3a4c9b9ca5bceeeab6d7b41e7d405",
		"author": [
			{
				"family": "Reffat",
				"given": "Rabee M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2003"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/CHMTNGMR",
		"type": "paper-conference",
		"abstract": "This paper presents a flexible, modular model architecture in Modelica for system modeling and simulation of military ground vehicles. The model platform and implemented interfaces are flexible enough to support virtual prototyping of conventional and hybrid vehicles with various physical architectures such as series, parallel, hydraulic, and plug-in implementations. Several sample model implementations of conventional and concept hybrid military ground vehicles are presented to illustrate the usage and flexibility of the model architecture to support systems engineering activities by maximizing model re-use throughout the product development process from concept assessment and design through testing and verification. INTRODUCTION Model-based systems development for commercial and military vehicles has proven effective in reducing development time and increasing product quality and reliability. ADVISOR [1] was one of the first wide ly-used tools to support system-level analysis of vehicle p owertrains. More recently tools such as PSAT [2], developed by Argonne National Laboratory for commercial vehicles , and VPSET [3], developed by TARDEC and Southwest Research Institute for military vehicles, provide configurable platforms with associated model implementations for fuel economy and performance simulations with conventional and hybrid propulsion systems. ADVISOR and PSAT have been implemented in MATLAB/Simulink platform. These tools offer either forward looking (VPSET, PSAT) or backward facing capability (ADVISOR). This paper presents a flexible, modular model archi te ture in Modelica [4] for system modeling and simulation f military ground vehicles. The model platform and implemented interfaces are flexible enough to suppo rt virtual prototyping of conventional and hybrid vehicles wit h various physical architectures such as series, parallel, hy draulic, and plug-in implementations. The architecture supports models of varying levels of details at multiple levels in the model hierarchy so that the same model architecture may s upport engineering activities over the entire systems engi neering V. By using a common framework and promoting substanti al model reuse with localized model refinement, implementations derived from this single model arch itecture can support a range of model-based engineering effo rts including high level concept assessment, requiremen tsdriven component and system design, control system d sign, and associated verification and validation (V&V) ac tivities. Plug-n-play capability enabled by formal Modelica l anguage constructs at the system, subsystem, and component level allows rapid model configuration of different vehic le implementations. The architecture includes represe ntations for the driver, environment, control system, and ve hicl physical system. Expandable elements within the architecture can support multi-voltage electrical b uses, electricification in all major subsystems, distribu ted controller implementations, and thermal modeling throughout the vehicle system including the cabin. Several sample model implementations of conventiona l and concept hybrid military ground vehicles are pre sented to illustrate the usage and flexibility of the model a rchitecture and compatible multi-domain component model librari es to support systems engineering activities throughout t he product development process. Sample analyses inclu de fuel economy assessments over mission profiles, vehicle attribute evaluation to targets, and detailed energy accounti ng based directly on the physical model implementation. Ext ensions of the implementations to support vehicle drivabili ty, NVH, and energy/thermal management are also discussed. MODELICA Before discussing the details of the model architec tur , a short introduction to the Modelica modeling languag e is provided. Modelica [4] is a non-proprietary, objec t-oriented, high-level modeling language that supports continuo us and discrete differential algebraic equations (DAE). M odelica supports both causal and acausal modeling and is es pecially suited for multi-domain physical modeling due to it s connector-based formulation. With a familiar physi cal representation of physical system components, Model ica is a key enabler for efficient, first principles modelin g and model-based systems development (MBSD). A few important features of the language are as follows: Proceedings of the 2009 Ground Vehicle Systems Engi neering and Technology Symposium (GVSETS) A Modular Model Architecture in Modelica for Rapid Virtual Prototyping of Conventional and Hybrid..., Ba tteh, et al. Page 2 of 14 • Non-proprietary nature of the language supports too l neutrality and competition while avoiding vendor lock-in • Natural physical modeling via connectors with built -in support for algebraic/kinematic constraints (DAEs) • Causal and acausal representations support a range of controls and physical modeling formalisms • Equation-oriented modeling allows symbolic processing and optimization for efficient code generation • Configuration management natively provided in the language • Model reuse via inheritance • Standard library provides interfaces and basic components in multiple physical domains • Active development community with a growing list of free and commercial application libraries Though the focus of this work is on the model archi tecture and not the modeling language, key enabling feature s of the Modelica language will be briefly highlighted in th e context of the model architecture and its capabilities. VEHICLE MODEL ARCHITECTURE Vehicle models can be used throughout the systems engineering V to support a range of model-based engineering efforts from concept assessment and des ign through testing and verification. Architecture-base d modeling is a key element of rapid virtual prototyp ing of vehicles to support model-based systems development . A formal model architecture provides consistent inter faces and system decomposition to promote distributed model development and plug-n-play interoperability betwee n models and application libraries. Figure 1 shows the top-level view of the vehicle mo del architecture in Modelica. This architecture has be en developed to support flexible, configurable modelin g of both conventional and hybrid vehicles. The architecture is based on the original Vehicle Model Architecture (VMA) [5 ] and the subsequent improvements implemented in the VehicleInterfaces library [6]. The architecture i ncludes elements for the external environment, driver, cont rol system, and all main subsystems of the vehicle. Ea ch element contains some subset of controller and mult i-domain physical connectors which are connected appropriate ly in the architecture. Note that the architecture simply def ines interfaces between components, not the implementati o details. The architecture is designed to support p lug-n-play modeling of conventional and hybrid ground vehicles at the system, subsystem, and even the component levels. While the architecture as shown is very modular and flexi b and can comprise a wide variety of vehicles and model-b ased engineering applications, it is trivial to extend f rom it to add additional top-level systems or even reconfigure fo r vehicles with different topologies such as multi-axle vehicl es while maximizing model reuse. Each top-level subsystem w ill be discussed in detail to provide context for the type s of models and implementations that are possible within the ve hicl model architecture to support a range of model-base d engineering activities. Figure 1: Vehicle model architecture The vehicle model architecture shown in Figure 1 in cludes both multi-domain physical and control system conne ctions between component connectors. Mechanical connectio ns between component connectors are shown in grey, ele ctrical connections in blue, thermal connections in red, an d controller connections in yellow. Note that the me chanical connections between components can be either 3D mul tibody connections or simple 1D rotational connection s depending on the component implementation. While Modelica has been used extensively for detailed veh icle dynamics modeling [7]-[8], this paper focuses on po wertrain torsional dynamics with 1D rotational connections. The architecture utilizes expandable connectors [4] in Modelica to provide additional flexibility. Expand able connectors can be used to construct buses of signal or even other connectors, including physical connectors. U nlike regular Modelica connectors whose contents are defi ned a priori, expandable connectors are constructed dynamically based on the union of all the connections supplied in the various components. Expandable connectors are used in the vehicle model architecture to create the following elements: • Flexible, hierarchical control bus • Multi-voltage electrical bus for electrical interac tions • Multi-node thermal bus for thermal interactions Proceedings of the 2009 Ground Vehicle Systems Engi neering and Technology Symposium (GVSETS) A Modular Model Architecture in Modelica for Rapid Virtual Prototyping of Conventional and Hybrid..., Ba tteh, et al. Page 3 of 14 Driver The driver model interfaces with the control system s and provides a range of typical driver inputs to the sy stem such as key on, accelerator demand, braking demand, gear selection (for manual transmissions), steering comm and, etc. corresponding to a particular mission profile. Vari ous driver implementations are available to provide different ways of driving the vehicle model. A direct driver model implementation actuates the model in a forward-faci ng way based on accelerator and brake inputs with resultin g vehicle speed response. This driver implementation is used for performance simulations such as accel tests. Drive r models have also been implemented to allow the vehicle to foll w a prescribed drive cycle. By taking advantage of the acausal nature of the Modelica formulation",
		"source": "Semantic Scholar",
		"title": "A MODULAR MODEL ARCHITECTURE IN MODELICA FOR RAPID VIRTUAL PROTOTYPING OF CONVENTIONAL AND HYBRID GROUND VEHICLES",
		"URL": "https://www.semanticscholar.org/paper/A-MODULAR-MODEL-ARCHITECTURE-IN-MODELICA-FOR-RAPID-Batteh-Tiller/43b0354647cee8f93f90a82518e0857bd4684e84",
		"author": [
			{
				"family": "Batteh",
				"given": "J."
			},
			{
				"family": "Tiller",
				"given": "M."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2009"
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/A7E7P7BZ",
		"type": "webpage",
		"title": "Physical and Verbal Strategies Peers Use to Facilitate the Social Inclusion of Friends with Autism Spectrum Disorders | Semantic Scholar",
		"URL": "https://www.semanticscholar.org/paper/Physical-and-Verbal-Strategies-Peers-Use-to-the-of-Ed.D.-Zascavage/77027fd994cf7779c57505fb3c4ad49e073a0bbe",
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		}
	},
	{
		"id": "http://zotero.org/users/local/escwiks7/items/76HCW3KY",
		"type": "paper-conference",
		"abstract": "The rapid evolution of nowadays network and computer technologies make it possible for the Grid to connect the nation's computers, databases, instruments, storage devices, and the user's point of view it will be act as a seamless, virtually homogeneous problem-solving environment in many field of science – including biochemistry, drug-research and engineering. The architectural differences between the single computers and the Grid resources infers the rise of the new classes of applications, in which computing resources are no longer localized, but distributed, heterogeneous and dynamic. Because the Grid is inherently more complex than existing computer systems, so the programs that execute on the Grid will reflect some of this complexity [2]. Finite difference (FD) modelling is a common way for solving partial differential equations. The more complicated the model is the greater the size of the linear system to be solved [3]. Independent of the FD problem usually a lot of tasks is repeated making possible to realize the principle of parallelisation. The main feature of a 2.5D problem is that the originally 3D problem is substituted by a series of 2D ones in the spatial wavenumber domain. The numerical determination of this frequency domain electromagnetic (FEM) response over 2D structure requires a great amount of computation, because after the Fourier transform of the Maxwell’s equations finite difference method is applied in the spatial wavenumber domain and linear set of equations has to be solved for each wavenumber. This paper describes the used practical technics and solutions that the forward 2.5D FEM modelling application are capable to exploits the computational capacity of the Grid in the aim of reducing its computational time. THE GEOPHYSICAL MODELLING SYSTEM There are different geophysical electromagnetic methods which can be classified on the basis of electromagnetic EM fields’ origin. In the magnetotelluric (MT) method natural source EM fields with variable frequency and changing direction and strength have been used to investigate the resistivity distribution of the earth from the sixties. To get rid of the natural signal strength problem in MT and to overcome the source polarization problem controlled source audio-frequency magnetotellurics (CSAMT) using grounded electric dipole source was introduced in the middle of seventies. CSAMT measurements are made not only in the far-field zone (where MT resistivity definition is a very good approximation ) but also in the transition regime (closer to the source where both frequency and separation dependence are present) approaching the near-field zone (characterized with separation and without frequency dependence). In the exploration elongated conductivity structures (twodimensional, 2D) can frequently occur, so the investigation of 2D inhomogeneities has practical importance. In these situation the EM fields become complex and depend not only upon the resistivity distribution and frequency but also on type of transmitter/receiver configuration separation. In our investigation we put emphasis on the so-called 2.5D problem ( the transmitter generates a 3D source of energy, and so calculating the frequency domain EM response over a 2D geological structure is usually termed a 2.5D problem (3D point-source plus 2D model). GRIDIFICATION PROCESS OF THE APPLICATION As a consequence of the pervasive permeation of the Grid concept, many researchers in the field of natural science as well as computer science have started to pay attention to the concept, and expect it to solve problems which are too large or too complicated to process on a single computer even on a supercomputer [1]. Grid computing offers powerful and inexpensive resources that can be pooled together to solve large problems. Employing Grid computing it can help to reduce the bottleneck problems in computing intensive application by using a widely distributed collection of resources to be tied together into a relatively seamless computing framework. Our aim is, that the forward 2.5D FEM modelling application will be capable to exploit the computing capacity of SEE-GRID [4] infrastructure. The application porting procedure can be divided into several stages: (1) enabling the application to be able to run on Grid as a single job (2) achieving the multi-site Parameter Study (PS) execution (3) composing the application into workflow (4) making the MPI based parallel version of the application. ENABLING THE APPLICATION TO BE ABLE TO RUN ON GRID AS A SINGLE JOB The original application was designed to single computers (desktop PCs, supercomputers) and written in Fortran77 programming language. Making the scientific application portable is important in the Grid, because the program binaries are transferred between the schedulers and computing resources, but Fortran77 binary codes are hardly architecture, operating system and library dependent. Hopefully, in the SEE-GRID Virtual Organization (VO) in the EGEE agreed in a same software architecture to help the VO's users in porting problem. We modified the application that it is use only the standard Fortran77 instructions, so the application can be compiled the GNU Fortran77 compiler. The application had to be compiled on a Grid entry point (UI machine), and should always use static linking to avoid the missing libraries problems. A description file has to be attached to the job, that describes the job properties and it's resource requirements, and it is written gLite JDL description language. The users submit the jobs through the Grid entry point to the schedulers, that transfer the job's binaries and input files to the suitable computing resources where the job starts running as a separated process. After the job finished successfully, the user can retrieve the output files to the Grid entry point's file system. ACHIEVING THE MULTI-SITE PARAMETER STUDY (PS) EXECUTION For the purpose of interpretation the EM responses over a model the numerical modelling system has to be executed in a frequency domain with different source polarizations. The results for ech frequency – source polarization pairs can be calculated by independent jobs. This frequency-level natural parallel aspect of the application makes possible, that it can be executed with the PS paralellization method, where the same binary executed simultaneously with different input parameters on multiple sites. The official SEE-GRID Grid portal – the P-GRADE portal version 2.5 and above [5] – has a built-in support for the PS execution, so we decided to use he portal as base running environment for our application. COMPOSING THE APPLICATION INTO WORKFLOW At first step, the generator job has been developed, that produces the input parameter space for each FEM 2.5D job. The first version of the generator job has been reached using the P-GRADE portal's auto-generator job type, that meets the requirements of the current executions At present, the generator substitutes the parameter keys with the frequency series of electromagnetic source and the related critical three element number set to create the generated parameter space. Inside of this space, the aim is to investigate the relationship between the EM field components and surface inhomogeneities by different frequency values. Additionally a collector job has been created to merging and processing the result files of the executions based on different parameter files. The result file of the collector contains the relevant EM field component's values in pre-defined format, that is already suitable for graphical processing. The software Surfer is used for the 3D visualization, that is widely-used by the researchers of the geophysical discipline. The structure of our workflow application is represented on the Figure 1. When the user execute the application, the common running scenario will be the following: 1 The generator job executed on a portal machine, that generates the input files and uploads them to a Storage Element (SE) 2 The portal's scheduler executes the FEM 2.5D job – as executable workflows (e-WFs) – simultaneously by the generated input files on multiple Computing Elements (CE). The SEE-GRID resource selection broker (WMS) is responsible for the optimal resource selection by the load of computing resources. The e-WFs upload the results to Storage Elements 3 When every e-WFs finished successfully, the collector job will be executed, that is responsible for 3.1 the collection of the e-WF’s outputs 3.2 the merging operation of the different results 3.3 generating of the visualization data for the visualizer program (Surfer) The UI of the P-GRADE portal can visualize the status of the jobs within a submitted workflow. (Fig. 2) Fig. 1 Application's structure User P-GRADE portal SEE-GRID WMS",
		"source": "Semantic Scholar",
		"title": "GRIDIFICATION OF A GEOPHYSICAL ELECTROMAGNETIC MODELLING SYSTEM",
		"URL": "https://www.semanticscholar.org/paper/GRIDIFICATION-OF-A-GEOPHYSICAL-ELECTROMAGNETIC-Ficsor-Peth%C5%91/f71e1daa5f6da6c679ffb0f27e801abedf20d537",
		"author": [
			{
				"family": "Ficsor",
				"given": "L."
			},
			{
				"family": "Pethő",
				"given": "G."
			},
			{
				"family": "Tóth",
				"given": "A."
			}
		],
		"accessed": {
			"date-parts": [
				[
					"2023",
					11,
					20
				]
			]
		},
		"issued": {
			"date-parts": [
				[
					"2008"
				]
			]
		}
	}
]